<!DOCTYPE html>
<html>

<head>
	<title>PWC Categories</title>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	<link rel="stylesheet" href="styles.css">
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="script.js"></script>
</head>

<body>
	<div class="lightbox" id="lightbox" style="display: none">
		<img src="" id="lightbox-image" alt="Enlarged Image">
	</div>
	<div class="navbar">
		<a class="nav-btn" href="general.html">General</a>
		<a class="nav-btn" href="computer-vision.html">Computer Vision</a>
		<a class="nav-btn" href="natural-language-processing.html">NLP</a>
		<a class="nav-btn" href="reinforcement-learning.html">Reinforcement Learning</a>
		<a class="nav-btn" href="audio.html">Audio</a>
		<a class="nav-btn" href="sequential.html">Sequential</a>
		<a class="nav-btn" href="graphs.html">Graphs</a>
	</div>
	<div class="bottom-right-buttons">
        <button id="toggle-cats-btn" class="bottom-right-button">Toggle categories</button>
        <button id="close-methods-btn" class="bottom-right-button">Collapse methods</button>
    </div>
	<div class="container mx-auto">
        <ul class="parent">
            <p>1. Attention</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Attention
    </strong>
    is a technique for attending to different parts of an input vector to capture long-term dependencies. Within the context of NLP, traditional sequence-to-sequence models compressed the input sequence to a fixed-length context vector, which hindered their ability to remember long inputs such as sentences. In contrast, attention creates shortcuts between the context vector and the entire source input. Below you will find a continuously updating list of attention based building blocks used in deep learning.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/SCALDE.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Scaled Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Scaled dot-product attention
  </strong>
  is an attention mechanism where the dot products are scaled down by $\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:
 </p>
 <p>
  $$ {\text{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V $$
 </p>
 <p>
  If we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \cdot k = \sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\sqrt{d_k}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Multi-Head Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-head Attention
  </strong>
  is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).
 </p>
 <p>
  $$ \text{MultiHead}\left(\textbf{Q}, \textbf{K}, \textbf{V}\right) = \left[\text{head}_{1},\dots,\text{head}_{h}\right]\textbf{W}_{0}$$
 </p>
 <p>
  $$\text{where} \text{ head}_{i} = \text{Attention} \left(\textbf{Q}\textbf{W}_{i}^{Q}, \textbf{K}\textbf{W}_{i}^{K}, \textbf{V}\textbf{W}_{i}^{V} \right) $$
 </p>
 <p>
  Above $\textbf{W}$ are all learnable parameter matrices.
 </p>
 <p>
  Note that
  <a href="https://paperswithcode.com/method/scaled">
   scaled dot-product attention
  </a>
  is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.
 </p>
 <p>
  Source:
  <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms">
   Lilian Weng
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Strided Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixed Factorized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Inspired by the success of ResNet,
Wang et al. proposed
the very deep convolutional residual attention network (RAN) by 
combining an attention mechanism with residual connections.
 </p>
 <p>
  Each attention module stacked in a residual attention network 
can be divided into a mask branch and a trunk branch. 
The trunk branch processes features,
and can be implemented by any state-of-the-art structure
including a pre-activation residual unit and an inception block.
The mask branch uses a bottom-up top-down structure
to learn a mask of the same size that 
softly weights output features from the trunk branch. 
A sigmoid layer normalizes the output to $[0,1]$ after two $1\times 1$ convolution layers. Overall the residual attention mechanism can be written as
 </p>
 <p>
  \begin{align}
s &amp;= \sigma(Conv_{2}^{1\times 1}(Conv_{1}^{1\times 1}( h_\text{up}(h_\text{down}(X))))) 
\end{align}
 </p>
 <p>
  \begin{align}
X_{out} &amp;= s f(X) + f(X)
\end{align}
where $h_\text{up}$ is a bottom-up structure, 
using max-pooling several times after residual units
to increase the receptive field, while
$h_\text{down}$ is the top-down part using 
linear interpolation to keep the output size the 
same as the input feature map. 
There are also skip-connections between the two parts,
which are omitted from the formulation.
$f$ represents the trunk branch
which can be any state-of-the-art structure.
 </p>
 <p>
  Inside each attention module, a
bottom-up top-down feedforward structure models
both spatial and cross-channel dependencies, 
 leading to a consistent performance improvement. 
Residual attention can be incorporated into
any deep network structure in an end-to-end training fashion.
However, the proposed bottom-up top-down structure fails to leverage global spatial information.
  <br/>
  Furthermore, directly predicting a 3D attention map  has high computational cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/80f0362a-596a-465c-ab74-26d89c6e66f5.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dot-Product Attention
  </strong>
  is an attention mechanism where the alignment score function is calculated as:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = h_{i}^{T}s_{j}$$
 </p>
 <p>
  It is equivalent to
  <a href="https://paperswithcode.com/method/multiplicative-attention">
   multiplicative attention
  </a>
  (without a trainable weight matrix, assuming this is instead an identity matrix). Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores/weights using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_12.32.09_PM_yYfmHYZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Temporal attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Temporal attention can be seen as a dynamic time selection mechanism determining when to pay attention, and is thus usually used for video processing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Additive Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Additive Attention
  </strong>
  , also known as
  <strong>
   Bahdanau Attention
  </strong>
  , uses a one-hidden layer feed-forward network to calculate the attention alignment score:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = v_{a}^{T}\tanh\left(\textbf{W}_{a}\left[\textbf{h}_{i};\textbf{s}_{j}\right]\right)$$
 </p>
 <p>
  where $\textbf{v}_{a}$ and $\textbf{W}_{a}$ are learned attention parameters. Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function. We can use a matrix of alignment scores to show the correlation between source and target words, as the Figure to the right shows.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.58.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Spatial Attention Module
  </strong>
  is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the
  <a href="https://paperswithcode.com/method/channel-attention-module">
   channel attention
  </a>
  , the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer to generate a spatial attention map $\textbf{M}_{s}\left(F\right) \in \mathcal{R}^{H×W}$ which encodes where to emphasize or suppress.
 </p>
 <p>
  We aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\mathbf{F}^{s}_{avg} \in \mathbb{R}^{1\times{H}\times{W}}$ and $\mathbf{F}^{s}_{max} \in \mathbb{R}^{1\times{H}\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\text{AvgPool}\left(F\right);\text{MaxPool}\left(F\right)\right]\right)\right) $$
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\mathbf{F}^{s}_{avg};\mathbf{F}^{s}_{max} \right]\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function and $f^{7×7}$ represents a convolution operation with the filter size of 7 × 7.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Visual Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  SENet pioneered channel attention. The core of SENet is a squeeze-and-excitation (SE) block which is used to collect global information, capture channel-wise relationships and improve representation ability.
SE blocks are divided into two parts, a squeeze module and an excitation module. Global spatial information is collected in the squeeze module by global average pooling. The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). Then, each channel of the input feature is scaled by multiplying the corresponding element in the attention vector. Overall, a squeeze-and-excitation block $F_\text{se}$ (with parameter $\theta$) which takes $X$ as input and outputs $Y$ can be formulated 
as:
\begin{align}
    s = F_\text{se}(X, \theta) &amp; = \sigma (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    Y = sX
\end{align}
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN Self-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   SAGAN Self-Attention Module
  </strong>
  is a self-attention module used in the
  <a href="https://paperswithcode.com/method/sagan">
   Self-Attention GAN
  </a>
  architecture for image synthesis. In the module, image features from the previous hidden layer $\textbf{x} \in \mathbb{R}^{C\text{x}N}$ are first transformed into two feature spaces $\textbf{f}$, $\textbf{g}$ to calculate the attention, where $\textbf{f(x) = W}_{\textbf{f}}{\textbf{x}}$, $\textbf{g}(\textbf{x})=\textbf{W}_{\textbf{g}}\textbf{x}$. We then calculate:
 </p>
 <p>
  $$\beta_{j, i} = \frac{\exp\left(s_{ij}\right)}{\sum^{N}_{i=1}\exp\left(s_{ij}\right)} $$
 </p>
 <p>
  $$ \text{where } s_{ij} = \textbf{f}(\textbf{x}_{i})^{T}\textbf{g}(\textbf{x}_{i}) $$
 </p>
 <p>
  and $\beta_{j, i}$ indicates the extent to which the model attends to the $i$th location when synthesizing the $j$th region. Here, $C$ is the number of channels and $N$ is the number of feature
locations of features from the previous hidden layer. The output of the attention layer is $\textbf{o} = \left(\textbf{o}_{\textbf{1}}, \textbf{o}_{\textbf{2}}, \ldots, \textbf{o}_{\textbf{j}} , \ldots, \textbf{o}_{\textbf{N}}\right) \in \mathbb{R}^{C\text{x}N}$ , where,
 </p>
 <p>
  $$ \textbf{o}_{\textbf{j}} = \textbf{v}\left(\sum^{N}_{i=1}\beta_{j, i}\textbf{h}\left(\textbf{x}_{\textbf{i}}\right)\right) $$
 </p>
 <p>
  $$ \textbf{h}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{h}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  $$ \textbf{v}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{v}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  In the above formulation, $\textbf{W}_{\textbf{g}} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\mathbf{W}_{f} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\textbf{W}_{\textbf{h}} \in \mathbb{R}^{\bar{C}\text{x}C}$ and $\textbf{W}_{\textbf{v}} \in \mathbb{R}^{C\text{x}\bar{C}}$ are the learned weight matrices, which are implemented as $1$×$1$ convolutions. The authors choose  $\bar{C} = C/8$.
 </p>
 <p>
  In addition, the module further multiplies the output of the attention layer by a scale parameter and adds back the input feature map. Therefore, the final output is given by,
 </p>
 <p>
  $$\textbf{y}_{\textbf{i}} = \gamma\textbf{o}_{\textbf{i}} + \textbf{x}_{\textbf{i}}$$
 </p>
 <p>
  where $\gamma$ is a learnable scalar and it is initialized as 0. Introducing $\gamma$ allows the network to first rely on the cues in the local neighborhood – since this is easier – and then gradually learn to assign more weight to the non-local evidence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM_79d4mU6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SPIN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9140bae0-27c1-4c07-8650-0acc229f18ed.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  RAM adopts RNNs and reinforcement learning (RL) to make the network learn where to pay attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cross-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Cross-Attention
  </strong>
  module is an attention module used in
  <a href="https://paperswithcode.com/method/crossvit">
   CrossViT
  </a>
  for fusion of multi-scale features. The CLS token of the large branch (circle) serves as a query token to interact with the patch tokens from the small branch through attention. $f\left(·\right)$ and $g\left(·\right)$ are projections to align dimensions. The small branch follows the same procedure but swaps CLS and patch tokens from another branch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.02.11_PM_O6x0hLv.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global and Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FAVOR+</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FAVOR+
  </strong>
  , or
  <strong>
   Fast Attention Via Positive Orthogonal Random Features
  </strong>
  , is an efficient attention mechanism used in the
  <a href="https://paperswithcode.com/method/performer">
   Performer
  </a>
  architecture which leverages approaches such as kernel methods and random features approximation for approximating
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  and Gaussian kernels.
 </p>
 <p>
  FAVOR+ works for attention blocks using matrices $\mathbf{A} \in \mathbb{R}^{L×L}$ of the form $\mathbf{A}(i, j) = K(\mathbf{q}_{i}^{T}, \mathbf{k}_{j}^{T})$, with $\mathbf{q}_{i}/\mathbf{k}_{j}$ standing for the $i^{th}/j^{th}$ query/key row-vector in $\mathbf{Q}/\mathbf{K}$ and kernel $K : \mathbb{R}^{d } × \mathbb{R}^{d} \rightarrow \mathbb{R}_{+}$ defined for the (usually randomized) mapping: $\phi : \mathbb{R}^{d } → \mathbb{R}^{r}_{+}$ (for some $r &gt; 0$) as:
 </p>
 <p>
  $$K(\mathbf{x}, \mathbf{y}) = E[\phi(\mathbf{x})^{T}\phi(\mathbf{y})] $$
 </p>
 <p>
  We call $\phi(\mathbf{u})$ a random feature map for $\mathbf{u} \in \mathbb{R}^{d}$ . For $\mathbf{Q}^{'}, \mathbf{K}^{'} \in \mathbb{R}^{L \times r}$ with rows given as $\phi(\mathbf{q}_{i}^{T})^{T}$ and $\phi(\mathbf{k}_{i}^{T})^{T}$  respectively, this leads directly to the efficient attention mechanism of the form:
 </p>
 <p>
  $$ \hat{Att_{\leftrightarrow}}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \hat{\mathbf{D}}^{-1}(\mathbf{Q^{'}}((\mathbf{K^{'}})^{T}\mathbf{V}))$$
 </p>
 <p>
  where
 </p>
 <p>
  $$\mathbf{\hat{D}} = \text{diag}(\mathbf{Q^{'}}((\mathbf{K^{'}})\mathbf{1}_{L})) $$
 </p>
 <p>
  The above scheme constitutes the
  <a href="https://paperswithcode.com/method/dfa">
   FA
  </a>
  -part of the FAVOR+ mechanism. The other parts are achieved by:
 </p>
 <ul>
  <li>
   The R part :  The softmax kernel is approximated though trigonometric functions, in the form of a regularized softmax-kernel SMREG, that employs positive random features (PRFs).
  </li>
  <li>
   The OR+ part : To reduce the variance of the estimator, so we can use a smaller number of random features, different samples are entangled to be exactly orthogonal using the Gram-Schmidt orthogonalization procedure.
  </li>
 </ul>
 <p>
  The details are quite technical, so it is recommended you read the paper for further information on these steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.34.37_PM_JjR0C9D.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Blender</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blender
  </strong>
  is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.
 </p>
 <p>
  The inputs of the blender module are bottom-level bases $\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  of Mask R-CNN to crop bases with each proposal $\mathbf{p}_{d}$ and then resize the region to a fixed size $R \times R$ feature map $\mathbf{r}_{d}$
 </p>
 <p>
  $$
\mathbf{r}_{d}=\operatorname{RoIPool}_{R \times R}\left(\mathbf{B}, \mathbf{p}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  More specifically,  asampling ratio 1 is used for
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference,
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  prediction results are used.
 </p>
 <p>
  The attention size $M$ is smaller than $R$. We interpolate $\mathbf{a}_{d}$ from $M \times M$ to $R \times R$, into the shapes of $R=\left(\mathbf{r}_{d} \mid d=1 \ldots D\right)$
 </p>
 <p>
  $$
\mathbf{a}_{d}^{\prime}=\text { interpolate }_{M \times M \rightarrow R \times R}\left(\mathbf{a}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then $\mathbf{a}_{d}^{\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\mathbf{s}_{d}$.
 </p>
 <p>
  $$
\mathbf{s}_{d}=\operatorname{softmax}\left(\mathbf{a}_{d}^{\prime}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then we apply element-wise product between each entity $\mathbf{r}_{d}, \mathbf{s}_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\mathbf{m}_{d}:$
 </p>
 <p>
  $$
\mathbf{m}_{d}=\sum_{k=1}^{K} \mathbf{s}_{d}^{k} \circ \mathbf{r}_{d}^{k}, \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  where $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8f129739-0f31-4b55-814d-798ea57ef403.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dynamic Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The extremely low computational cost of lightweight CNNs constrains the depth and width of the networks, further decreasing their representational power. To address the above problem, Chen et al. proposed dynamic convolution, a novel operator design that increases  representational power with negligible additional computational cost and does not change the width or depth of the network in parallel with CondConv.
 </p>
 <p>
  Dynamic convolution uses $K$ parallel convolution kernels of the same  size and input/output dimensions instead of one kernel per layer. Like SE blocks, it adopts a squeeze-and-excitation mechanism to generate the attention weights for the different convolution kernels. These kernels are then aggregated dynamically by weighted summation and applied to the input feature map $X$:
\begin{align}
    s &amp; = \text{softmax} (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    \text{DyConv} &amp;= \sum_{i=1}^{K} s_k \text{Conv}_k 
\end{align}
\begin{align}
    Y &amp;= \text{DyConv}(X)
\end{align}
Here the convolutions are combined by summation of weights and biases of convolutional kernels.
 </p>
 <p>
  Compared to applying convolution to the feature map, the computational cost of squeeze-and-excitation and weighted summation is extremely low. Dynamic convolution thus provides an efficient operation to improve  representational power and can be easily used as a replacement for any convolution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/76ecd4c4-4c2a-44c2-b248-a8cc2786f5a6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Disentangled Attention Mechanism</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Disentangled Attention Mechanism
  </strong>
  is an attention mechanism used in the
  <a href="https://paperswithcode.com/method/deberta">
   DeBERTa
  </a>
  architecture. Unlike
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  where each word in the input layer is represented using a vector which is the sum of its word (content) embedding and position embedding, each word in DeBERTa is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices based on their contents and relative positions, respectively. This is motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1cc628e2-a516-4871-8e5a-c420cf11614c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Channel Attention Module
  </strong>
  is a module for channel-based attention in convolutional neural networks. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map.
 </p>
 <p>
  We first aggregate spatial information of a feature map by using both average-pooling and max-pooling operations, generating two different spatial context descriptors: $\mathbf{F}^{c}_{avg}$ and $\mathbf{F}^{c}_{max}$, which denote average-pooled features and max-pooled features respectively.
 </p>
 <p>
  Both descriptors are then forwarded to a shared network to produce our channel attention map $\mathbf{M}_{c} \in \mathbb{R}^{C\times{1}\times{1}}$. Here $C$ is the number of channels. The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $\mathbb{R}^{C/r×1×1}$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, we merge the output feature vectors using element-wise summation. In short, the channel attention is computed as:
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\text{MLP}\left(\text{AvgPool}\left(\mathbf{F}\right)\right)+\text{MLP}\left(\text{MaxPool}\left(\mathbf{F}\right)\right)\right) $$
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{avg}\right)\right) +\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{max}\right)\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function, $\mathbf{W}_{0} \in \mathbb{R}^{C/r\times{C}}$, and $\mathbf{W}_{1} \in \mathbb{R}^{C\times{C/r}}$. Note that the MLP weights, $\mathbf{W}_{0}$ and $\mathbf{W}_{1}$, are shared for both inputs and the
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activation function is followed by $\mathbf{W}_{0}$.
 </p>
 <p>
  Note that the channel attention module with just
  <a href="https://paperswithcode.com/method/average-pooling">
   average pooling
  </a>
  is the same as the
  <a href="https://paperswithcode.com/method/squeeze-and-excitation-block">
   Squeeze-and-Excitation Module
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.21_PM_YDoPGUi.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Axial Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Axial Attention
  </strong>
  is a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. It was first proposed in
  <a href="https://paperswithcode.com/method/ccnet">
   CCNet
  </a>
  [1] named as criss-cross attention, which harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Ho et al [2] extents CCNet to process multi-dimensional data.  The proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. It serves as the basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors, e.g., Axial Transformers. It has been applied in
  <a href="https://paperswithcode.com/method/alphafold">
   AlphaFold
  </a>
  [3] for interpreting protein sequences.
 </p>
 <p>
  [1] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu. CCNet: Criss-Cross Attention for Semantic Segmentation. ICCV, 2019.
 </p>
 <p>
  [2] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans. arXiv:1912.12180
 </p>
 <p>
  [3] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A, Bridgland A. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Jul 15:1-1.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cca_ANy2LjX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Attention Mechanisms</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Attention Mechanisms
    </strong>
    are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/22f26a20-1608-449b-ab55-55d8136e097e.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Scaled Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Scaled dot-product attention
  </strong>
  is an attention mechanism where the dot products are scaled down by $\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:
 </p>
 <p>
  $$ {\text{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V $$
 </p>
 <p>
  If we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \cdot k = \sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\sqrt{d_k}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Strided Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixed Factorized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Inspired by the success of ResNet,
Wang et al. proposed
the very deep convolutional residual attention network (RAN) by 
combining an attention mechanism with residual connections.
 </p>
 <p>
  Each attention module stacked in a residual attention network 
can be divided into a mask branch and a trunk branch. 
The trunk branch processes features,
and can be implemented by any state-of-the-art structure
including a pre-activation residual unit and an inception block.
The mask branch uses a bottom-up top-down structure
to learn a mask of the same size that 
softly weights output features from the trunk branch. 
A sigmoid layer normalizes the output to $[0,1]$ after two $1\times 1$ convolution layers. Overall the residual attention mechanism can be written as
 </p>
 <p>
  \begin{align}
s &amp;= \sigma(Conv_{2}^{1\times 1}(Conv_{1}^{1\times 1}( h_\text{up}(h_\text{down}(X))))) 
\end{align}
 </p>
 <p>
  \begin{align}
X_{out} &amp;= s f(X) + f(X)
\end{align}
where $h_\text{up}$ is a bottom-up structure, 
using max-pooling several times after residual units
to increase the receptive field, while
$h_\text{down}$ is the top-down part using 
linear interpolation to keep the output size the 
same as the input feature map. 
There are also skip-connections between the two parts,
which are omitted from the formulation.
$f$ represents the trunk branch
which can be any state-of-the-art structure.
 </p>
 <p>
  Inside each attention module, a
bottom-up top-down feedforward structure models
both spatial and cross-channel dependencies, 
 leading to a consistent performance improvement. 
Residual attention can be incorporated into
any deep network structure in an end-to-end training fashion.
However, the proposed bottom-up top-down structure fails to leverage global spatial information.
  <br/>
  Furthermore, directly predicting a 3D attention map  has high computational cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/80f0362a-596a-465c-ab74-26d89c6e66f5.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dot-Product Attention
  </strong>
  is an attention mechanism where the alignment score function is calculated as:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = h_{i}^{T}s_{j}$$
 </p>
 <p>
  It is equivalent to
  <a href="https://paperswithcode.com/method/multiplicative-attention">
   multiplicative attention
  </a>
  (without a trainable weight matrix, assuming this is instead an identity matrix). Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores/weights using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_12.32.09_PM_yYfmHYZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Temporal attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Temporal attention can be seen as a dynamic time selection mechanism determining when to pay attention, and is thus usually used for video processing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Additive Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Additive Attention
  </strong>
  , also known as
  <strong>
   Bahdanau Attention
  </strong>
  , uses a one-hidden layer feed-forward network to calculate the attention alignment score:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = v_{a}^{T}\tanh\left(\textbf{W}_{a}\left[\textbf{h}_{i};\textbf{s}_{j}\right]\right)$$
 </p>
 <p>
  where $\textbf{v}_{a}$ and $\textbf{W}_{a}$ are learned attention parameters. Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function. We can use a matrix of alignment scores to show the correlation between source and target words, as the Figure to the right shows.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.58.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  SENet pioneered channel attention. The core of SENet is a squeeze-and-excitation (SE) block which is used to collect global information, capture channel-wise relationships and improve representation ability.
SE blocks are divided into two parts, a squeeze module and an excitation module. Global spatial information is collected in the squeeze module by global average pooling. The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). Then, each channel of the input feature is scaled by multiplying the corresponding element in the attention vector. Overall, a squeeze-and-excitation block $F_\text{se}$ (with parameter $\theta$) which takes $X$ as input and outputs $Y$ can be formulated 
as:
\begin{align}
    s = F_\text{se}(X, \theta) &amp; = \sigma (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    Y = sX
\end{align}
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SPIN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9140bae0-27c1-4c07-8650-0acc229f18ed.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  RAM adopts RNNs and reinforcement learning (RL) to make the network learn where to pay attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global and Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FAVOR+</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FAVOR+
  </strong>
  , or
  <strong>
   Fast Attention Via Positive Orthogonal Random Features
  </strong>
  , is an efficient attention mechanism used in the
  <a href="https://paperswithcode.com/method/performer">
   Performer
  </a>
  architecture which leverages approaches such as kernel methods and random features approximation for approximating
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  and Gaussian kernels.
 </p>
 <p>
  FAVOR+ works for attention blocks using matrices $\mathbf{A} \in \mathbb{R}^{L×L}$ of the form $\mathbf{A}(i, j) = K(\mathbf{q}_{i}^{T}, \mathbf{k}_{j}^{T})$, with $\mathbf{q}_{i}/\mathbf{k}_{j}$ standing for the $i^{th}/j^{th}$ query/key row-vector in $\mathbf{Q}/\mathbf{K}$ and kernel $K : \mathbb{R}^{d } × \mathbb{R}^{d} \rightarrow \mathbb{R}_{+}$ defined for the (usually randomized) mapping: $\phi : \mathbb{R}^{d } → \mathbb{R}^{r}_{+}$ (for some $r &gt; 0$) as:
 </p>
 <p>
  $$K(\mathbf{x}, \mathbf{y}) = E[\phi(\mathbf{x})^{T}\phi(\mathbf{y})] $$
 </p>
 <p>
  We call $\phi(\mathbf{u})$ a random feature map for $\mathbf{u} \in \mathbb{R}^{d}$ . For $\mathbf{Q}^{'}, \mathbf{K}^{'} \in \mathbb{R}^{L \times r}$ with rows given as $\phi(\mathbf{q}_{i}^{T})^{T}$ and $\phi(\mathbf{k}_{i}^{T})^{T}$  respectively, this leads directly to the efficient attention mechanism of the form:
 </p>
 <p>
  $$ \hat{Att_{\leftrightarrow}}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \hat{\mathbf{D}}^{-1}(\mathbf{Q^{'}}((\mathbf{K^{'}})^{T}\mathbf{V}))$$
 </p>
 <p>
  where
 </p>
 <p>
  $$\mathbf{\hat{D}} = \text{diag}(\mathbf{Q^{'}}((\mathbf{K^{'}})\mathbf{1}_{L})) $$
 </p>
 <p>
  The above scheme constitutes the
  <a href="https://paperswithcode.com/method/dfa">
   FA
  </a>
  -part of the FAVOR+ mechanism. The other parts are achieved by:
 </p>
 <ul>
  <li>
   The R part :  The softmax kernel is approximated though trigonometric functions, in the form of a regularized softmax-kernel SMREG, that employs positive random features (PRFs).
  </li>
  <li>
   The OR+ part : To reduce the variance of the estimator, so we can use a smaller number of random features, different samples are entangled to be exactly orthogonal using the Gram-Schmidt orthogonalization procedure.
  </li>
 </ul>
 <p>
  The details are quite technical, so it is recommended you read the paper for further information on these steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.34.37_PM_JjR0C9D.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dynamic Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The extremely low computational cost of lightweight CNNs constrains the depth and width of the networks, further decreasing their representational power. To address the above problem, Chen et al. proposed dynamic convolution, a novel operator design that increases  representational power with negligible additional computational cost and does not change the width or depth of the network in parallel with CondConv.
 </p>
 <p>
  Dynamic convolution uses $K$ parallel convolution kernels of the same  size and input/output dimensions instead of one kernel per layer. Like SE blocks, it adopts a squeeze-and-excitation mechanism to generate the attention weights for the different convolution kernels. These kernels are then aggregated dynamically by weighted summation and applied to the input feature map $X$:
\begin{align}
    s &amp; = \text{softmax} (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    \text{DyConv} &amp;= \sum_{i=1}^{K} s_k \text{Conv}_k 
\end{align}
\begin{align}
    Y &amp;= \text{DyConv}(X)
\end{align}
Here the convolutions are combined by summation of weights and biases of convolutional kernels.
 </p>
 <p>
  Compared to applying convolution to the feature map, the computational cost of squeeze-and-excitation and weighted summation is extremely low. Dynamic convolution thus provides an efficient operation to improve  representational power and can be easily used as a replacement for any convolution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/76ecd4c4-4c2a-44c2-b248-a8cc2786f5a6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Disentangled Attention Mechanism</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Disentangled Attention Mechanism
  </strong>
  is an attention mechanism used in the
  <a href="https://paperswithcode.com/method/deberta">
   DeBERTa
  </a>
  architecture. Unlike
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  where each word in the input layer is represented using a vector which is the sum of its word (content) embedding and position embedding, each word in DeBERTa is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices based on their contents and relative positions, respectively. This is motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1cc628e2-a516-4871-8e5a-c420cf11614c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Axial Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Axial Attention
  </strong>
  is a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. It was first proposed in
  <a href="https://paperswithcode.com/method/ccnet">
   CCNet
  </a>
  [1] named as criss-cross attention, which harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Ho et al [2] extents CCNet to process multi-dimensional data.  The proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. It serves as the basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors, e.g., Axial Transformers. It has been applied in
  <a href="https://paperswithcode.com/method/alphafold">
   AlphaFold
  </a>
  [3] for interpreting protein sequences.
 </p>
 <p>
  [1] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu. CCNet: Criss-Cross Attention for Semantic Segmentation. ICCV, 2019.
 </p>
 <p>
  [2] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans. arXiv:1912.12180
 </p>
 <p>
  [3] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A, Bridgland A. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Jul 15:1-1.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cca_ANy2LjX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li>
            <details class="category depth2">
            <summary>Attention Patterns</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The original
    <a href="https://paperswithcode.com/method/scaled">
     self-attention component
    </a>
    in the
    <a href="https://paperswithcode.com/method/transformer">
     Transformer
    </a>
    architecture has a $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Attention pattern methods look to reduce this complexity by looking at a subset of the space.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/strided.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Strided Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixed Factorized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global and Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Synthesized Attention Mechanisms</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Dense Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dense Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is a type of synthetic attention mechanism that replaces the notion of
  <a href="https://paperswithcode.com/method/scaled">
   query-key-values
  </a>
  in the self-attention module and directly synthesizes the alignment matrix instead. Dense attention is conditioned on each input token. The method accepts an input $X \in \mathbb{R}^{l\text{ x }d}$ and produces an output of $Y \in \mathbb{R}^{l\text{ x }d}$. Here $l$ refers to the sequence length and $d$ refers to the dimensionality of the model. We first adopt $F\left(.\right)$, a parameterized function, for projecting input $X_{i}$ from $d$ dimensions to $l$ dimensions.
 </p>
 <p>
  $$B_{i} = F\left(X_{i}\right)$$
 </p>
 <p>
  where $F\left(.\right)$ is a parameterized function that maps $\mathbb{R}^{d}$ to $\mathbb{R}^{l}$ and $i$ is the $i$-th token of $X$. Intuitively, this can be interpreted as learning a token-wise projection to the sequence length $l$. Essentially, with this model, each token predicts weights for each token in the input sequence. In practice, a simple two layered feed-forward layer with
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activations for $F\left(.\right)$ is adopted:
 </p>
 <p>
  $$ F\left(X\right) = W\left(\sigma_{R}\left(W(X) + b\right)\right) + b$$
 </p>
 <p>
  where $\sigma_{R}$ is the ReLU activation function. Hence, $B$ is now of $\mathbb{R}^{l\text{ x }d}$. Given $B$, we now compute:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(B\right)G\left(X\right) $$
 </p>
 <p>
  where $G\left(.\right)$ is another parameterized function of $X$ that is analogous to $V$ (value) in the standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model. This approach eliminates the
  <a href="https://paperswithcode.com/method/scaled">
   dot product
  </a>
  altogether by replacing $QK^{T}$ in standard Transformers with the synthesizing function $F\left(.\right)$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Synthesized Attention
  </strong>
  is a form of synthesized attention where the attention weights are not conditioned on any input tokens. Instead, the attention weights are initialized to random values. It was introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. Random Synthesized Attention contrasts with
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   Dense Synthesized Attention
  </a>
  which conditions on each token independently, as opposed to pairwise token interactions in the vanilla
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model.
 </p>
 <p>
  Let $R$ be a randomly initialized matrix. Random Synthesized Attention is defined as:
 </p>
 <p>
  $$Y = \text{Softmax}\left(R\right)G\left(X\right) $$
 </p>
 <p>
  where $R \in \mathbb{R}^{l \text{ x } l}$. Notably, each head adds 2 parameters to the overall network. The basic idea of the Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples. This is a direct generalization of the recently proposed fixed self-attention patterns of
  <a href="https://arxiv.org/abs/2002.10260">
   Raganato et al (2020)
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Factorized Random Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Random Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is similar to
  <a href="https://paperswithcode.com/method/factorized-dense-synthesized-attention">
   factorized dense synthesized attention
  </a>
  but for random synthesizers. Letting $R$ being a randomly initialized matrix, we factorize $R$ into low rank matrices $R_{1}, R_{2} \in \mathbb{R}^{l\text{ x}k}$ in the attention function:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(R_{1}R_{2}^{T}\right)G\left(X\right) . $$
 </p>
 <p>
  Here $G\left(.\right)$ is a parameterized function that is equivalent to $V$ in
  <a href="https://paperswithcode.com/method/scaled">
   Scaled Dot-Product Attention
  </a>
  .
 </p>
 <p>
  For each head, the factorization reduces the parameter costs from $l^{2}$ to $2\left(lk\right)$ where
$k &lt;&lt; l$ and hence helps prevent overfitting. In practice, we use a small value of $k = 8$.
 </p>
 <p>
  The basic idea of a  Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM_PkacRfG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Factorized Dense Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Dense Synthesized Attention
  </strong>
  is a synthesized attention mechanism, similar to
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   dense synthesized attention
  </a>
  , but we factorize the outputs to reduce parameters and prevent overfitting. It was proposed as part of the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. The factorized variant of the dense synthesizer can be expressed as follows:
 </p>
 <p>
  $$A, B = F_{A}\left(X_{i}\right), F_{B}\left(X_{i}\right)$$
 </p>
 <p>
  where $F_{A}\left(.\right)$ projects input $X_{i}$ into $a$ dimensions, $F_B\left(.\right)$ projects $X_{i}$ to $b$ dimensions, and $a \text{ x } b = l$. The output of the factorized module is now written as:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(C\right)G\left(X\right) $$
 </p>
 <p>
  where $C = H_{A}\left(A\right) * H_{B}\left(B\right)$, where $H_{A}$, $H_{B}$ are tiling functions and $C \in \mathbb{R}^{l \text{ x } l}$. The tiling function simply duplicates the vector $k$ times, i.e., $\mathbb{R}^{l} \rightarrow \mathbb{R}^{lk}$. In this case, $H_{A}\left(\right)$ is a projection of $\mathbb{R}^{a} \rightarrow \mathbb{R}^{ab}$ and $H_{B}\left(\right)$ is a projection of $\mathbb{R}^{b} \rightarrow \mathbb{R}^{ba}$. To avoid having similar values within the same block, we compose the outputs of $H_{A}$ and $H_{B}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM_52J3Q9s.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Attention Modules</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Attention Modules
    </strong>
    refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Multi-Head Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-head Attention
  </strong>
  is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).
 </p>
 <p>
  $$ \text{MultiHead}\left(\textbf{Q}, \textbf{K}, \textbf{V}\right) = \left[\text{head}_{1},\dots,\text{head}_{h}\right]\textbf{W}_{0}$$
 </p>
 <p>
  $$\text{where} \text{ head}_{i} = \text{Attention} \left(\textbf{Q}\textbf{W}_{i}^{Q}, \textbf{K}\textbf{W}_{i}^{K}, \textbf{V}\textbf{W}_{i}^{V} \right) $$
 </p>
 <p>
  Above $\textbf{W}$ are all learnable parameter matrices.
 </p>
 <p>
  Note that
  <a href="https://paperswithcode.com/method/scaled">
   scaled dot-product attention
  </a>
  is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.
 </p>
 <p>
  Source:
  <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms">
   Lilian Weng
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Spatial Attention Module
  </strong>
  is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the
  <a href="https://paperswithcode.com/method/channel-attention-module">
   channel attention
  </a>
  , the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer to generate a spatial attention map $\textbf{M}_{s}\left(F\right) \in \mathcal{R}^{H×W}$ which encodes where to emphasize or suppress.
 </p>
 <p>
  We aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\mathbf{F}^{s}_{avg} \in \mathbb{R}^{1\times{H}\times{W}}$ and $\mathbf{F}^{s}_{max} \in \mathbb{R}^{1\times{H}\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\text{AvgPool}\left(F\right);\text{MaxPool}\left(F\right)\right]\right)\right) $$
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\mathbf{F}^{s}_{avg};\mathbf{F}^{s}_{max} \right]\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function and $f^{7×7}$ represents a convolution operation with the filter size of 7 × 7.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN Self-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   SAGAN Self-Attention Module
  </strong>
  is a self-attention module used in the
  <a href="https://paperswithcode.com/method/sagan">
   Self-Attention GAN
  </a>
  architecture for image synthesis. In the module, image features from the previous hidden layer $\textbf{x} \in \mathbb{R}^{C\text{x}N}$ are first transformed into two feature spaces $\textbf{f}$, $\textbf{g}$ to calculate the attention, where $\textbf{f(x) = W}_{\textbf{f}}{\textbf{x}}$, $\textbf{g}(\textbf{x})=\textbf{W}_{\textbf{g}}\textbf{x}$. We then calculate:
 </p>
 <p>
  $$\beta_{j, i} = \frac{\exp\left(s_{ij}\right)}{\sum^{N}_{i=1}\exp\left(s_{ij}\right)} $$
 </p>
 <p>
  $$ \text{where } s_{ij} = \textbf{f}(\textbf{x}_{i})^{T}\textbf{g}(\textbf{x}_{i}) $$
 </p>
 <p>
  and $\beta_{j, i}$ indicates the extent to which the model attends to the $i$th location when synthesizing the $j$th region. Here, $C$ is the number of channels and $N$ is the number of feature
locations of features from the previous hidden layer. The output of the attention layer is $\textbf{o} = \left(\textbf{o}_{\textbf{1}}, \textbf{o}_{\textbf{2}}, \ldots, \textbf{o}_{\textbf{j}} , \ldots, \textbf{o}_{\textbf{N}}\right) \in \mathbb{R}^{C\text{x}N}$ , where,
 </p>
 <p>
  $$ \textbf{o}_{\textbf{j}} = \textbf{v}\left(\sum^{N}_{i=1}\beta_{j, i}\textbf{h}\left(\textbf{x}_{\textbf{i}}\right)\right) $$
 </p>
 <p>
  $$ \textbf{h}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{h}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  $$ \textbf{v}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{v}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  In the above formulation, $\textbf{W}_{\textbf{g}} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\mathbf{W}_{f} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\textbf{W}_{\textbf{h}} \in \mathbb{R}^{\bar{C}\text{x}C}$ and $\textbf{W}_{\textbf{v}} \in \mathbb{R}^{C\text{x}\bar{C}}$ are the learned weight matrices, which are implemented as $1$×$1$ convolutions. The authors choose  $\bar{C} = C/8$.
 </p>
 <p>
  In addition, the module further multiplies the output of the attention layer by a scale parameter and adds back the input feature map. Therefore, the final output is given by,
 </p>
 <p>
  $$\textbf{y}_{\textbf{i}} = \gamma\textbf{o}_{\textbf{i}} + \textbf{x}_{\textbf{i}}$$
 </p>
 <p>
  where $\gamma$ is a learnable scalar and it is initialized as 0. Introducing $\gamma$ allows the network to first rely on the cues in the local neighborhood – since this is easier – and then gradually learn to assign more weight to the non-local evidence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM_79d4mU6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cross-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Cross-Attention
  </strong>
  module is an attention module used in
  <a href="https://paperswithcode.com/method/crossvit">
   CrossViT
  </a>
  for fusion of multi-scale features. The CLS token of the large branch (circle) serves as a query token to interact with the patch tokens from the small branch through attention. $f\left(·\right)$ and $g\left(·\right)$ are projections to align dimensions. The small branch follows the same procedure but swaps CLS and patch tokens from another branch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.02.11_PM_O6x0hLv.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Blender</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blender
  </strong>
  is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.
 </p>
 <p>
  The inputs of the blender module are bottom-level bases $\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  of Mask R-CNN to crop bases with each proposal $\mathbf{p}_{d}$ and then resize the region to a fixed size $R \times R$ feature map $\mathbf{r}_{d}$
 </p>
 <p>
  $$
\mathbf{r}_{d}=\operatorname{RoIPool}_{R \times R}\left(\mathbf{B}, \mathbf{p}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  More specifically,  asampling ratio 1 is used for
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference,
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  prediction results are used.
 </p>
 <p>
  The attention size $M$ is smaller than $R$. We interpolate $\mathbf{a}_{d}$ from $M \times M$ to $R \times R$, into the shapes of $R=\left(\mathbf{r}_{d} \mid d=1 \ldots D\right)$
 </p>
 <p>
  $$
\mathbf{a}_{d}^{\prime}=\text { interpolate }_{M \times M \rightarrow R \times R}\left(\mathbf{a}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then $\mathbf{a}_{d}^{\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\mathbf{s}_{d}$.
 </p>
 <p>
  $$
\mathbf{s}_{d}=\operatorname{softmax}\left(\mathbf{a}_{d}^{\prime}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then we apply element-wise product between each entity $\mathbf{r}_{d}, \mathbf{s}_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\mathbf{m}_{d}:$
 </p>
 <p>
  $$
\mathbf{m}_{d}=\sum_{k=1}^{K} \mathbf{s}_{d}^{k} \circ \mathbf{r}_{d}^{k}, \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  where $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8f129739-0f31-4b55-814d-798ea57ef403.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Channel Attention Module
  </strong>
  is a module for channel-based attention in convolutional neural networks. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map.
 </p>
 <p>
  We first aggregate spatial information of a feature map by using both average-pooling and max-pooling operations, generating two different spatial context descriptors: $\mathbf{F}^{c}_{avg}$ and $\mathbf{F}^{c}_{max}$, which denote average-pooled features and max-pooled features respectively.
 </p>
 <p>
  Both descriptors are then forwarded to a shared network to produce our channel attention map $\mathbf{M}_{c} \in \mathbb{R}^{C\times{1}\times{1}}$. Here $C$ is the number of channels. The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $\mathbb{R}^{C/r×1×1}$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, we merge the output feature vectors using element-wise summation. In short, the channel attention is computed as:
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\text{MLP}\left(\text{AvgPool}\left(\mathbf{F}\right)\right)+\text{MLP}\left(\text{MaxPool}\left(\mathbf{F}\right)\right)\right) $$
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{avg}\right)\right) +\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{max}\right)\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function, $\mathbf{W}_{0} \in \mathbb{R}^{C/r\times{C}}$, and $\mathbf{W}_{1} \in \mathbb{R}^{C\times{C/r}}$. Note that the MLP weights, $\mathbf{W}_{0}$ and $\mathbf{W}_{1}$, are shared for both inputs and the
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activation function is followed by $\mathbf{W}_{0}$.
 </p>
 <p>
  Note that the channel attention module with just
  <a href="https://paperswithcode.com/method/average-pooling">
   average pooling
  </a>
  is the same as the
  <a href="https://paperswithcode.com/method/squeeze-and-excitation-block">
   Squeeze-and-Excitation Module
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.21_PM_YDoPGUi.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>2. Optimization</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Stochastic Gradient Descent
  </strong>
  is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:
 </p>
 <p>
  $$ w_{t+1} = w_{t} - \eta\hat{\nabla}_{w}{L(w_{t})} $$
 </p>
 <p>
  Where $\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.
 </p>
 <p>
  (Image Source:
  <a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_2.57.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RMSProp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RMSProp
  </strong>
  is an unpublished adaptive learning rate optimizer
  <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
   proposed by Geoff Hinton
  </a>
  . The motivation is that the magnitude of gradients can differ for different weights, and can change during learning, making it hard to choose a single global learning rate. RMSProp tackles this by keeping a moving average of the squared gradient and adjusting the weight updates by this magnitude. The gradient updates are performed as:
 </p>
 <p>
  $$E\left[g^{2}\right]_{t} = \gamma E\left[g^{2}\right]_{t-1} + \left(1 - \gamma\right) g^{2}_{t}$$
 </p>
 <p>
  $$\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E\left[g^{2}\right]_{t} + \epsilon}}g_{t}$$
 </p>
 <p>
  Hinton suggests $\gamma=0.9$, with a good default for $\eta$ as $0.001$.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_3.16.15_PM_kjTCskF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ADMM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   alternating direction method of multipliers
  </strong>
  (
  <strong>
   ADMM
  </strong>
  ) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. It takes the form of a decomposition-coordination procedure, in which the solutions to small
local subproblems are coordinated to find a solution to a large global problem. ADMM can be viewed as an attempt to blend the benefits of dual decomposition and augmented Lagrangian methods for constrained optimization. It turns out to be equivalent or closely related to many other algorithms
as well, such as Douglas-Rachford splitting from numerical analysis, Spingarn’s method of partial inverses, Dykstra’s alternating projections method, Bregman iterative algorithms for l1 problems in signal processing, proximal methods, and many others.
 </p>
 <p>
  Text Source:
  <a href="https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">
   https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://www.slideshare.net/derekcypang/alternating-direction">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.00.39_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Search
  </strong>
  replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.
 </p>
 <p>
  Extracted from
  <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search">
   Wikipedia
  </a>
 </p>
 <p>
  Source
  <a href="https://dl.acm.org/doi/10.5555/2188385.2188395">
   Paper
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://dl.acm.org/doi/pdf/10.5555/2188385.2188395">
   BERGSTRA AND BENGIO
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.51.13_mfhxuoD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdamW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdamW
  </strong>
  is a stochastic optimization method that modifies the typical implementation of weight decay in
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  , by decoupling
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  from the gradient update. To see this, $L_{2}$ regularization in Adam is usually implemented with the below modification where $w_{t}$ is the rate of the weight decay at time $t$:
 </p>
 <p>
  $$ g_{t} = \nabla{f\left(\theta_{t}\right)} + w_{t}\theta_{t}$$
 </p>
 <p>
  while AdamW adjusts the weight decay term to appear in the gradient update:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \eta\left(\frac{1}{\sqrt{\hat{v}_{t} + \epsilon}}\cdot{\hat{m}_{t}} + w_{t, i}\theta_{t, i}\right), \forall{t}$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_8.27.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD with Momentum</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <h3>
  Why SGD with  Momentum?
 </h3>
 <p>
  In deep learning, we have used stochastic gradient descent as one of the optimizers because at the end we will find the minimum weight and bias at which the model loss is lowest. In the SGD we have some issues in which the SGD does not work perfectly because in deep learning we got a non-convex cost function graph and if use the simple SGD then it leads to low performance. There are 3 main reasons why it does not work:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  1) We end up in local minima and not able to reach global minima
At the start, we randomly start at some point and we are going to end up at the local minimum and not able to reach the global minimum.
 </p>
 <p>
  2) Saddle Point will be the stop for reaching global minima
A saddle point is a point where in one direction the surface goes in the upward direction and in another direction it goes downwards. So that the slope is changing very gradually so the speed of changing is going to slow and as result, the training also going to slow.
 </p>
 <p>
  3) High curvature can be a reason
The larger radius leads to low curvature and vice-versa. It will be difficult to traverse in the large curvature which was generally high in non-convex optimization.
By using the SGD with Momentum optimizer we can overcome the problems like high curvature, consistent gradient, and noisy gradient.
 </p>
 <h3>
  What is SGD with Momentum?
 </h3>
 <p>
  SGD with  Momentum is one of the optimizers which is used to improve the performance of the neural network.
 </p>
 <p>
  Let's take an example and understand the intuition behind the optimizer suppose we have a ball which is sliding from the start of the slope as it goes the speed of the bowl is increased over time. If we have one point A and we want to reach point B and we don't know in which direction to move but we ask for the 4 points which have already reached point B. If all 4 points are pointing you in the same direction then the confidence of the A is more and it goes in the direction pointed very fast. This is the main concept behind the SGD with Momentum.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*zNbZqU_uDIV13c9ZCJOEXA.jpeg" style="width: 400px; height: 250px;"/>
 </p>
 <h3>
  How does SGD with Momentum work?
 </h3>
 <p>
  So first to understand the concept of exponentially weighted moving average (EWMA). It was a technique through which try to find the trend in time series data. The formula of the EWMA is :
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*O9Wcq-mbRgNOdRNTivSefw.png" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  In the formula, β represents the weightage that is going to assign to the past values of the gradient. The values of β is from 0 &lt; β &lt; 1. If the value of the beta is 0.5 then it means that the 1/1–0.5 = 2 so it represents that the calculated average was from the previous 2 readings.
 </p>
 <p>
  The value of Vt depends on β. The higher the value of β the more we try to get an average of more past data and vice-versa. For example, let's take the value of β 0.98 and 0.5 for two different scenarios so if we do 1/1-β then we get 50 and 10 respectively so it was clear that to calculate the average we take past 50 and 10 outcomes respectively for both cases.
Now in SGD with Momentum, we use the same concept of EWMA. Here we introduce the term velocity v which is used to denote the change in the gradient to get to the global minima. The change in the weights is denoted by the formula:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/0*i_r3u7LACa6dQyXd" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  the β part of the V formula denotes and is useful to compute the confidence or we can say the past velocity for calculating Vt we have to calculate Vt-1 and for calculating Vt-1 we have to calculate Vt-2 and likewise. So we are using the history of velocity to calculate the momentum and this is the part that provides acceleration to the formula.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*L5lNKxAHLPYNc6-Zs4Vscw.png" style="width: 300px; height: 100px;"/>
 </p>
 <p>
  Here we have to consider two cases:
1. β=0 then, as per the formula weight updating is going to just work as a Stochastic gradient descent. Here we called β a decaying factor because it is defining the speed of past velocity.
 </p>
 <ol>
  <li>
   β=1 then, there will be no decay. It involves the dynamic equilibrium which is not desired so we generally use the value of β like 0.9,0.99or 0.5 only.
  </li>
 </ol>
 <h3>
  Advantages of SGD with Momentum :
 </h3>
 <ol>
  <li>
   Momentum is faster than stochastic gradient descent the training will be faster than SGD.
  </li>
  <li>
   Local minima can be an escape and reach global minima due to the momentum involved.
  </li>
 </ol>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*Nb39bHHUWGXqgisr2WcLGQ.gif" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  Here in the video, we can see that purple is SGD with Momentum and light blue is for SGD the SGD with Momentum can reach global minima whereas SGD is stuck in local minima.
But there is a catch, the momentum itself can be a problem sometimes because of the high momentum after reaching global minima it is still fluctuating and take some time to get stable at global minima. And that kind of behavior leads to time consumption which makes SGD with Momentum slower than other optimization out there but still faster than SGD.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deep Ensembles</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TTUR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Two Time-scale Update Rule (TTUR)
  </strong>
  is an update rule for generative adversarial networks trained with stochastic gradient descent. TTUR has an individual learning rate for both the discriminator and the generator. The main premise is that the discriminator converges to a local minimum when the generator is fixed. If the generator changes slowly enough, then the discriminator still converges, since the generator perturbations are small. Besides ensuring convergence, the performance may also improve since the discriminator must first learn new patterns before they are transferred to the generator. In contrast, a generator which is overly fast, drives the discriminator steadily into new regions without capturing its gathered information.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.01.58_PM_4837H8V.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gravity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Gravity is a kinematic approach to optimization based on gradients.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gradient Clipping</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  One difficulty that arises with optimization of deep neural networks is that large parameter gradients can lead an
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  optimizer to update the parameters strongly into a region where the loss function is much greater, effectively undoing much of the work that was needed to get to the current solution.
 </p>
 <p>
  <strong>
   Gradient Clipping
  </strong>
  clips the size of the gradients to ensure optimization performs more reasonably near sharp areas of the loss surface. It can be performed in a number of ways. One option is to simply clip the parameter gradient element-wise before a parameter update. Another option is to clip the norm ||$\textbf{g}$|| of the gradient $\textbf{g}$ before a parameter update:
 </p>
 <p>
  $$\text{ if } ||\textbf{g}||  &gt; v \text{ then } \textbf{g} \leftarrow \frac{\textbf{g}{v}}{||\textbf{g}||}$$
 </p>
 <p>
  where $v$ is a norm threshold.
 </p>
 <p>
  Source: Deep Learning, Goodfellow et al
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/pdf/1211.5063.pdf">
   Pascanu et al
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.41.14_PM_LLryIVA.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MAS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  This optimizer mix
  <a href="https://paperswithcode.com/method/adam">
   ADAM
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  creating the MAS optimizer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sharpness-Aware Minimization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sharpness-Aware Minimization
  </strong>
  , or
  <strong>
   SAM
  </strong>
  , is a procedure that improves model generalization by simultaneously minimizing loss value and loss sharpness. SAM functions by seeking parameters that lie in neighborhoods having uniformly low loss value (rather than parameters that only themselves have low loss value).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_1.29.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  SAGA is a method in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AMP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Based on the understanding that the flat local minima of the empirical risk cause the model to generalize better. Adversarial Model Perturbation (AMP) improves generalization via minimizing the
  <strong>
   AMP loss
  </strong>
  , which is obtained from the empirical risk by applying the
  <strong>
   worst
  </strong>
  norm-bounded perturbation on each point in the parameter space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/20210423163756_vLFJ4RI.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SLR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Please enter a description about the method here
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Dynamic algorithm configuration (DAC) is capable of generalizing over prior optimization approaches, as well as handling optimization of hyperparameters that need to be adjusted over multiple time-steps.
 </p>
 <p>
  Image Source:
  <a href="http://ecai2020.eu/papers/1237_paper.pdf">
   Biedenkapp et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.52.52_lOWtUby.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Hybrid Optimization</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Hybrid Optimization
    </strong>
    methods combine various optimization methods to attempt to yield the benefits of both while minimizing or averaging out the downsides. Below you can find a continuously updating list of hybrid optimization methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MEUZZ</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MEUZZ
  </strong>
  is a machine learning-based hybrid fuzzer which employs supervised machine learning for adaptive and generalizable seed scheduling -- a prominent factor in determining the yields of hybrid fuzzing. MEUZZ determines which new seeds are expected to produce better fuzzing yields based on the knowledge learned from past seed scheduling decisions made on the same or similar programs. MEUZZ's learning is based on a series of features extracted via code reachability and dynamic analysis, which incurs negligible runtime overhead (in microseconds). Moreover, MEUZZ automatically infers the data labels by evaluating the fuzzing performance of each selected seed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.41.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HFPSO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hybrid Firefly and Particle Swarm Optimization (HFPSO)
  </strong>
  is a metaheuristic optimization algorithm that combines strong points of firefly and particle swarm optimization. HFPSO tries to determine the start of the local search process properly by checking the previous global best fitness values.
 </p>
 <p>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S156849461830084X">
   Click Here for the Paper
  </a>
 </p>
 <p>
  <a href="https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso">
   Codes (MATLAB)
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/screenshot_P5onJoX.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Hyperparameter Search</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Hyperparameter Search
    </strong>
    methods are used to search for hyperparameters during the training stage of a neural network. Below you can find a continuously updating list of (specialized) hyperparameter search methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Random Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Search
  </strong>
  replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.
 </p>
 <p>
  Extracted from
  <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search">
   Wikipedia
  </a>
 </p>
 <p>
  Source
  <a href="https://dl.acm.org/doi/10.5555/2188385.2188395">
   Paper
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://dl.acm.org/doi/pdf/10.5555/2188385.2188395">
   BERGSTRA AND BENGIO
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.51.13_mfhxuoD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Dynamic algorithm configuration (DAC) is capable of generalizing over prior optimization approaches, as well as handling optimization of hyperparameters that need to be adjusted over multiple time-steps.
 </p>
 <p>
  Image Source:
  <a href="http://ecai2020.eu/papers/1237_paper.pdf">
   Biedenkapp et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.52.52_lOWtUby.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Population Based Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Population Based Training
  </strong>
  , or
  <strong>
   PBT
  </strong>
  , is an optimization method for finding parameters and hyperparameters, and extends upon parallel search methods and sequential optimisation methods.
It leverages information sharing across a population of concurrently running optimisation processes, and allows for online propagation/transfer of parameters and hyperparameters between members of the population based on their performance. Furthermore, unlike most other adaptation schemes, the method is capable of performing online adaptation of hyperparameters -- which can be particularly important in problems with highly non-stationary learning dynamics, such as reinforcement learning settings. PBT is decentralised and asynchronous, although it could also be executed semi-serially or with partial synchrony if there is a binding budget constraint.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_12.32.38_PM_Y92E9S5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Differentiable Hyperparameter Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Differentiable simultaneous optimization of hyperparameters and neural network architecture. Also a
  <a href="https://paperswithcode.com/method/neural-architecture-search">
   Neural Architecture Search
  </a>
  (NAS) method.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/multi_channel_net_2x2_x2x2x2_5UpurvB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Prioritized Sampling</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Prioritized Sampling
    </strong>
    methods are used in tasks like object detection to prioritize examples (e.g. hard examples) to induce better detection performance. Below you can find a continuously updating list of prioritized sampling methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ATSS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Training Sample Selection
  </strong>
  , or
  <strong>
   ATSS
  </strong>
  , is a method to automatically select positive and negative samples according to statistical characteristics of object. It bridges the gap between anchor-based and anchor-free detectors.
 </p>
 <p>
  For each ground-truth box $g$ on the image, we first find out its candidate positive samples. As described in Line $3$ to $6$, on each pyramid level, we select $k$ anchor boxes whose center are closest to the center of $g$ based on L2 distance. Supposing there are $\mathcal{L}$ feature pyramid levels, the ground-truth box $g$ will have $k\times\mathcal{L}$ candidate positive samples. After that, we compute the IoU between these candidates and the ground-truth $g$ as $\mathcal{D}_g$ in Line $7$, whose mean and standard deviation are computed as $m_g$ and $v_g$ in Line $8$ and Line $9$. With these statistics, the IoU threshold for this ground-truth $g$ is obtained as $t_g=m_g+v_g$ in Line $10$. Finally, we select these candidates whose IoU are greater than or equal to the threshold $t_g$ as final positive samples in Line $11$ to $15$.
 </p>
 <p>
  Notably ATSS also limits the positive samples' center to the ground-truth box as shown in Line $12$. Besides, if an anchor box is assigned to multiple ground-truth boxes, the one with the highest IoU will be selected. The rest are negative samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_3.26.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PISA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PrIme Sample Attention (PISA)
  </strong>
  directs the training of object detection frameworks towards prime samples. These are samples that play a key role in driving the detection performance. The authors define Hierarchical Local Rank (HLR) as a metric of importance. Specifically, they use IoU-HLR to rank positive samples and ScoreHLR to rank negative samples in each mini-batch. This ranking strategy places the positive samples with highest IoUs around each object and the negative samples with highest scores in each cluster to the top of the ranked list and directs the focus of the training process to them via a simple re-weighting scheme. The authors also devise a classification-aware regression loss to jointly optimize the classification and regression branches. Particularly, this loss would suppress those samples with large regression loss, thus reinforcing the attention to prime samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_1.58.27_PM_2LW9YFb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>OHEM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Some object detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more
effective and efficient.
  <strong>
   OHEM
  </strong>
  , or
  <strong>
   Online Hard Example Mining
  </strong>
  , is a bootstrapping technique that modifies
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  to sample from examples in a non-uniform way depending on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution
that favors diverse, high loss instances.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_11.34.39_AM_RvmJwmo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>IoU-Balanced Sampling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IoU-Balanced Sampling
  </strong>
  is hard mining method for object detection. Suppose we need to sample $N$ negative samples from $M$ corresponding candidates. The selected probability for each sample under random sampling is:
 </p>
 <p>
  $$ p = \frac{N}{M} $$
 </p>
 <p>
  To raise the selected probability of hard negatives, we evenly split the sampling interval into $K$ bins according to IoU. $N$ demanded negative samples are equally distributed to each bin. Then we select samples from them uniformly. Therefore, we get the selected probability under IoU-balanced sampling:
 </p>
 <p>
  $$ p_{k} = \frac{N}{K}*\frac{1}{M_{k}}\text{ , } k\in\left[0, K\right)$$
 </p>
 <p>
  where $M_{k}$ is the number of sampling candidates in the corresponding interval denoted by $k$. $K$ is set to 3 by default in our experiments.
 </p>
 <p>
  The sampled histogram with IoU-balanced sampling is shown by green color in the Figure to the right. The IoU-balanced sampling can guide the distribution of training samples close to the one of hard negatives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_9.42.43_PM_DwR5Ggy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Stochastic Optimization</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Stochastic Optimization
    </strong>
    methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.50.29_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Stochastic Gradient Descent
  </strong>
  is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:
 </p>
 <p>
  $$ w_{t+1} = w_{t} - \eta\hat{\nabla}_{w}{L(w_{t})} $$
 </p>
 <p>
  Where $\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.
 </p>
 <p>
  (Image Source:
  <a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_2.57.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RMSProp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RMSProp
  </strong>
  is an unpublished adaptive learning rate optimizer
  <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
   proposed by Geoff Hinton
  </a>
  . The motivation is that the magnitude of gradients can differ for different weights, and can change during learning, making it hard to choose a single global learning rate. RMSProp tackles this by keeping a moving average of the squared gradient and adjusting the weight updates by this magnitude. The gradient updates are performed as:
 </p>
 <p>
  $$E\left[g^{2}\right]_{t} = \gamma E\left[g^{2}\right]_{t-1} + \left(1 - \gamma\right) g^{2}_{t}$$
 </p>
 <p>
  $$\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E\left[g^{2}\right]_{t} + \epsilon}}g_{t}$$
 </p>
 <p>
  Hinton suggests $\gamma=0.9$, with a good default for $\eta$ as $0.001$.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_3.16.15_PM_kjTCskF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdamW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdamW
  </strong>
  is a stochastic optimization method that modifies the typical implementation of weight decay in
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  , by decoupling
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  from the gradient update. To see this, $L_{2}$ regularization in Adam is usually implemented with the below modification where $w_{t}$ is the rate of the weight decay at time $t$:
 </p>
 <p>
  $$ g_{t} = \nabla{f\left(\theta_{t}\right)} + w_{t}\theta_{t}$$
 </p>
 <p>
  while AdamW adjusts the weight decay term to appear in the gradient update:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \eta\left(\frac{1}{\sqrt{\hat{v}_{t} + \epsilon}}\cdot{\hat{m}_{t}} + w_{t, i}\theta_{t, i}\right), \forall{t}$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_8.27.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD with Momentum</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <h3>
  Why SGD with  Momentum?
 </h3>
 <p>
  In deep learning, we have used stochastic gradient descent as one of the optimizers because at the end we will find the minimum weight and bias at which the model loss is lowest. In the SGD we have some issues in which the SGD does not work perfectly because in deep learning we got a non-convex cost function graph and if use the simple SGD then it leads to low performance. There are 3 main reasons why it does not work:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  1) We end up in local minima and not able to reach global minima
At the start, we randomly start at some point and we are going to end up at the local minimum and not able to reach the global minimum.
 </p>
 <p>
  2) Saddle Point will be the stop for reaching global minima
A saddle point is a point where in one direction the surface goes in the upward direction and in another direction it goes downwards. So that the slope is changing very gradually so the speed of changing is going to slow and as result, the training also going to slow.
 </p>
 <p>
  3) High curvature can be a reason
The larger radius leads to low curvature and vice-versa. It will be difficult to traverse in the large curvature which was generally high in non-convex optimization.
By using the SGD with Momentum optimizer we can overcome the problems like high curvature, consistent gradient, and noisy gradient.
 </p>
 <h3>
  What is SGD with Momentum?
 </h3>
 <p>
  SGD with  Momentum is one of the optimizers which is used to improve the performance of the neural network.
 </p>
 <p>
  Let's take an example and understand the intuition behind the optimizer suppose we have a ball which is sliding from the start of the slope as it goes the speed of the bowl is increased over time. If we have one point A and we want to reach point B and we don't know in which direction to move but we ask for the 4 points which have already reached point B. If all 4 points are pointing you in the same direction then the confidence of the A is more and it goes in the direction pointed very fast. This is the main concept behind the SGD with Momentum.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*zNbZqU_uDIV13c9ZCJOEXA.jpeg" style="width: 400px; height: 250px;"/>
 </p>
 <h3>
  How does SGD with Momentum work?
 </h3>
 <p>
  So first to understand the concept of exponentially weighted moving average (EWMA). It was a technique through which try to find the trend in time series data. The formula of the EWMA is :
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*O9Wcq-mbRgNOdRNTivSefw.png" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  In the formula, β represents the weightage that is going to assign to the past values of the gradient. The values of β is from 0 &lt; β &lt; 1. If the value of the beta is 0.5 then it means that the 1/1–0.5 = 2 so it represents that the calculated average was from the previous 2 readings.
 </p>
 <p>
  The value of Vt depends on β. The higher the value of β the more we try to get an average of more past data and vice-versa. For example, let's take the value of β 0.98 and 0.5 for two different scenarios so if we do 1/1-β then we get 50 and 10 respectively so it was clear that to calculate the average we take past 50 and 10 outcomes respectively for both cases.
Now in SGD with Momentum, we use the same concept of EWMA. Here we introduce the term velocity v which is used to denote the change in the gradient to get to the global minima. The change in the weights is denoted by the formula:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/0*i_r3u7LACa6dQyXd" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  the β part of the V formula denotes and is useful to compute the confidence or we can say the past velocity for calculating Vt we have to calculate Vt-1 and for calculating Vt-1 we have to calculate Vt-2 and likewise. So we are using the history of velocity to calculate the momentum and this is the part that provides acceleration to the formula.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*L5lNKxAHLPYNc6-Zs4Vscw.png" style="width: 300px; height: 100px;"/>
 </p>
 <p>
  Here we have to consider two cases:
1. β=0 then, as per the formula weight updating is going to just work as a Stochastic gradient descent. Here we called β a decaying factor because it is defining the speed of past velocity.
 </p>
 <ol>
  <li>
   β=1 then, there will be no decay. It involves the dynamic equilibrium which is not desired so we generally use the value of β like 0.9,0.99or 0.5 only.
  </li>
 </ol>
 <h3>
  Advantages of SGD with Momentum :
 </h3>
 <ol>
  <li>
   Momentum is faster than stochastic gradient descent the training will be faster than SGD.
  </li>
  <li>
   Local minima can be an escape and reach global minima due to the momentum involved.
  </li>
 </ol>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*Nb39bHHUWGXqgisr2WcLGQ.gif" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  Here in the video, we can see that purple is SGD with Momentum and light blue is for SGD the SGD with Momentum can reach global minima whereas SGD is stuck in local minima.
But there is a catch, the momentum itself can be a problem sometimes because of the high momentum after reaching global minima it is still fluctuating and take some time to get stable at global minima. And that kind of behavior leads to time consumption which makes SGD with Momentum slower than other optimization out there but still faster than SGD.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deep Ensembles</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gravity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Gravity is a kinematic approach to optimization based on gradients.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MAS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  This optimizer mix
  <a href="https://paperswithcode.com/method/adam">
   ADAM
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  creating the MAS optimizer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li>
            <details class="category depth2">
            <summary>Large Batch Optimization</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Momentum Rules</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Demon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Decaying Momentum
  </strong>
  , or
  <strong>
   Demon
  </strong>
  , is a stochastic optimizer motivated by decaying the total contribution of a gradient to all future updates. By decaying the momentum parameter, the total contribution of a gradient to all future updates is decayed. A particular gradient term $g_{t}$ contributes a total of  $\eta\sum_{i}\beta^{i}$ of its "energy" to all future gradient updates, and this results in the geometric sum, $\sum^{\infty}_{i=1}\beta^{i} = \beta\sum^{\infty}_{i=0}\beta^{i} = \frac{\beta}{\left(1-\beta\right)}$. Decaying this sum results in the Demon algorithm. Letting $\beta_{init}$ be the initial $\beta$; then at the current step $t$ with total $T$ steps, the decay routine is given by solving the below for $\beta_{t}$:
 </p>
 <p>
  $$ \frac{\beta_{t}}{\left(1-\beta_{t}\right)} =  \left(1-t/T\right)\beta_{init}/\left(1-\beta_{init}\right)$$
 </p>
 <p>
  Where $\left(1-t/T\right)$ refers to the proportion of iterations remaining. Note that Demon typically requires no hyperparameter tuning as it is usually decayed to $0$ or a small negative value at time 
$T$. Improved performance is observed by delaying the decaying. Demon can be applied to any gradient descent algorithm with a momentum parameter.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_9.45.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>3. Attention Mechanisms</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Attention Mechanisms
    </strong>
    are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/22f26a20-1608-449b-ab55-55d8136e097e.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Scaled Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Scaled dot-product attention
  </strong>
  is an attention mechanism where the dot products are scaled down by $\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:
 </p>
 <p>
  $$ {\text{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V $$
 </p>
 <p>
  If we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \cdot k = \sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\sqrt{d_k}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Strided Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixed Factorized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Inspired by the success of ResNet,
Wang et al. proposed
the very deep convolutional residual attention network (RAN) by 
combining an attention mechanism with residual connections.
 </p>
 <p>
  Each attention module stacked in a residual attention network 
can be divided into a mask branch and a trunk branch. 
The trunk branch processes features,
and can be implemented by any state-of-the-art structure
including a pre-activation residual unit and an inception block.
The mask branch uses a bottom-up top-down structure
to learn a mask of the same size that 
softly weights output features from the trunk branch. 
A sigmoid layer normalizes the output to $[0,1]$ after two $1\times 1$ convolution layers. Overall the residual attention mechanism can be written as
 </p>
 <p>
  \begin{align}
s &amp;= \sigma(Conv_{2}^{1\times 1}(Conv_{1}^{1\times 1}( h_\text{up}(h_\text{down}(X))))) 
\end{align}
 </p>
 <p>
  \begin{align}
X_{out} &amp;= s f(X) + f(X)
\end{align}
where $h_\text{up}$ is a bottom-up structure, 
using max-pooling several times after residual units
to increase the receptive field, while
$h_\text{down}$ is the top-down part using 
linear interpolation to keep the output size the 
same as the input feature map. 
There are also skip-connections between the two parts,
which are omitted from the formulation.
$f$ represents the trunk branch
which can be any state-of-the-art structure.
 </p>
 <p>
  Inside each attention module, a
bottom-up top-down feedforward structure models
both spatial and cross-channel dependencies, 
 leading to a consistent performance improvement. 
Residual attention can be incorporated into
any deep network structure in an end-to-end training fashion.
However, the proposed bottom-up top-down structure fails to leverage global spatial information.
  <br/>
  Furthermore, directly predicting a 3D attention map  has high computational cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/80f0362a-596a-465c-ab74-26d89c6e66f5.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dot-Product Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dot-Product Attention
  </strong>
  is an attention mechanism where the alignment score function is calculated as:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = h_{i}^{T}s_{j}$$
 </p>
 <p>
  It is equivalent to
  <a href="https://paperswithcode.com/method/multiplicative-attention">
   multiplicative attention
  </a>
  (without a trainable weight matrix, assuming this is instead an identity matrix). Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores/weights using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_12.32.09_PM_yYfmHYZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Temporal attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Temporal attention can be seen as a dynamic time selection mechanism determining when to pay attention, and is thus usually used for video processing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Additive Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Additive Attention
  </strong>
  , also known as
  <strong>
   Bahdanau Attention
  </strong>
  , uses a one-hidden layer feed-forward network to calculate the attention alignment score:
 </p>
 <p>
  $$f_{att}\left(\textbf{h}_{i}, \textbf{s}_{j}\right) = v_{a}^{T}\tanh\left(\textbf{W}_{a}\left[\textbf{h}_{i};\textbf{s}_{j}\right]\right)$$
 </p>
 <p>
  where $\textbf{v}_{a}$ and $\textbf{W}_{a}$ are learned attention parameters. Here $\textbf{h}$ refers to the hidden states for the encoder, and $\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function. We can use a matrix of alignment scores to show the correlation between source and target words, as the Figure to the right shows.
 </p>
 <p>
  Within a neural network, once we have the alignment scores, we calculate the final scores using a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  function of these alignment scores (ensuring it sums to 1).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.58.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  SENet pioneered channel attention. The core of SENet is a squeeze-and-excitation (SE) block which is used to collect global information, capture channel-wise relationships and improve representation ability.
SE blocks are divided into two parts, a squeeze module and an excitation module. Global spatial information is collected in the squeeze module by global average pooling. The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). Then, each channel of the input feature is scaled by multiplying the corresponding element in the attention vector. Overall, a squeeze-and-excitation block $F_\text{se}$ (with parameter $\theta$) which takes $X$ as input and outputs $Y$ can be formulated 
as:
\begin{align}
    s = F_\text{se}(X, \theta) &amp; = \sigma (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    Y = sX
\end{align}
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SPIN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9140bae0-27c1-4c07-8650-0acc229f18ed.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  RAM adopts RNNs and reinforcement learning (RL) to make the network learn where to pay attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global and Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FAVOR+</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FAVOR+
  </strong>
  , or
  <strong>
   Fast Attention Via Positive Orthogonal Random Features
  </strong>
  , is an efficient attention mechanism used in the
  <a href="https://paperswithcode.com/method/performer">
   Performer
  </a>
  architecture which leverages approaches such as kernel methods and random features approximation for approximating
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  and Gaussian kernels.
 </p>
 <p>
  FAVOR+ works for attention blocks using matrices $\mathbf{A} \in \mathbb{R}^{L×L}$ of the form $\mathbf{A}(i, j) = K(\mathbf{q}_{i}^{T}, \mathbf{k}_{j}^{T})$, with $\mathbf{q}_{i}/\mathbf{k}_{j}$ standing for the $i^{th}/j^{th}$ query/key row-vector in $\mathbf{Q}/\mathbf{K}$ and kernel $K : \mathbb{R}^{d } × \mathbb{R}^{d} \rightarrow \mathbb{R}_{+}$ defined for the (usually randomized) mapping: $\phi : \mathbb{R}^{d } → \mathbb{R}^{r}_{+}$ (for some $r &gt; 0$) as:
 </p>
 <p>
  $$K(\mathbf{x}, \mathbf{y}) = E[\phi(\mathbf{x})^{T}\phi(\mathbf{y})] $$
 </p>
 <p>
  We call $\phi(\mathbf{u})$ a random feature map for $\mathbf{u} \in \mathbb{R}^{d}$ . For $\mathbf{Q}^{'}, \mathbf{K}^{'} \in \mathbb{R}^{L \times r}$ with rows given as $\phi(\mathbf{q}_{i}^{T})^{T}$ and $\phi(\mathbf{k}_{i}^{T})^{T}$  respectively, this leads directly to the efficient attention mechanism of the form:
 </p>
 <p>
  $$ \hat{Att_{\leftrightarrow}}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \hat{\mathbf{D}}^{-1}(\mathbf{Q^{'}}((\mathbf{K^{'}})^{T}\mathbf{V}))$$
 </p>
 <p>
  where
 </p>
 <p>
  $$\mathbf{\hat{D}} = \text{diag}(\mathbf{Q^{'}}((\mathbf{K^{'}})\mathbf{1}_{L})) $$
 </p>
 <p>
  The above scheme constitutes the
  <a href="https://paperswithcode.com/method/dfa">
   FA
  </a>
  -part of the FAVOR+ mechanism. The other parts are achieved by:
 </p>
 <ul>
  <li>
   The R part :  The softmax kernel is approximated though trigonometric functions, in the form of a regularized softmax-kernel SMREG, that employs positive random features (PRFs).
  </li>
  <li>
   The OR+ part : To reduce the variance of the estimator, so we can use a smaller number of random features, different samples are entangled to be exactly orthogonal using the Gram-Schmidt orthogonalization procedure.
  </li>
 </ul>
 <p>
  The details are quite technical, so it is recommended you read the paper for further information on these steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.34.37_PM_JjR0C9D.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dynamic Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The extremely low computational cost of lightweight CNNs constrains the depth and width of the networks, further decreasing their representational power. To address the above problem, Chen et al. proposed dynamic convolution, a novel operator design that increases  representational power with negligible additional computational cost and does not change the width or depth of the network in parallel with CondConv.
 </p>
 <p>
  Dynamic convolution uses $K$ parallel convolution kernels of the same  size and input/output dimensions instead of one kernel per layer. Like SE blocks, it adopts a squeeze-and-excitation mechanism to generate the attention weights for the different convolution kernels. These kernels are then aggregated dynamically by weighted summation and applied to the input feature map $X$:
\begin{align}
    s &amp; = \text{softmax} (W_{2} \delta (W_{1}\text{GAP}(X)))
\end{align}
\begin{align}
    \text{DyConv} &amp;= \sum_{i=1}^{K} s_k \text{Conv}_k 
\end{align}
\begin{align}
    Y &amp;= \text{DyConv}(X)
\end{align}
Here the convolutions are combined by summation of weights and biases of convolutional kernels.
 </p>
 <p>
  Compared to applying convolution to the feature map, the computational cost of squeeze-and-excitation and weighted summation is extremely low. Dynamic convolution thus provides an efficient operation to improve  representational power and can be easily used as a replacement for any convolution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/76ecd4c4-4c2a-44c2-b248-a8cc2786f5a6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Disentangled Attention Mechanism</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Disentangled Attention Mechanism
  </strong>
  is an attention mechanism used in the
  <a href="https://paperswithcode.com/method/deberta">
   DeBERTa
  </a>
  architecture. Unlike
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  where each word in the input layer is represented using a vector which is the sum of its word (content) embedding and position embedding, each word in DeBERTa is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices based on their contents and relative positions, respectively. This is motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1cc628e2-a516-4871-8e5a-c420cf11614c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Axial Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Axial Attention
  </strong>
  is a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. It was first proposed in
  <a href="https://paperswithcode.com/method/ccnet">
   CCNet
  </a>
  [1] named as criss-cross attention, which harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Ho et al [2] extents CCNet to process multi-dimensional data.  The proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. It serves as the basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors, e.g., Axial Transformers. It has been applied in
  <a href="https://paperswithcode.com/method/alphafold">
   AlphaFold
  </a>
  [3] for interpreting protein sequences.
 </p>
 <p>
  [1] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu. CCNet: Criss-Cross Attention for Semantic Segmentation. ICCV, 2019.
 </p>
 <p>
  [2] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans. arXiv:1912.12180
 </p>
 <p>
  [3] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A, Bridgland A. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Jul 15:1-1.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cca_ANy2LjX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Attention Patterns</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The original
    <a href="https://paperswithcode.com/method/scaled">
     self-attention component
    </a>
    in the
    <a href="https://paperswithcode.com/method/transformer">
     Transformer
    </a>
    architecture has a $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Attention pattern methods look to reduce this complexity by looking at a subset of the space.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/strided.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Strided Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixed Factorized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global and Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Sliding Window Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Synthesized Attention Mechanisms</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Dense Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dense Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is a type of synthetic attention mechanism that replaces the notion of
  <a href="https://paperswithcode.com/method/scaled">
   query-key-values
  </a>
  in the self-attention module and directly synthesizes the alignment matrix instead. Dense attention is conditioned on each input token. The method accepts an input $X \in \mathbb{R}^{l\text{ x }d}$ and produces an output of $Y \in \mathbb{R}^{l\text{ x }d}$. Here $l$ refers to the sequence length and $d$ refers to the dimensionality of the model. We first adopt $F\left(.\right)$, a parameterized function, for projecting input $X_{i}$ from $d$ dimensions to $l$ dimensions.
 </p>
 <p>
  $$B_{i} = F\left(X_{i}\right)$$
 </p>
 <p>
  where $F\left(.\right)$ is a parameterized function that maps $\mathbb{R}^{d}$ to $\mathbb{R}^{l}$ and $i$ is the $i$-th token of $X$. Intuitively, this can be interpreted as learning a token-wise projection to the sequence length $l$. Essentially, with this model, each token predicts weights for each token in the input sequence. In practice, a simple two layered feed-forward layer with
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activations for $F\left(.\right)$ is adopted:
 </p>
 <p>
  $$ F\left(X\right) = W\left(\sigma_{R}\left(W(X) + b\right)\right) + b$$
 </p>
 <p>
  where $\sigma_{R}$ is the ReLU activation function. Hence, $B$ is now of $\mathbb{R}^{l\text{ x }d}$. Given $B$, we now compute:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(B\right)G\left(X\right) $$
 </p>
 <p>
  where $G\left(.\right)$ is another parameterized function of $X$ that is analogous to $V$ (value) in the standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model. This approach eliminates the
  <a href="https://paperswithcode.com/method/scaled">
   dot product
  </a>
  altogether by replacing $QK^{T}$ in standard Transformers with the synthesizing function $F\left(.\right)$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Synthesized Attention
  </strong>
  is a form of synthesized attention where the attention weights are not conditioned on any input tokens. Instead, the attention weights are initialized to random values. It was introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. Random Synthesized Attention contrasts with
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   Dense Synthesized Attention
  </a>
  which conditions on each token independently, as opposed to pairwise token interactions in the vanilla
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model.
 </p>
 <p>
  Let $R$ be a randomly initialized matrix. Random Synthesized Attention is defined as:
 </p>
 <p>
  $$Y = \text{Softmax}\left(R\right)G\left(X\right) $$
 </p>
 <p>
  where $R \in \mathbb{R}^{l \text{ x } l}$. Notably, each head adds 2 parameters to the overall network. The basic idea of the Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples. This is a direct generalization of the recently proposed fixed self-attention patterns of
  <a href="https://arxiv.org/abs/2002.10260">
   Raganato et al (2020)
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Factorized Random Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Random Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is similar to
  <a href="https://paperswithcode.com/method/factorized-dense-synthesized-attention">
   factorized dense synthesized attention
  </a>
  but for random synthesizers. Letting $R$ being a randomly initialized matrix, we factorize $R$ into low rank matrices $R_{1}, R_{2} \in \mathbb{R}^{l\text{ x}k}$ in the attention function:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(R_{1}R_{2}^{T}\right)G\left(X\right) . $$
 </p>
 <p>
  Here $G\left(.\right)$ is a parameterized function that is equivalent to $V$ in
  <a href="https://paperswithcode.com/method/scaled">
   Scaled Dot-Product Attention
  </a>
  .
 </p>
 <p>
  For each head, the factorization reduces the parameter costs from $l^{2}$ to $2\left(lk\right)$ where
$k &lt;&lt; l$ and hence helps prevent overfitting. In practice, we use a small value of $k = 8$.
 </p>
 <p>
  The basic idea of a  Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM_PkacRfG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Factorized Dense Synthesized Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Dense Synthesized Attention
  </strong>
  is a synthesized attention mechanism, similar to
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   dense synthesized attention
  </a>
  , but we factorize the outputs to reduce parameters and prevent overfitting. It was proposed as part of the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. The factorized variant of the dense synthesizer can be expressed as follows:
 </p>
 <p>
  $$A, B = F_{A}\left(X_{i}\right), F_{B}\left(X_{i}\right)$$
 </p>
 <p>
  where $F_{A}\left(.\right)$ projects input $X_{i}$ into $a$ dimensions, $F_B\left(.\right)$ projects $X_{i}$ to $b$ dimensions, and $a \text{ x } b = l$. The output of the factorized module is now written as:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(C\right)G\left(X\right) $$
 </p>
 <p>
  where $C = H_{A}\left(A\right) * H_{B}\left(B\right)$, where $H_{A}$, $H_{B}$ are tiling functions and $C \in \mathbb{R}^{l \text{ x } l}$. The tiling function simply duplicates the vector $k$ times, i.e., $\mathbb{R}^{l} \rightarrow \mathbb{R}^{lk}$. In this case, $H_{A}\left(\right)$ is a projection of $\mathbb{R}^{a} \rightarrow \mathbb{R}^{ab}$ and $H_{B}\left(\right)$ is a projection of $\mathbb{R}^{b} \rightarrow \mathbb{R}^{ba}$. To avoid having similar values within the same block, we compose the outputs of $H_{A}$ and $H_{B}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM_52J3Q9s.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>4. Activation Functions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Activation functions
    </strong>
    are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.49.45_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ReLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Rectified Linear Units
  </strong>
  , or
  <strong>
   ReLUs
  </strong>
  , are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with
  <a href="https://paperswithcode.com/method/sigmoid-activation">
   sigmoid activations
  </a>
  ), although for half of the real line its gradient is zero.
 </p>
 <p>
  $$ f\left(x\right) = \max\left(0, x\right) $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_1.47.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GELU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Gaussian Error Linear Unit
  </strong>
  , or
  <strong>
   GELU
  </strong>
  ,  is an activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their percentile, rather than gates inputs by their sign as in
  <a href="https://paperswithcode.com/method/relu">
   ReLUs
  </a>
  ($x\mathbf{1}_{x&gt;0}$). Consequently the GELU can be thought of as a smoother ReLU.
 </p>
 <p>
  $$\text{GELU}\left(x\right) = x{P}\left(X\leq{x}\right) = x\Phi\left(x\right) = x \cdot \frac{1}{2}\left[1 + \text{erf}(x/\sqrt{2})\right],$$
if $X\sim \mathcal{N}(0,1)$.
 </p>
 <p>
  One can approximate the GELU with
$0.5x\left(1+\tanh\left[\sqrt{2/\pi}\left(x + 0.044715x^{3}\right)\right]\right)$ or $x\sigma\left(1.702x\right),$
but PyTorch's exact implementation is sufficiently fast such that these approximations may be unnecessary. (See also the
  <a href="https://paperswithcode.com/method/silu">
   SiLU
  </a>
  $x\sigma(x)$ which was also coined in the paper that introduced the GELU.)
 </p>
 <p>
  GELUs are used in
  <a href="https://paperswithcode.com/method/gpt-3">
   GPT-3
  </a>
  ,
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  , and most other Transformers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.48.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sigmoid Activation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sigmoid Activations
  </strong>
  are a type of activation function for neural networks:
 </p>
 <p>
  $$f\left(x\right) = \frac{1}{\left(1+\exp\left(-x\right)\right)}$$
 </p>
 <p>
  Some drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1200px-Logistic-curve.svg_VXkoEDF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tanh Activation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tanh Activation
  </strong>
  is an activation function used for neural networks:
 </p>
 <p>
  $$f\left(x\right) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
 </p>
 <p>
  Historically, the tanh function became preferred over the
  <a href="https://paperswithcode.com/method/sigmoid-activation">
   sigmoid function
  </a>
  as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activations.
 </p>
 <p>
  Image Source:
  <a href="https://www.researchgate.net/profile/Junxi_Feng">
   Junxi Feng
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Leaky ReLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Leaky Rectified Linear Unit
  </strong>
  , or
  <strong>
   Leaky ReLU
  </strong>
  , is a type of activation function based on a
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  , but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Gated Linear Unit
  </strong>
  , or
  <strong>
   GLU
  </strong>
  computes:
 </p>
 <p>
  $$ \text{GLU}\left(a, b\right) = a\otimes \sigma\left(b\right) $$
 </p>
 <p>
  It is used in natural language processing architectures, for example the
  <a href="https://paperswithcode.com/method/gated-convolution-network">
   Gated CNN
  </a>
  , because here $b$ is the gate that control what information from $a$ is passed up to the following layer. Intuitively, for a language modeling task, the gating mechanism allows selection of words or features that are important for predicting the next word. The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_architecture_8UYjVkL.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Swish</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Swish
  </strong>
  is an activation function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, where $\beta$ a learnable parameter. Nearly all implementations do not use the learnable parameter $\beta$, in which case the activation function is $x\sigma(x)$ ("Swish-1").
 </p>
 <p>
  The function $x\sigma(x)$ is exactly the
  <a href="https://paperswithcode.com/method/silu">
   SiLU
  </a>
  , which was introduced by other authors before the swish.
See
  <a href="https://arxiv.org/abs/1606.08415">
   Gaussian Error Linear Units
  </a>
  (
  <a href="https://paperswithcode.com/method/gelu">
   GELUs
  </a>
  ) where the SiLU (Sigmoid Linear Unit) was originally coined, and see
  <a href="https://arxiv.org/abs/1702.03118">
   Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning
  </a>
  and
  <a href="https://arxiv.org/abs/1710.05941v1">
   Swish: a Self-Gated Activation Function
  </a>
  where the same activation function was experimented with later.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_2.02.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Softplus</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Softplus
  </strong>
  is an activation function $f\left(x\right) = \log\left(1+\exp\left(x\right)\right)$. It can be viewed as a smooth version of
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_2.07.07_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SELU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Scaled Exponential Linear Units
  </strong>
  , or
  <strong>
   SELUs
  </strong>
  , are activation functions that induce self-normalizing properties.
 </p>
 <p>
  The SELU activation function is given by
 </p>
 <p>
  $$f\left(x\right) = \lambda{x} \text{ if } x \geq{0}$$
$$f\left(x\right) = \lambda{\alpha\left(\exp\left(x\right) -1 \right)} \text{ if } x &lt; 0 $$
 </p>
 <p>
  with $\alpha \approx 1.6733$ and $\lambda \approx 1.0507$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_secondMomentMainSubfunctionPlot.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mish</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mish
  </strong>
  is an activation function for neural networks which can be defined as:
 </p>
 <p>
  $$ f\left(x\right) = x\cdot\tanh{\text{softplus}\left(x\right)}$$
 </p>
 <p>
  where
 </p>
 <p>
  $$\text{softplus}\left(x\right) = \ln\left(1+e^{x}\right)$$
 </p>
 <p>
  (Compare with functionally similar previously proposed activation functions such as the
  <a href="https://paperswithcode.com/method/silu">
   GELU
  </a>
  $x\Phi(x)$ and the
  <a href="https://paperswithcode.com/method/silu">
   SiLU
  </a>
  $x\sigma(x)$.)
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/relu-mish-comparison-2_lC90syt.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PReLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Parametric Rectified Linear Unit
  </strong>
  , or
  <strong>
   PReLU
  </strong>
  , is an activation function that generalizes the traditional rectified unit with a slope for negative values. Formally:
 </p>
 <p>
  $$f\left(y_{i}\right) = y_{i} \text{ if } y_{i} \ge 0$$
$$f\left(y_{i}\right) = a_{i}y_{i} \text{ if } y_{i} \leq 0$$
 </p>
 <p>
  The intuition is that different layers may require different types of nonlinearity. Indeed the authors find in experiments with convolutional neural networks that PReLus for the initial layer have more positive slopes, i.e. closer to linear. Since the filters of the first layers are Gabor-like filters such as edge or texture detectors, this shows a circumstance where positive and negative responses of filters are respected. In contrast the authors find deeper layers have smaller coefficients, suggesting the model becomes more discriminative at later layers (while it wants to retain more information at earlier layers).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_prelu.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ReLU6</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ReLU6
  </strong>
  is a modification of the
  <a href="https://paperswithcode.com/method/relu">
   rectified linear unit
  </a>
  where we limit the activation to a maximum size of $6$. This is due to increased robustness when used with low-precision computation.
 </p>
 <p>
  Image Credit:
  <a href="https://pytorch.org/docs/master/generated/torch.nn.ReLU6.html">
   PyTorch
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ReLU61_jm4UDCx.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Hard Swish</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hard Swish
  </strong>
  is a type of activation function based on
  <a href="https://paperswithcode.com/method/swish">
   Swish
  </a>
  , but replaces the computationally expensive sigmoid with a piecewise linear analogue:
 </p>
 <p>
  $$\text{h-swish}\left(x\right) = x\frac{\text{ReLU6}\left(x+3\right)}{6} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.10.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FEM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Adaptive Activation Functions</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Adaptive or trainable activation functions are the functions with trainable parameters that are able to adapt (change, optimize) their shape and amplitude to the target dataset.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Swish</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Swish
  </strong>
  is an activation function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, where $\beta$ a learnable parameter. Nearly all implementations do not use the learnable parameter $\beta$, in which case the activation function is $x\sigma(x)$ ("Swish-1").
 </p>
 <p>
  The function $x\sigma(x)$ is exactly the
  <a href="https://paperswithcode.com/method/silu">
   SiLU
  </a>
  , which was introduced by other authors before the swish.
See
  <a href="https://arxiv.org/abs/1606.08415">
   Gaussian Error Linear Units
  </a>
  (
  <a href="https://paperswithcode.com/method/gelu">
   GELUs
  </a>
  ) where the SiLU (Sigmoid Linear Unit) was originally coined, and see
  <a href="https://arxiv.org/abs/1702.03118">
   Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning
  </a>
  and
  <a href="https://arxiv.org/abs/1710.05941v1">
   Swish: a Self-Gated Activation Function
  </a>
  where the same activation function was experimented with later.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_2.02.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LEAF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>modReLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   modReLU
  </strong>
  is an activation that is a modification of a
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  . It is a pointwise nonlinearity, $\sigma_{modReLU}\left(z\right) : C \rightarrow C$, which affects only the absolute value of a complex number, defined as:
 </p>
 <p>
  $$ \sigma_{modReLU}\left(z\right) = \left(|z| + b\right)\frac{z}{|z|} \text{ if } |z| + b \geq 0 $$
$$ \sigma_{modReLU}\left(z\right) = 0 \text{ if } |z| + b \leq 0 $$
 </p>
 <p>
  where $b \in \mathbb{R}$ is a bias parameter of the nonlinearity. For a $n_{h}$ dimensional hidden space we learn $n_{h}$ nonlinearity bias parameters, one per dimension.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.19.15_PM_pWVAtEh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Rational Activation function</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>5. Stochastic Optimization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Stochastic Optimization
    </strong>
    methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.50.29_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Stochastic Gradient Descent
  </strong>
  is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:
 </p>
 <p>
  $$ w_{t+1} = w_{t} - \eta\hat{\nabla}_{w}{L(w_{t})} $$
 </p>
 <p>
  Where $\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.
 </p>
 <p>
  (Image Source:
  <a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_2.57.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RMSProp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RMSProp
  </strong>
  is an unpublished adaptive learning rate optimizer
  <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
   proposed by Geoff Hinton
  </a>
  . The motivation is that the magnitude of gradients can differ for different weights, and can change during learning, making it hard to choose a single global learning rate. RMSProp tackles this by keeping a moving average of the squared gradient and adjusting the weight updates by this magnitude. The gradient updates are performed as:
 </p>
 <p>
  $$E\left[g^{2}\right]_{t} = \gamma E\left[g^{2}\right]_{t-1} + \left(1 - \gamma\right) g^{2}_{t}$$
 </p>
 <p>
  $$\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E\left[g^{2}\right]_{t} + \epsilon}}g_{t}$$
 </p>
 <p>
  Hinton suggests $\gamma=0.9$, with a good default for $\eta$ as $0.001$.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_3.16.15_PM_kjTCskF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdamW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdamW
  </strong>
  is a stochastic optimization method that modifies the typical implementation of weight decay in
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  , by decoupling
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  from the gradient update. To see this, $L_{2}$ regularization in Adam is usually implemented with the below modification where $w_{t}$ is the rate of the weight decay at time $t$:
 </p>
 <p>
  $$ g_{t} = \nabla{f\left(\theta_{t}\right)} + w_{t}\theta_{t}$$
 </p>
 <p>
  while AdamW adjusts the weight decay term to appear in the gradient update:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \eta\left(\frac{1}{\sqrt{\hat{v}_{t} + \epsilon}}\cdot{\hat{m}_{t}} + w_{t, i}\theta_{t, i}\right), \forall{t}$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_8.27.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SGD with Momentum</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <h3>
  Why SGD with  Momentum?
 </h3>
 <p>
  In deep learning, we have used stochastic gradient descent as one of the optimizers because at the end we will find the minimum weight and bias at which the model loss is lowest. In the SGD we have some issues in which the SGD does not work perfectly because in deep learning we got a non-convex cost function graph and if use the simple SGD then it leads to low performance. There are 3 main reasons why it does not work:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  1) We end up in local minima and not able to reach global minima
At the start, we randomly start at some point and we are going to end up at the local minimum and not able to reach the global minimum.
 </p>
 <p>
  2) Saddle Point will be the stop for reaching global minima
A saddle point is a point where in one direction the surface goes in the upward direction and in another direction it goes downwards. So that the slope is changing very gradually so the speed of changing is going to slow and as result, the training also going to slow.
 </p>
 <p>
  3) High curvature can be a reason
The larger radius leads to low curvature and vice-versa. It will be difficult to traverse in the large curvature which was generally high in non-convex optimization.
By using the SGD with Momentum optimizer we can overcome the problems like high curvature, consistent gradient, and noisy gradient.
 </p>
 <h3>
  What is SGD with Momentum?
 </h3>
 <p>
  SGD with  Momentum is one of the optimizers which is used to improve the performance of the neural network.
 </p>
 <p>
  Let's take an example and understand the intuition behind the optimizer suppose we have a ball which is sliding from the start of the slope as it goes the speed of the bowl is increased over time. If we have one point A and we want to reach point B and we don't know in which direction to move but we ask for the 4 points which have already reached point B. If all 4 points are pointing you in the same direction then the confidence of the A is more and it goes in the direction pointed very fast. This is the main concept behind the SGD with Momentum.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*zNbZqU_uDIV13c9ZCJOEXA.jpeg" style="width: 400px; height: 250px;"/>
 </p>
 <h3>
  How does SGD with Momentum work?
 </h3>
 <p>
  So first to understand the concept of exponentially weighted moving average (EWMA). It was a technique through which try to find the trend in time series data. The formula of the EWMA is :
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*O9Wcq-mbRgNOdRNTivSefw.png" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  In the formula, β represents the weightage that is going to assign to the past values of the gradient. The values of β is from 0 &lt; β &lt; 1. If the value of the beta is 0.5 then it means that the 1/1–0.5 = 2 so it represents that the calculated average was from the previous 2 readings.
 </p>
 <p>
  The value of Vt depends on β. The higher the value of β the more we try to get an average of more past data and vice-versa. For example, let's take the value of β 0.98 and 0.5 for two different scenarios so if we do 1/1-β then we get 50 and 10 respectively so it was clear that to calculate the average we take past 50 and 10 outcomes respectively for both cases.
Now in SGD with Momentum, we use the same concept of EWMA. Here we introduce the term velocity v which is used to denote the change in the gradient to get to the global minima. The change in the weights is denoted by the formula:
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/0*i_r3u7LACa6dQyXd" style="width: 400px; height: 100px;"/>
 </p>
 <p>
  the β part of the V formula denotes and is useful to compute the confidence or we can say the past velocity for calculating Vt we have to calculate Vt-1 and for calculating Vt-1 we have to calculate Vt-2 and likewise. So we are using the history of velocity to calculate the momentum and this is the part that provides acceleration to the formula.
 </p>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*L5lNKxAHLPYNc6-Zs4Vscw.png" style="width: 300px; height: 100px;"/>
 </p>
 <p>
  Here we have to consider two cases:
1. β=0 then, as per the formula weight updating is going to just work as a Stochastic gradient descent. Here we called β a decaying factor because it is defining the speed of past velocity.
 </p>
 <ol>
  <li>
   β=1 then, there will be no decay. It involves the dynamic equilibrium which is not desired so we generally use the value of β like 0.9,0.99or 0.5 only.
  </li>
 </ol>
 <h3>
  Advantages of SGD with Momentum :
 </h3>
 <ol>
  <li>
   Momentum is faster than stochastic gradient descent the training will be faster than SGD.
  </li>
  <li>
   Local minima can be an escape and reach global minima due to the momentum involved.
  </li>
 </ol>
 <p>
  <img alt="Non-convex graph" src="https://cdn-images-1.medium.com/max/1000/1*Nb39bHHUWGXqgisr2WcLGQ.gif" style="width: 400px; height: 300px;"/>
 </p>
 <p>
  Here in the video, we can see that purple is SGD with Momentum and light blue is for SGD the SGD with Momentum can reach global minima whereas SGD is stuck in local minima.
But there is a catch, the momentum itself can be a problem sometimes because of the high momentum after reaching global minima it is still fluctuating and take some time to get stable at global minima. And that kind of behavior leads to time consumption which makes SGD with Momentum slower than other optimization out there but still faster than SGD.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deep Ensembles</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gravity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Gravity is a kinematic approach to optimization based on gradients.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MAS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  This optimizer mix
  <a href="https://paperswithcode.com/method/adam">
   ADAM
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  creating the MAS optimizer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Large Batch Optimization</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Momentum Rules</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Demon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Decaying Momentum
  </strong>
  , or
  <strong>
   Demon
  </strong>
  , is a stochastic optimizer motivated by decaying the total contribution of a gradient to all future updates. By decaying the momentum parameter, the total contribution of a gradient to all future updates is decayed. A particular gradient term $g_{t}$ contributes a total of  $\eta\sum_{i}\beta^{i}$ of its "energy" to all future gradient updates, and this results in the geometric sum, $\sum^{\infty}_{i=1}\beta^{i} = \beta\sum^{\infty}_{i=0}\beta^{i} = \frac{\beta}{\left(1-\beta\right)}$. Decaying this sum results in the Demon algorithm. Letting $\beta_{init}$ be the initial $\beta$; then at the current step $t$ with total $T$ steps, the decay routine is given by solving the below for $\beta_{t}$:
 </p>
 <p>
  $$ \frac{\beta_{t}}{\left(1-\beta_{t}\right)} =  \left(1-t/T\right)\beta_{init}/\left(1-\beta_{init}\right)$$
 </p>
 <p>
  Where $\left(1-t/T\right)$ refers to the proportion of iterations remaining. Note that Demon typically requires no hyperparameter tuning as it is usually decayed to $0$ or a small negative value at time 
$T$. Improved performance is observed by delaying the decaying. Demon can be applied to any gradient descent algorithm with a momentum parameter.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_9.45.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>6. Regularization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/dropout.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Dropout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dropout
  </strong>
  is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).
 </p>
 <p>
  The idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Label Smoothing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Label Smoothing
  </strong>
  is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\log{p}\left(y\mid{x}\right)$ directly can be harmful. Assume for a small constant $\epsilon$, the training set label $y$ is correct with probability $1-\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\frac{\epsilon}{k-1}$ and $1-\epsilon$ respectively.
 </p>
 <p>
  Source: Deep Learning, Goodfellow et al
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/abs/1906.02629">
   When Does Label Smoothing Help?
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/image3_1_oTiwmLN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Attention Dropout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Attention Dropout
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  used in attention-based architectures, where elements are randomly dropped out of the
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  in the attention equation. For example, for scaled-dot product attention, we would drop elements from the first term:
 </p>
 <p>
  $$ {\text{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM_vn04Hx7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Weight Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Decay
  </strong>
  , or
  <strong>
   $L_{2}$ Regularization
  </strong>
  , is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{w^{T}w}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty (encouraging smaller weights).
 </p>
 <p>
  Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).
 </p>
 <p>
  Image Source: Deep Learning, Goodfellow et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.15.13_PM_YGbJW74.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Entropy Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Entropy Regularization
  </strong>
  is a type of regularization used in
  <a href="https://paperswithcode.com/methods/area/reinforcement-learning">
   reinforcement learning
  </a>
  . For on-policy policy gradient based methods like
  <a href="https://paperswithcode.com/method/a3c">
   A3C
  </a>
  , the same mutual  reinforcement behaviour leads to a highly-peaked $\pi\left(a\mid{s}\right)$ towards a few actions or action sequences, since it is easier for the actor and critic to overoptimise to a small portion of the environment. To reduce this problem, entropy regularization adds an entropy term to the loss to promote action diversity:
 </p>
 <p>
  $$H(X) = -\sum\pi\left(x\right)\log\left(\pi\left(x\right)\right) $$
 </p>
 <p>
  Image Credit: Wikipedia
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Binary_entropy_plot.svg.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>R1 Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   R_INLINE_MATH_1 Regularization
  </strong>
  is a regularization technique and gradient penalty for training
  <a href="https://paperswithcode.com/methods/category/generative-adversarial-networks">
   generative adversarial networks
  </a>
  . It penalizes the discriminator from deviating from the Nash Equilibrium via penalizing the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  game.
 </p>
 <p>
  This leads to the following regularization term:
 </p>
 <p>
  $$ R_{1}\left(\psi\right) = \frac{\gamma}{2}E_{p_{D}\left(x\right)}\left[||\nabla{D_{\psi}\left(x\right)}||^{2}\right] $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_11.03.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Early Stopping</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Early Stopping
  </strong>
  is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence, we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space.
 </p>
 <p>
  Image Source:
  <a href="https://www.researchgate.net/figure/Early-stopping-based-on-cross-validation_fig1_3302948">
   Ramazan Gençay
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.59.56_PM_1D7lrVF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Stochastic Depth</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Stochastic Depth
  </strong>
  aims to shrink the depth of a network during training, while
keeping it unchanged during testing. This is achieved by randomly dropping entire
  <a href="https://paperswithcode.com/method/residual-block">
   ResBlocks
  </a>
  during training and bypassing their transformations through skip connections.
 </p>
 <p>
  Let $b_{l} \in$ {$0, 1$} denote a Bernoulli random variable, which indicates whether the $l$th ResBlock is active ($b_{l} = 1$) or inactive ($b_{l} = 0$). Further, let us denote the “survival” probability of ResBlock $l$ as $p_{l} = \text{Pr}\left(b_{l} = 1\right)$. With this definition we can bypass the $l$th ResBlock by multiplying its function $f_{l}$ with $b_{l}$ and we extend the update rule to:
 </p>
 <p>
  $$ H_{l} = \text{ReLU}\left(b_{l}f_{l}\left(H_{l-1}\right) + \text{id}\left(H_{l-1}\right)\right) $$
 </p>
 <p>
  If $b_{l} = 1$, this reduces to the original
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  update and this ResBlock remains unchanged. If $b_{l} = 0$, the ResBlock reduces to the identity function, $H_{l} = \text{id}\left((H_{l}−1\right)$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_12.42.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Path Length Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Path Length Regularization
  </strong>
  is a type of regularization for
  <a href="https://paperswithcode.com/methods/category/generative-adversarial-networks">
   generative adversarial networks
  </a>
  that encourages good conditioning in the mapping from latent codes to images. The idea is to encourage that a fixed-size step in the latent space $\mathcal{W}$ results in a non-zero, fixed-magnitude change in the image.
 </p>
 <p>
  We can measure the deviation from this ideal empirically by stepping into random directions in the image space and observing the corresponding $\mathbf{w}$ gradients. These gradients should have close to an equal length regardless of $\mathbf{w}$ or the image-space direction, indicating that the mapping from the latent space to image space is well-conditioned.
 </p>
 <p>
  At a single $\mathbf{w} \in \mathcal{W}$ the local metric scaling properties of the generator mapping $g\left(\mathbf{w}\right) : \mathcal{W} \rightarrow \mathcal{Y}$ are captured by the Jacobian matrix $\mathbf{J_{w}} = \delta{g}\left(\mathbf{w}\right)/\delta{\mathbf{w}}$. Motivated by the desire to preserve the expected lengths of vectors regardless of the direction, we formulate the regularizer as:
 </p>
 <p>
  $$ \mathbb{E}_{\mathbf{w},\mathbf{y} \sim \mathcal{N}\left(0, \mathbf{I}\right)} \left(||\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}||_{2} - a\right)^{2} $$
 </p>
 <p>
  where $y$ are random images with normally distributed pixel intensities, and $w \sim f\left(z\right)$, where $z$ are normally distributed.
 </p>
 <p>
  To avoid explicit computation of the Jacobian matrix, we use the identity $\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y} = \nabla_{\mathbf{w}}\left(g\left(\mathbf{w}\right)·y\right)$, which is efficiently computable using standard backpropagation. The constant $a$ is set dynamically during optimization as the long-running exponential moving average of the lengths $||\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}||_{2}$, allowing the optimization to find a suitable global scale by itself.
 </p>
 <p>
  The authors note that they find that path length regularization leads to more reliable and consistently behaving models, making architecture exploration easier. They also observe that the smoother generator is significantly easier to invert.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_10.26.15_PM_bQYhAg8.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Variational Dropout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Variational Dropout
  </strong>
  is a regularization technique based on
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  , but uses a variational inference grounded approach. In Variational Dropout, we repeat the same dropout mask at each time step for both inputs, outputs, and recurrent layers (drop the same network units at each time step). This is in contrast to ordinary Dropout where different dropout masks are sampled at each time step for the inputs and outputs alone.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_6.25.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Off-Diagonal Orthogonal Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Off-Diagonal Orthogonal Regularization
  </strong>
  is a modified form of
  <a href="https://paperswithcode.com/method/orthogonal-regularization">
   orthogonal regularization
  </a>
  originally used in
  <a href="https://paperswithcode.com/method/biggan">
   BigGAN
  </a>
  . The original orthogonal regularization is known to be limiting so the authors explore several variants designed to relax the constraint while still imparting the desired smoothness to the models. They opt for a modification where they remove diagonal terms from the regularization, and aim to minimize the pairwise cosine similarity between filters but does not constrain their norm:
 </p>
 <p>
  $$ R_{\beta}\left(W\right) = \beta|| W^{T}W \odot \left(\mathbf{1}-I\right) ||^{2}_{F} $$
 </p>
 <p>
  where $\mathbf{1}$ denotes a matrix with all elements set to 1. The authors sweep $\beta$ values and select $10^{−4}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-03_at_4.20.38_PM_EmKu2bY.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DropBlock</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DropBlock
  </strong>
  is a structured form of
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  directed at regularizing convolutional networks. In DropBlock, units in a contiguous region of a feature map are dropped together.  As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_12.36.48_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Target Policy Smoothing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Target Policy Smoothing
  </strong>
  is a regularization strategy for the value function in reinforcement learning. Deterministic policies can overfit to narrow peaks in the value estimate, making them highly susceptible to functional approximation error, increasing the variance of the target. To reduce this variance, target policy smoothing adds a small amount of random noise to the target policy and averages over mini-batches - approximating a
  <a href="https://paperswithcode.com/method/sarsa">
   SARSA
  </a>
  -like expectation/integral.
 </p>
 <p>
  The modified target update is:
 </p>
 <p>
  $$ y = r + \gamma{Q}_{\theta'}\left(s', \pi_{\theta'}\left(s'\right) + \epsilon \right) $$
 </p>
 <p>
  $$ \epsilon \sim \text{clip}\left(\mathcal{N}\left(0, \sigma\right), -c, c \right) $$
 </p>
 <p>
  where the added noise is clipped to keep the target close to the original action. The outcome is an algorithm reminiscent of
  <a href="https://paperswithcode.com/method/expected-sarsa">
   Expected SARSA
  </a>
  , where the value estimate is instead learned off-policy and the noise added to the target policy is chosen independently of the exploration policy. The value estimate learned is with respect to a noisy policy defined by the parameter $\sigma$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-05_at_1.44.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DropConnect</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DropConnect
  </strong>
  generalizes
  <a href="https://paperswithcode.com/method/dropout">
   Dropout
  </a>
  by randomly dropping the weights rather than the activations with probability $1-p$. DropConnect is similar to Dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights $W$, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. Note that this is not equivalent to setting $W$ to be a fixed sparse matrix during training.
 </p>
 <p>
  For a DropConnect layer, the output is given as:
 </p>
 <p>
  $$ r = a \left(\left(M * W\right){v}\right)$$
 </p>
 <p>
  Here $r$ is the output of a layer, $v$ is the input to a layer, $W$ are weight parameters, and $M$ is a binary matrix encoding the connection information where $M_{ij} \sim \text{Bernoulli}\left(p\right)$. Each element of the mask $M$ is drawn independently for each example during training, essentially instantiating a different connectivity for each example seen. Additionally, the biases are also masked out during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_6.03.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>L1 Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   $L_{1}$ Regularization
  </strong>
  is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{1}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{||w||}_{1}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty. In contrast to
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  , $L_{1}$ regularization promotes sparsity; i.e. some parameters have an optimal value of zero.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Sparsityl1.png">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.02.16_AM_iDsext7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Embedding Dropout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embedding Dropout
  </strong>
  is equivalent to performing
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  on the embedding matrix at a word level, where the dropout is broadcast across all the word vector’s embedding. The remaining non-dropped-out word embeddings are scaled by $\frac{1}{1-p_{e}}$ where $p_{e}$ is the probability of embedding dropout. As the dropout occurs on the embedding matrix that is used for a full forward and backward pass, this means that all occurrences of a specific word will disappear within that pass, equivalent to performing
  <a href="https://paperswithcode.com/method/variational-dropout">
   variational dropout
  </a>
  on the connection between the one-hot embedding and the embedding lookup.
 </p>
 <p>
  Source: Merity et al, Regularizing and Optimizing
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  Language Models
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_6.31.52_PM_cxlhtbJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ALS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PGM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A regularization criterion that, differently from
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  and its variants, is deterministic rather than random. It grounds on the empirical evidence that feature descriptors with larger L2-norm and highly-active nodes are strongly correlated to confident class predictions. Thus, the criterion guides towards dropping a percentage of the most active nodes of the descriptors, proportionally to the estimated class probability
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/method_aZ5K90U.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Parameter Norm Penalties</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Parameter Norm Penalties
    </strong>
    are regularization methods that apply a penalty to the norm of parameters in the objective function of a neural network. Below you can find a continuously updating list of parameter norm penalties.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Weight Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Decay
  </strong>
  , or
  <strong>
   $L_{2}$ Regularization
  </strong>
  , is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{w^{T}w}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty (encouraging smaller weights).
 </p>
 <p>
  Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).
 </p>
 <p>
  Image Source: Deep Learning, Goodfellow et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.15.13_PM_YGbJW74.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>L1 Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   $L_{1}$ Regularization
  </strong>
  is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{1}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{||w||}_{1}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty. In contrast to
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  , $L_{1}$ regularization promotes sparsity; i.e. some parameters have an optimal value of zero.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Sparsityl1.png">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.02.16_AM_iDsext7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ROME</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>7. Skip Connection Blocks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Skip Connection Blocks
    </strong>
    are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.49.06_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Blocks
  </strong>
  are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  architecture.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}({x}):=\mathcal{H}({x})-{x}$. The original mapping is recast into $\mathcal{F}({x})+{x}$. The $\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.
 </p>
 <p>
  The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.
 </p>
 <p>
  Note that in practice,
  <a href="https://paperswithcode.com/method/bottleneck-residual-block">
   Bottleneck Residual Blocks
  </a>
  are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/resnet-e1548261477164_2_mD02h5A.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bottleneck Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Bottleneck Residual Block
  </strong>
  is a variant of the
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.12.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inverted Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An
  <strong>
   Inverted Residual Block
  </strong>
  , sometimes called an
  <strong>
   MBConv Block
  </strong>
  , is a type of residual block used for image models that uses an inverted structure for efficiency reasons. It was originally proposed for the
  <a href="https://paperswithcode.com/method/mobilenetv2">
   MobileNetV2
  </a>
  CNN architecture. It has since been reused for several mobile-optimized CNNs.
 </p>
 <p>
  A traditional
  <a href="https://paperswithcode.com/method/residual-block">
   Residual Block
  </a>
  has a wide -&gt; narrow -&gt; wide structure with the number of channels. The input has a high number of channels, which are compressed with a
  <a href="https://paperswithcode.com/method/1x1-convolution">
   1x1 convolution
  </a>
  . The number of channels is then increased again with a 1x1
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  so input and output can be added.
 </p>
 <p>
  In contrast, an Inverted Residual Block follows a narrow -&gt; wide -&gt; narrow approach, hence the inversion. We first widen with a 1x1 convolution, then use a 3x3
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolution
  </a>
  (which greatly reduces the number of parameters), then we use a 1x1 convolution to reduce the number of channels so input and output can be added.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.08.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dense Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Dense Block
  </strong>
  is a module used in convolutional neural networks that connects
  <em>
   all layers
  </em>
  (with matching feature-map sizes) directly with each other. It was originally proposed as part of the
  <a href="https://paperswithcode.com/method/densenet">
   DenseNet
  </a>
  architecture. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. In contrast to
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  , we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the $\ell^{th}$ layer has $\ell$ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all $L-\ell$ subsequent layers. This introduces $\frac{L(L+1)}{2}$  connections in an $L$-layer network, instead of just $L$, as in traditional architectures: "dense connectivity".
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.33.17_PM_Mt0HOZL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Non-Local Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Non-Local Block
  </strong>
  is an image block module used in neural networks that wraps a
  <a href="https://paperswithcode.com/method/non-local-operation">
   non-local operation
  </a>
  . We can define a non-local block as:
 </p>
 <p>
  $$ \mathbb{z}_{i} = W_{z}\mathbb{y_{i}} + \mathbb{x}_{i} $$
 </p>
 <p>
  where $y_{i}$ is the output from the non-local operation and $+ \mathbb{x}_{i}$ is a
  <a href="https://paperswithcode.com/method/residual-connection">
   residual connection
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_5.20.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ResNeXt Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   ResNeXt Block
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  used as part of the
  <a href="https://paperswithcode.com/method/resnext">
   ResNeXt
  </a>
  CNN architecture. It uses a "split-transform-merge" strategy (branched paths within a single module) similar to an
  <a href="https://paperswithcode.com/method/inception-module">
   Inception module
  </a>
  , i.e. it aggregates a set of transformations. Compared to a Residual Block, it exposes a new dimension,
  <em>
   cardinality
  </em>
  (size of set of transformations) $C$, as an essential factor in addition to depth and width.
 </p>
 <p>
  Formally, a set of aggregated transformations can be represented as: $\mathcal{F}(x)=\sum_{i=1}^{C}\mathcal{T}_i(x)$, where $\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM_iXtkYE5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CBHG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CBHG
  </strong>
  is a building block used in the
  <a href="https://paperswithcode.com/method/tacotron">
   Tacotron
  </a>
  text-to-speech model. It consists of a bank of 1-D convolutional filters, followed by highway networks and a bidirectional gated recurrent unit (
  <a href="https://paperswithcode.com/method/bigru">
   BiGRU
  </a>
  ).
 </p>
 <p>
  The module is used to extract representations from sequences. The input sequence is first
convolved with $K$ sets of 1-D convolutional filters, where the $k$-th set contains $C_{k}$ filters of width $k$ (i.e. $k = 1, 2, \dots , K$). These filters explicitly model local and contextual information (akin to modeling unigrams, bigrams, up to K-grams). The
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  outputs are stacked together and further max pooled along time to increase local invariances. A stride of 1 is used to  preserve the original time resolution. The processed sequence is further passed to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections.
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch normalization
  </a>
  is used for all convolutional layers. The convolution outputs are fed into a multi-layer
  <a href="https://paperswithcode.com/method/highway-network">
   highway network
  </a>
  to extract high-level features. Finally, a bidirectional
  <a href="https://paperswithcode.com/method/gru">
   GRU
  </a>
  RNN is stacked on top to extract sequential features from both forward and backward context.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_9.50.03_PM_1fzwGwI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Wide Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Wide Residual Block
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  that utilises two conv 3x3 layers (with
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  ). This is wider than other variants of residual blocks (for instance
  <a href="https://paperswithcode.com/method/bottleneck-residual-block">
   bottleneck residual blocks
  </a>
  ). It was proposed as part of the
  <a href="https://paperswithcode.com/method/wideresnet">
   WideResNet
  </a>
  CNN architecture.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.18.54_PM_cxmFnC3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>8. Self-Supervised Learning</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Self-Supervised Learning
    </strong>
    refers to a category of methods where we learn representations in a self-supervised way (i.e without labels). These methods generally involve a pretext task that is solved to learn a good representation and a loss function to learn with. Below you can find a continuously updating list of self-supervised methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Inpainting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Train a convolutional neural network to generate the contents of an arbitrary image region conditioned on its surroundings.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SimCLR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SimCLR
  </strong>
  is a framework for contrastive learning of visual representations. It learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. It consists of:
 </p>
 <ul>
  <li>
   <p>
    A stochastic data augmentation module that transforms any given data example randomly resulting in two correlated views of the same example, denoted $\mathbf{\tilde{x}_{i}}$ and $\mathbf{\tilde{x}_{j}}$, which is considered a positive pair. SimCLR sequentially applies three simple augmentations: random cropping followed by resize back to the original size, random color distortions, and
    <a href="https://paperswithcode.com/method/random-gaussian-blur">
     random Gaussian blur
    </a>
    . The authors find random crop and color distortion is crucial to achieve good performance.
   </p>
  </li>
  <li>
   <p>
    A neural network base encoder $f\left(·\right)$ that extracts representation vectors from augmented data examples. The framework allows various choices of the network architecture without any constraints. The authors opt for simplicity and adopt
    <a href="https://paperswithcode.com/method/resnet">
     ResNet
    </a>
    to obtain $h_{i} = f\left(\mathbf{\tilde{x}}_{i}\right) = \text{ResNet}\left(\mathbf{\tilde{x}}_{i}\right)$ where $h_{i} \in \mathbb{R}^{d}$ is the output after the
    <a href="https://paperswithcode.com/method/average-pooling">
     average pooling
    </a>
    layer.
   </p>
  </li>
  <li>
   <p>
    A small neural network projection head $g\left(·\right)$ that maps representations to the space where contrastive loss is applied. Authors use a MLP with one hidden layer to obtain $z_{i} = g\left(h_{i}\right) = W^{(2)}\sigma\left(W^{(1)}h_{i}\right)$ where $\sigma$ is a
    <a href="https://paperswithcode.com/method/relu">
     ReLU
    </a>
    nonlinearity. The authors find it beneficial to define the contrastive loss on $z_{i}$’s rather than $h_{i}$’s.
   </p>
  </li>
  <li>
   <p>
    A contrastive loss function defined for a contrastive prediction task. Given a set {$\mathbf{\tilde{x}}_{k}$} including a positive pair of examples $\mathbf{\tilde{x}}_{i}$ and $\mathbf{\tilde{x}_{j}}$ , the contrastive prediction task aims to identify $\mathbf{\tilde{x}}_{j}$ in {$\mathbf{\tilde{x}}_{k}$}$_{k\neq{i}}$ for a given $\mathbf{\tilde{x}}_{i}$.
   </p>
  </li>
 </ul>
 <p>
  A minibatch of $N$ examples is randomly sampled and the contrastive prediction task is defined on pairs of augmented examples derived from the minibatch, resulting in $2N$ data points. Negative examples are not sampled explicitly. Instead, given a positive pair, the other $2(N − 1)$ augmented examples within a minibatch are treated as negative examples. A
  <a href="https://paperswithcode.com/method/nt-xent">
   NT-Xent
  </a>
  (the normalized
temperature-scaled cross entropy loss) loss function is used (see components).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-02_at_4.31.34_PM_7zlWDQE.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Colorization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Colorization
  </strong>
  is a self-supervision approach that relies on colorization as the pretext task in order to learn image representations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-25_at_9.38.46_AM_Um1wWHP.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MoCo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MoCo
  </strong>
  , or
  <strong>
   Momentum Contrast
  </strong>
  , is a self-supervised learning algorithm with a contrastive loss.
 </p>
 <p>
  Contrastive loss methods can be thought of as building dynamic dictionaries. The "keys" (tokens) in the dictionary are sampled from data (e.g., images or patches) and are represented by an encoder network. Unsupervised learning trains encoders to perform dictionary look-up: an encoded “query” should be similar to its matching key and dissimilar to others. Learning is formulated as minimizing a contrastive loss.
 </p>
 <p>
  MoCo can be viewed as a way to build large and consistent dictionaries for unsupervised learning with a contrastive loss. In MoCo, we maintain the dictionary as a queue of data samples: the encoded representations of the current mini-batch are enqueued, and the oldest are dequeued. The queue decouples the dictionary size from the mini-batch size, allowing it to be large. Moreover, as the dictionary keys come from the preceding several mini-batches, a slowly progressing key encoder, implemented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-30_at_11.05.58_PM_YG15Xo7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Jigsaw</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Jigsaw
  </strong>
  is a self-supervision approach that relies on jigsaw-like puzzles as the pretext task in order to learn image representations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-25_at_9.22.16_AM_uTavqLM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BYOL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  BYOL (Bootstrap Your Own Latent) is a new approach to self-supervised learning. BYOL’s goal is to learn a representation $y_θ$ which can then be used for downstream tasks. BYOL uses two neural networks to learn: the online and target networks. The online network is defined by a set of weights $θ$ and is comprised of three stages: an encoder $f_θ$, a projector $g_θ$ and a predictor $q_θ$. The target network has the same architecture
as the online network, but uses a different set of weights $ξ$. The target network provides the regression
targets to train the online network, and its parameters $ξ$ are an exponential moving average of the
online parameters $θ$.
 </p>
 <p>
  Given the architecture diagram on the right, BYOL minimizes a similarity loss between $q_θ(z_θ)$ and $sg(z'{_ξ})$, where $θ$ are the trained weights, $ξ$ are an exponential moving average of $θ$ and $sg$ means stop-gradient. At the end of training, everything but $f_θ$ is discarded, and $y_θ$ is used as the image representation.
 </p>
 <p>
  Source:
  <a href="https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1">
   Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning
  </a>
 </p>
 <p>
  Image credit:
  <a href="https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1">
   Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-03-15_at_19.58.32_ENGHInW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Contrastive Predictive Coding</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contrastive Predictive Coding (CPC)
  </strong>
  learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful
to predict future samples.
 </p>
 <p>
  First, a non-linear encoder $g_{enc}$ maps the input sequence of observations $x_{t}$ to a sequence of latent representations $z_{t} = g_{enc}\left(x_{t}\right)$, potentially with a lower temporal resolution. Next, an autoregressive model $g_{ar}$ summarizes all $z\leq{t}$ in the latent space and produces a context latent representation $c_{t} = g_{ar}\left(z\leq{t}\right)$.
 </p>
 <p>
  A density ratio is modelled which preserves the mutual information between $x_{t+k}$ and $c_{t}$ as follows:
 </p>
 <p>
  $$ f_{k}\left(x_{t+k}, c_{t}\right) \propto \frac{p\left(x_{t+k}|c_{t}\right)}{p\left(x_{t+k}\right)} $$
 </p>
 <p>
  where $\propto$ stands for ’proportional to’ (i.e. up to a multiplicative constant). Note that the density ratio $f$ can be unnormalized (does not have to integrate to 1). The authors use a simple log-bilinear model:
 </p>
 <p>
  $$ f_{k}\left(x_{t+k}, c_{t}\right) = \exp\left(z^{T}_{t+k}W_{k}c_{t}\right) $$
 </p>
 <p>
  Any type of autoencoder and autoregressive can be used. An example the authors opt for is strided convolutional layers with residual blocks and GRUs.
 </p>
 <p>
  The autoencoder and autoregressive models are trained to minimize an
  <a href="https://paperswithcode.com/method/infonce">
   InfoNCE
  </a>
  loss (see components).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-02_at_4.04.47_PM_pBIVKAh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DINO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DINO
  </strong>
  (self-distillation with no labels) is a self-supervised learning method that directly predicts the output of a teacher network - built with a momentum encoder - using a standard cross-entropy loss.
 </p>
 <p>
  In the example to the right, DINO is illustrated in the case of one single pair of views $\left(x_{1}, x_{2}\right)$ for simplicity.
The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but other parameters.
The output of the teacher network is centered with a mean computed over the batch. Each network outputs a $K$ dimensional feature normalized with a temperature
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  over the feature dimension.
Their similarity is then measured with a cross-entropy loss.
A stop-gradient (sg) operator is applied to the teacher to propagate gradients only through the student.
The teacher parameters are updated with the student parameters' exponential moving average (ema).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.14.59_PM_fumVop5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Barlow Twins</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Barlow Twins
  </strong>
  is a self-supervised learning method that applies redundancy-reduction — a principle first proposed in neuroscience — to self supervised learning. The objective function measures the cross-correlation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity. This causes the embedding vectors of distorted version of a sample to be similar, while minimizing the redundancy between the components of these vectors. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9225168e-e638-476b-a683-49826a7d8605.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>9. Distributed Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed methods for scaling deep learning to very large models. There are many different strategies for scaling training across multiple devices, including:
   </p>
   <ul>
    <li>
     <p>
      <a href="https://paperswithcode.com/methods/category/data-parallel-methods">
       Data Parallel
      </a>
      : for each node we use the same model parameters to do forward propagation, but we send a small batch of different data to each node, compute the gradient normally, and send it back to the main node. Once we have all the gradients, we calculate the weighted average and use this to update the model parameters.
     </p>
    </li>
    <li>
     <p>
      <a href="https://paperswithcode.com/methods/category/model-parallel-methods">
       Model Parallel
      </a>
      : for each node we assign different layers to it. During forward propagation, we start in the node with the first layers, then move onto the next, and so on. Once forward propagation is done we calculate gradients for the last node,  and update model parameters for that node. Then we backpropagate onto the penultimate node, update the parameters, and so on.
     </p>
    </li>
    <li>
     <p>
      Additional methods including
      <a href="https://paperswithcode.com/methods/category/hybrid-parallel-methods">
       Hybrid Parallel
      </a>
      ,
      <a href="https://paperswithcode.com/methods/category/auto-parallel-methods">
       Auto Parallel
      </a>
      , and
      <a href="https://paperswithcode.com/methods/category/distributed-communication">
       Distributed Communication
      </a>
      .
     </p>
    </li>
   </ul>
   <p>
    Image credit:
    <a href="https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef">
     Jordi Torres
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/modeldata.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gradient Sparsification</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gradient Sparsification
  </strong>
  is a technique for distributed training that sparsifies stochastic gradients to reduce the communication cost, with minor increase in the number of iterations. The key idea behind our sparsification technique is to drop some coordinates of the stochastic gradient and appropriately amplify the remaining coordinates to ensure the unbiasedness of the sparsified stochastic gradient. The sparsification approach can significantly reduce the coding length of the stochastic gradient and only slightly increase the variance of the stochastic gradient.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_4.30.40_PM_Tuf4sM5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>IMPALA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IMPALA
  </strong>
  , or the
  <strong>
   Importance Weighted Actor Learner Architecture
  </strong>
  , is an off-policy actor-critic framework that decouples acting from learning and learns from experience trajectories using
  <a href="https://paperswithcode.com/method/v-trace">
   V-trace
  </a>
  . Unlike the popular
  <a href="https://paperswithcode.com/method/a3c">
   A3C
  </a>
  -based agents, in which workers communicate gradients with respect to the parameters of the policy to a central parameter server, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralized learner. Since the learner in IMPALA has access to full trajectories of experience we use a GPU to perform updates on mini-batches of trajectories while aggressively parallelising all time independent operations.
 </p>
 <p>
  This type of decoupled architecture can achieve very high throughput. However, because the policy used to generate a trajectory can lag behind the policy on the learner by several updates at the time of gradient calculation, learning becomes off-policy. The V-trace off-policy actor-critic algorithm is used to correct for this harmful discrepancy.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_Single_worker.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Auto Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed auto parallel methods for scaling deep learning to very large models. Auto parallel methods involve strategies for optimizing steps of parallelization, including hyperparameter tuning and model replication and partitioning.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>AutoSync</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AutoSync
  </strong>
  is a pipeline for automatically optimizing synchronization strategies, given model structures and resource specifications, in data-parallel distributed machine learning. By factorizing the synchronization strategy with respect to each trainable building block of a DL model, we can construct a valid and large strategy space spanned by multiple factors. AutoSync efficiently navigates the space and locates the optimal strategy. AutoSync leverages domain knowledge about synchronization systems to reduce the search space, and is equipped with a domain adaptive simulator, which combines principled communication modeling and data-driven ML models, to estimate the runtime of strategy proposals without launching real distributed execution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.17.06_AM_1erK68k.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>KungFu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   KungFu
  </strong>
  is a distributed ML library for TensorFlow that is designed to enable adaptive training. KungFu allows users to express high-level Adaptation Policies (APs) that describe how to change hyper- and system parameters during training. APs take real-time monitored metrics (e.g. signal-to-noise ratios and noise scale) as input and trigger control actions (e.g. cluster rescaling or synchronisation strategy updates). For execution, APs are translated into monitoring and control operators, which are embedded in the dataflow graph. APs exploit an efficient asynchronous collective communication layer, which ensures concurrency and consistency
of monitoring and adaptation operations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.23.04_AM_M8VhgxU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FlexFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FlexFlow
  </strong>
  is a deep learning engine that uses guided randomized search of the SOAP (Sample, Operator, Attribute, and Parameter) space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy’s performance and is three orders of magnitude faster than prior approaches that execute each strategy.
 </p>
 <p>
  FlexFlow uses two main components: a fast, incremental execution simulator to evaluate different parallelization strategies, and a Markov Chain Monte Carlo (MCMC) search algorithm that takes advantage of the incremental simulator to rapidly explore the large search space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.24.22_AM_xqSP19s.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Data Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed data parallel methods for deep learning. For each node we use the same model parameters to do forward propagation, but we send a small batch of different data to each node, compute the gradient normally, and send it back to the main node. Once we have all the gradients, we calculate the weighted average and use this to update the model parameters.
   </p>
   <p>
    Image credit:
    <a href="https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef">
     Jordi Torres
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2021-07-27_at_12.28.46_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gradient Sparsification</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gradient Sparsification
  </strong>
  is a technique for distributed training that sparsifies stochastic gradients to reduce the communication cost, with minor increase in the number of iterations. The key idea behind our sparsification technique is to drop some coordinates of the stochastic gradient and appropriately amplify the remaining coordinates to ensure the unbiasedness of the sparsified stochastic gradient. The sparsification approach can significantly reduce the coding length of the stochastic gradient and only slightly increase the variance of the stochastic gradient.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_4.30.40_PM_Tuf4sM5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Zero Redundancy Optimizer (ZeRO)
  </strong>
  is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Accordion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Accordion
  </strong>
  is a gradient communication scheduling algorithm that is generic across models while imposing low computational overheads. Accordion inspects the change in the gradient norms to detect critical regimes and adjusts the communication schedule dynamically. Accordion works for both adjusting the gradient compression rate or the batch size without additional parameter tuning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_4.33.01_PM_DijEBBV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li>
            <details class="category depth2">
            <summary>Asynchronous Data Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Crossbow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Crossbow
  </strong>
  is a single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size—however small—while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method.
  <a href="https://paperswithcode.com/method/slime-mould-algorithm-sma">
   SMA
  </a>
  , a synchronous variant of model averaging, is used in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.40.01_PM_cRpRUA0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SlowMo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Slow Momentum
  </strong>
  (SlowMo) is a distributed optimization method where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm.  Periodically, after taking some number $\tau$ of base algorithm steps, workers average their parameters using ALLREDUCE and perform a momentum update.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.37.05_PM_fSm5arz.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Wavelet Distributed Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wavelet
  </strong>
  is an asynchronous data parallel approach that interleaves waves of training tasks on the same group of GPUs, such that tasks belong to one wave can leverage on-device memory from tasks in another wave during their memory valley period, thus boost-up the training throughput. As shown in the Figure, Wavelet divides dataparallel training tasks into two waves, namely tick-wave and tock-wave. The task launching offset is achieved by delaying the launch time of tock-wave tasks for half of a whole forward-backward training cycle. Therefore, the tock-wave tasks can directly leverage GPU memory valley period of tick-wave tasks (e.g. 0.4s-0.6s in Figure 2(a)), since backward propagation of tick-wave tasks is compute-heavy but memory is often unused. Similarly, tick-wave tasks can leverage memory valley period of tock-wave tasks in the same way.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.32.39_PM_zK3TmHK.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Replicated Data Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PyTorch DDP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PyTorch DDP
  </strong>
  (Distributed Data Parallel) is a distributed data parallel implementation for PyTorch. To guarantee mathematical equivalence, all replicas start from the same initial values for model parameters and synchronize gradients to keep parameters consistent across training iterations. To minimize the intrusiveness, the implementation exposes the same forward API as the user model, allowing applications to seamlessly replace subsequent occurrences of a user model with the distributed data parallel model object with no additional code changes. Several techniques are integrated into the design to deliver high-performance training, including bucketing gradients, overlapping communication with computation, and skipping synchronization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.14.24_PM_bwiSAyG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BAGUA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BAGUA
  </strong>
  is a communication framework whose design goal is to provide a system abstraction that is both flexible and modular to support state-of-the-art system relaxation techniques of distributed training. The abstraction goes beyond parameter server and Allreduce paradigms, and provides a collection of MPI-style collective operations to facilitate communications with different precision and centralization strategies.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.42.11_AM_YssuQWS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ByteScheduler</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ByteScheduler
  </strong>
  is a generic communication scheduler for distributed DNN training acceleration. It is based on analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.46.42_AM_NJSaHRh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DABMD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Distributed Any-Batch Mirror Descent
  </strong>
  (DABMD) is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Sharded Data Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ZeRO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Zero Redundancy Optimizer (ZeRO)
  </strong>
  is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Infinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ZeRO-Infinity
  </strong>
  is a sharded data parallel system that extends
  <a href="https://paperswithcode.com/method/zero">
   ZeRO
  </a>
  with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinity to support massive model sizes on limited GPU resources by exploiting CPU and NVMe memory simultaneously. In addition, ZeRO-Infinity also introduces a novel GPU memory optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.23.58_PM_lX9OD5c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Offload</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  ZeRO-Offload is a sharded data parallel method for distributed training. It exploits both CPU memory and compute for offloading, while offering a clear path towards efficiently scaling on multiple GPUs by working with
  <a href="https://www.paperswithcode.com/method/zero">
   ZeRO-powered data parallelism
  </a>
  . The symbiosis allows ZeRO-Offload to maintain a single copy of the optimizer states on the CPU memory regardless of the data parallel degree. Furthermore, it keeps the aggregate communication volume between GPU and CPU, as well as the aggregate CPU computation a constant regardless of data parallelism, allowing ZeRO-Offload to effectively utilize the linear increase in CPU compute with the increase in the data parallelism degree.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.26.18_PM_n7gzPNO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Distributed Communication</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed communication methods for scaling deep learning to very large models. Communication methods aim to minimize communication overhead between nodes in a distributed system, and link nodes in an optimal way.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Blink Communication</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blink
  </strong>
  is a communication library for inter-GPU parameter exchange that achieves near-optimal link utilization. To handle topology heterogeneity from hardware generations or partial allocations from cluster schedulers, Blink dynamically generates optimal communication primitives for a given topology. Blink probes the set of links available for a given job at runtime and builds a topology with appropriate link capacities. Given the topology, Blink achieves the optimal communication rate by packing spanning trees, that can utilize more links (Lovasz, 1976; Edmonds, 1973) when compared to rings. The authors use a multiplicative-weight update based approximation algorithm to quickly compute the maximal packing and extend the algorithm to further minimize the number of trees generated. Blink’s collectives extend across multiple machines effectively utilizing all available network interfaces.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.32.52_AM_wlZxXmR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Hybrid Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed hybrid parallel methods for scaling deep learning to very large models. Hybrid methods combine
    <a href="https://paperswithcode.com/methods/category/data-parallel-methods">
     data parallel
    </a>
    and
    <a href="https://paperswithcode.com/methods/category/model-parallel-methods">
     model parallel
    </a>
    methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Herring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Herring
  </strong>
  is a parameter server based distributed training method. It combines AWS's Elastic Fabric
  <a href="https://paperswithcode.com/method/adapter">
   Adapter
  </a>
  (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.39.12_PM_uue6cPp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FastMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FastMoE
  </strong>
  is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  and Megatron-LM.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.14.28_AM_HRX4ubS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BytePS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BytePS
  </strong>
  is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.21.50_PM_lsD4NtJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li>
            <details class="category depth2">
            <summary>2D Parallel Distributed Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>FastMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FastMoE
  </strong>
  is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  and Megatron-LM.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.14.28_AM_HRX4ubS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeTransformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeTransformer
  </strong>
  is a method for automated elastic pipelining for efficient distributed training of
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  models. In PipeTransformer, an adaptive on the fly freeze algorithm is used that can identify and freeze some layers gradually during training, as well as an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.11.59_AM_kQn4WA1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>E2EAdaptiveDistTraining</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Distributed training has become a pervasive and effective approach for training a large neural network
(NN) model with processing massive data. However, it is very challenging to satisfy requirements
from various NN models, diverse computing resources, and their dynamic changes during a training
job. In this study, we design our distributed training framework in a systematic end-to-end view to
provide the built-in adaptive ability for different scenarios, especially for industrial applications and
production environments, by fully considering resource allocation, model partition, task placement,
and distributed execution. Based on the unified distributed graph and the unified cluster object,
our adaptive framework is equipped with a global cost model and a global planner, which can
enable arbitrary parallelism, resource-aware placement, multi-mode execution, fault-tolerant, and
elastic distributed training. The experiments demonstrate that our framework can satisfy various
requirements from the diversity of applications and the heterogeneity of resources with highly
competitive performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Parameter Server Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Herring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Herring
  </strong>
  is a parameter server based distributed training method. It combines AWS's Elastic Fabric
  <a href="https://paperswithcode.com/method/adapter">
   Adapter
  </a>
  (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.39.12_PM_uue6cPp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BytePS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BytePS
  </strong>
  is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.21.50_PM_lsD4NtJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HetPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HetPipe
  </strong>
  is a hybrid parallel method that integrates pipelined model parallelism (PMP) with data parallelism (DP). In HetPipe, a group of multiple GPUs, called a virtual worker, processes minibatches in a pipelined manner, and multiple such virtual workers employ data parallelism for higher performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.08.59_AM_JmC5QuV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Model Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed model parallel methods for scaling deep learning to very large models. For each node we assign different layers to it. During forward propagation, we start in the node with the first layers, then move onto the next, and so on. Once forward propagation is done we calculate gradients for the last node, and update model parameters for that node. Then we backpropagate onto the penultimate node, update the parameters, and so on.
   </p>
   <p>
    Image credit:
    <a href="https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef">
     Jordi Torres
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2021-07-27_at_12.30.22_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Chimera</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Chimera
  </strong>
  is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines).
 </p>
 <p>
  Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.
 </p>
 <p>
  The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active.
 </p>
 <p>
  In the down pipeline, stage$_{0}$∼stage$_{3}$ are mapped to $P_{0}∼P_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.14.50_PM_5WN2Zje.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPipe
  </strong>
  is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.08.43_PM_bzmZL2Z.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GShard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GShard
  </strong>
  is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.11.31_PM_8iMLLNb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tofu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tofu
  </strong>
  is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.14.55_PM_X6x7Lzy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li>
            <details class="category depth2">
            <summary>Asynchronous Pipeline Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PipeDream</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  PipeDream is an asynchronous pipeline parallel strategy for training large neural networks. It adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.26.16_PM_H8uJn8L.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeDream-2BW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeDream-2BW
  </strong>
  is an asynchronous pipeline parallel method that supports memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators, and topologies and bandwidths of interconnects. PipeDream-2BW also determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint.
 </p>
 <p>
  The two main features are a double-buffered weight update (2BW) and flush mechanisms ensure high throughput. PipeDream-2BW
splits models into stages over multiple workers, and each stage is replicated an equal number of times (with data-parallel updates across replicas of the same stage).  Such parallel pipelines work well for models where each layer is repeated a fixed number of times (e.g.,
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  models).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.29.19_PM_8a0zspQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeMare</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeMare
  </strong>
  is an asynchronous (bubble-free) pipeline parallel method for training large neural networks. It involves two main techniques: learning rate rescheduling and discrepancy correction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.38.39_PM_K5cqhcI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pipelined Backpropagation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pipelined Backpropagation
  </strong>
  is an asynchronous pipeline parallel training algorithm. It was first introduced by Petrowski et al (1993). It avoids fill and drain overhead by updating the weights without draining the pipeline first. This results in weight inconsistency, the use of different weights on the forward and backward passes for a given micro-batch. The weights used to produce a particular gradient may also have been updated when the gradient is applied, resulting in stale (or delayed) gradients. For these reasons PB resembles Asynchronous
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  and is not equivalent to standard SGD. Finegrained pipelining increases the number of pipeline stages and hence increases the weight inconsistency and delay.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.40.28_PM_k1eTzBU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Intra-Layer Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GShard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GShard
  </strong>
  is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.11.31_PM_8iMLLNb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tofu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tofu
  </strong>
  is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.14.55_PM_X6x7Lzy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mesh-TensorFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mesh-TensorFlow
  </strong>
  is a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A MeshTensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.07.19_PM_YjSCU38.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth2">
            <summary>Synchronous Pipeline Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Chimera</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Chimera
  </strong>
  is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines).
 </p>
 <p>
  Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.
 </p>
 <p>
  The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active.
 </p>
 <p>
  In the down pipeline, stage$_{0}$∼stage$_{3}$ are mapped to $P_{0}∼P_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.14.50_PM_5WN2Zje.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPipe
  </strong>
  is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.08.43_PM_bzmZL2Z.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>10. Attention Modules</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Attention Modules
    </strong>
    refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Multi-Head Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-head Attention
  </strong>
  is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).
 </p>
 <p>
  $$ \text{MultiHead}\left(\textbf{Q}, \textbf{K}, \textbf{V}\right) = \left[\text{head}_{1},\dots,\text{head}_{h}\right]\textbf{W}_{0}$$
 </p>
 <p>
  $$\text{where} \text{ head}_{i} = \text{Attention} \left(\textbf{Q}\textbf{W}_{i}^{Q}, \textbf{K}\textbf{W}_{i}^{K}, \textbf{V}\textbf{W}_{i}^{V} \right) $$
 </p>
 <p>
  Above $\textbf{W}$ are all learnable parameter matrices.
 </p>
 <p>
  Note that
  <a href="https://paperswithcode.com/method/scaled">
   scaled dot-product attention
  </a>
  is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.
 </p>
 <p>
  Source:
  <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms">
   Lilian Weng
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Spatial Attention Module
  </strong>
  is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the
  <a href="https://paperswithcode.com/method/channel-attention-module">
   channel attention
  </a>
  , the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer to generate a spatial attention map $\textbf{M}_{s}\left(F\right) \in \mathcal{R}^{H×W}$ which encodes where to emphasize or suppress.
 </p>
 <p>
  We aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\mathbf{F}^{s}_{avg} \in \mathbb{R}^{1\times{H}\times{W}}$ and $\mathbf{F}^{s}_{max} \in \mathbb{R}^{1\times{H}\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\text{AvgPool}\left(F\right);\text{MaxPool}\left(F\right)\right]\right)\right) $$
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\mathbf{F}^{s}_{avg};\mathbf{F}^{s}_{max} \right]\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function and $f^{7×7}$ represents a convolution operation with the filter size of 7 × 7.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN Self-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   SAGAN Self-Attention Module
  </strong>
  is a self-attention module used in the
  <a href="https://paperswithcode.com/method/sagan">
   Self-Attention GAN
  </a>
  architecture for image synthesis. In the module, image features from the previous hidden layer $\textbf{x} \in \mathbb{R}^{C\text{x}N}$ are first transformed into two feature spaces $\textbf{f}$, $\textbf{g}$ to calculate the attention, where $\textbf{f(x) = W}_{\textbf{f}}{\textbf{x}}$, $\textbf{g}(\textbf{x})=\textbf{W}_{\textbf{g}}\textbf{x}$. We then calculate:
 </p>
 <p>
  $$\beta_{j, i} = \frac{\exp\left(s_{ij}\right)}{\sum^{N}_{i=1}\exp\left(s_{ij}\right)} $$
 </p>
 <p>
  $$ \text{where } s_{ij} = \textbf{f}(\textbf{x}_{i})^{T}\textbf{g}(\textbf{x}_{i}) $$
 </p>
 <p>
  and $\beta_{j, i}$ indicates the extent to which the model attends to the $i$th location when synthesizing the $j$th region. Here, $C$ is the number of channels and $N$ is the number of feature
locations of features from the previous hidden layer. The output of the attention layer is $\textbf{o} = \left(\textbf{o}_{\textbf{1}}, \textbf{o}_{\textbf{2}}, \ldots, \textbf{o}_{\textbf{j}} , \ldots, \textbf{o}_{\textbf{N}}\right) \in \mathbb{R}^{C\text{x}N}$ , where,
 </p>
 <p>
  $$ \textbf{o}_{\textbf{j}} = \textbf{v}\left(\sum^{N}_{i=1}\beta_{j, i}\textbf{h}\left(\textbf{x}_{\textbf{i}}\right)\right) $$
 </p>
 <p>
  $$ \textbf{h}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{h}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  $$ \textbf{v}\left(\textbf{x}_{\textbf{i}}\right) = \textbf{W}_{\textbf{v}}\textbf{x}_{\textbf{i}} $$
 </p>
 <p>
  In the above formulation, $\textbf{W}_{\textbf{g}} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\mathbf{W}_{f} \in \mathbb{R}^{\bar{C}\text{x}C}$, $\textbf{W}_{\textbf{h}} \in \mathbb{R}^{\bar{C}\text{x}C}$ and $\textbf{W}_{\textbf{v}} \in \mathbb{R}^{C\text{x}\bar{C}}$ are the learned weight matrices, which are implemented as $1$×$1$ convolutions. The authors choose  $\bar{C} = C/8$.
 </p>
 <p>
  In addition, the module further multiplies the output of the attention layer by a scale parameter and adds back the input feature map. Therefore, the final output is given by,
 </p>
 <p>
  $$\textbf{y}_{\textbf{i}} = \gamma\textbf{o}_{\textbf{i}} + \textbf{x}_{\textbf{i}}$$
 </p>
 <p>
  where $\gamma$ is a learnable scalar and it is initialized as 0. Introducing $\gamma$ allows the network to first rely on the cues in the local neighborhood – since this is easier – and then gradually learn to assign more weight to the non-local evidence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM_79d4mU6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cross-Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Cross-Attention
  </strong>
  module is an attention module used in
  <a href="https://paperswithcode.com/method/crossvit">
   CrossViT
  </a>
  for fusion of multi-scale features. The CLS token of the large branch (circle) serves as a query token to interact with the patch tokens from the small branch through attention. $f\left(·\right)$ and $g\left(·\right)$ are projections to align dimensions. The small branch follows the same procedure but swaps CLS and patch tokens from another branch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.02.11_PM_O6x0hLv.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Blender</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blender
  </strong>
  is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.
 </p>
 <p>
  The inputs of the blender module are bottom-level bases $\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  of Mask R-CNN to crop bases with each proposal $\mathbf{p}_{d}$ and then resize the region to a fixed size $R \times R$ feature map $\mathbf{r}_{d}$
 </p>
 <p>
  $$
\mathbf{r}_{d}=\operatorname{RoIPool}_{R \times R}\left(\mathbf{B}, \mathbf{p}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  More specifically,  asampling ratio 1 is used for
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference,
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  prediction results are used.
 </p>
 <p>
  The attention size $M$ is smaller than $R$. We interpolate $\mathbf{a}_{d}$ from $M \times M$ to $R \times R$, into the shapes of $R=\left(\mathbf{r}_{d} \mid d=1 \ldots D\right)$
 </p>
 <p>
  $$
\mathbf{a}_{d}^{\prime}=\text { interpolate }_{M \times M \rightarrow R \times R}\left(\mathbf{a}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then $\mathbf{a}_{d}^{\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\mathbf{s}_{d}$.
 </p>
 <p>
  $$
\mathbf{s}_{d}=\operatorname{softmax}\left(\mathbf{a}_{d}^{\prime}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then we apply element-wise product between each entity $\mathbf{r}_{d}, \mathbf{s}_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\mathbf{m}_{d}:$
 </p>
 <p>
  $$
\mathbf{m}_{d}=\sum_{k=1}^{K} \mathbf{s}_{d}^{k} \circ \mathbf{r}_{d}^{k}, \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  where $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8f129739-0f31-4b55-814d-798ea57ef403.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Channel Attention Module
  </strong>
  is a module for channel-based attention in convolutional neural networks. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map.
 </p>
 <p>
  We first aggregate spatial information of a feature map by using both average-pooling and max-pooling operations, generating two different spatial context descriptors: $\mathbf{F}^{c}_{avg}$ and $\mathbf{F}^{c}_{max}$, which denote average-pooled features and max-pooled features respectively.
 </p>
 <p>
  Both descriptors are then forwarded to a shared network to produce our channel attention map $\mathbf{M}_{c} \in \mathbb{R}^{C\times{1}\times{1}}$. Here $C$ is the number of channels. The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $\mathbb{R}^{C/r×1×1}$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, we merge the output feature vectors using element-wise summation. In short, the channel attention is computed as:
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\text{MLP}\left(\text{AvgPool}\left(\mathbf{F}\right)\right)+\text{MLP}\left(\text{MaxPool}\left(\mathbf{F}\right)\right)\right) $$
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{avg}\right)\right) +\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{max}\right)\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function, $\mathbf{W}_{0} \in \mathbb{R}^{C/r\times{C}}$, and $\mathbf{W}_{1} \in \mathbb{R}^{C\times{C/r}}$. Note that the MLP weights, $\mathbf{W}_{0}$ and $\mathbf{W}_{1}$, are shared for both inputs and the
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activation function is followed by $\mathbf{W}_{0}$.
 </p>
 <p>
  Note that the channel attention module with just
  <a href="https://paperswithcode.com/method/average-pooling">
   average pooling
  </a>
  is the same as the
  <a href="https://paperswithcode.com/method/squeeze-and-excitation-block">
   Squeeze-and-Excitation Module
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.21_PM_YDoPGUi.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>11. Loss Functions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Loss Functions
    </strong>
    are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Focal Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Focal Loss
  </strong>
  function addresses class imbalance during training in tasks like object detection. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.
 </p>
 <p>
  Formally, the Focal Loss adds a factor $(1 - p_{t})^\gamma$ to the standard cross entropy criterion. Setting $\gamma&gt;0$ reduces the relative loss for well-classified examples ($p_{t}&gt;.5$), putting more focus on hard, misclassified examples. Here there is tunable
  <em>
   focusing
  </em>
  parameter $\gamma \ge 0$.
 </p>
 <p>
  $$ {\text{FL}(p_{t}) = - (1 - p_{t})^\gamma \log\left(p_{t}\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.45.06_PM_leJm2yh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cycle Consistency Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cycle Consistency Loss
  </strong>
  is a type of loss used for generative adversarial networks that performs unpaired image-to-image translation. It was introduced with the
  <a href="https://paperswithcode.com/method/cyclegan">
   CycleGAN
  </a>
  architecture. For two domains $X$ and $Y$, we want to learn a mapping $G : X \rightarrow Y$ and $F: Y \rightarrow X$. We want to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. Cycle Consistency Loss encourages $F\left(G\left(x\right)\right) \approx x$ and $G\left(F\left(y\right)\right) \approx y$.  It reduces the space of possible mapping functions by enforcing forward and backwards consistency:
 </p>
 <p>
  $$ \mathcal{L}_{cyc}\left(G, F\right) = \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[||F\left(G\left(x\right)\right) - x||_{1}\right] + \mathbb{E}_{y \sim p_{data}\left(y\right)}\left[||G\left(F\left(y\right)\right) - y||_{1}\right] $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_3.54.24_PM_d9gQdLL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Triplet Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The goal of
  <strong>
   Triplet loss
  </strong>
  , in the context of Siamese Networks, is to maximize the joint probability among all score-pairs i.e. the product of all probabilities. By using its negative logarithm, we can get the loss formulation as follows:
 </p>
 <p>
  $$
L_{t}\left(\mathcal{V}_{p}, \mathcal{V}_{n}\right)=-\frac{1}{M N} \sum_{i}^{M} \sum_{j}^{N} \log \operatorname{prob}\left(v p_{i}, v n_{j}\right)
$$
 </p>
 <p>
  where the balance weight $1/MN$ is used to keep the loss with the same scale for different number of instance sets.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_14.41.53_S6MJc8J.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GAN Least Squares Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GAN Least Squares Loss
  </strong>
  is a least squares loss function for generative adversarial networks. Minimizing this objective function is equivalent to minimizing the Pearson $\chi^{2}$ divergence. The objective function (here for
  <a href="https://paperswithcode.com/method/lsgan">
   LSGAN
  </a>
  ) can be defined as:
 </p>
 <p>
  $$ \min_{D}V_{LS}\left(D\right) = \frac{1}{2}\mathbb{E}_{\mathbf{x} \sim p_{data}\left(\mathbf{x}\right)}\left[\left(D\left(\mathbf{x}\right) - b\right)^{2}\right] + \frac{1}{2}\mathbb{E}_{\mathbf{z}\sim p_{data}\left(\mathbf{z}\right)}\left[\left(D\left(G\left(\mathbf{z}\right)\right) - a\right)^{2}\right] $$
 </p>
 <p>
  $$ \min_{G}V_{LS}\left(G\right) = \frac{1}{2}\mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}\left(\mathbf{z}\right)}\left[\left(D\left(G\left(\mathbf{z}\right)\right) - c\right)^{2}\right] $$
 </p>
 <p>
  where $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_4.25.10_PM_4ZC9F5Q.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>InfoNCE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   InfoNCE
  </strong>
  , where NCE stands for Noise-Contrastive Estimation, is a type of contrastive loss function used for
  <a href="https://paperswithcode.com/methods/category/self-supervised-learning">
   self-supervised learning
  </a>
  .
 </p>
 <p>
  Given a set $X = ${$x_{1}, \dots, x_{N}$} of $N$ random samples containing one positive sample from $p\left(x_{t+k}|c_{t}\right)$ and $N − 1$ negative samples from the 'proposal' distribution $p\left(x_{t+k}\right)$, we optimize:
 </p>
 <p>
  $$ \mathcal{L}_{N} = - \mathbb{E}_{X}\left[\log\frac{f_{k}\left(x_{t+k}, c_{t}\right)}{\sum_{x_{j}\in{X}}f_{k}\left(x_{j}, c_{t}\right)}\right] $$
 </p>
 <p>
  Optimizing this loss will result in $f_{k}\left(x_{t+k}, c_{t}\right)$ estimating the density ratio, which is:
 </p>
 <p>
  $$ f_{k}\left(x_{t+k}, c_{t}\right) \propto \frac{p\left(x_{t+k}|c_{t}\right)}{p\left(x_{t+k}\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-02_at_4.18.03_PM_KrnAlPG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NT-Xent</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NT-Xent
  </strong>
  , or
  <strong>
   Normalized Temperature-scaled Cross Entropy Loss
  </strong>
  , is a loss function. Let $\text{sim}\left(\mathbf{u}, \mathbf{v}\right) = \mathbf{u}^{T}\mathbf{v}/||\mathbf{u}|| ||\mathbf{v}||$ denote the cosine similarity between two vectors $\mathbf{u}$ and $\mathbf{v}$. Then the loss function for a positive pair of examples $\left(i, j\right)$ is :
 </p>
 <p>
  $$ \mathbb{l}_{i,j} = -\log\frac{\exp\left(\text{sim}\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)/\tau\right)}{\sum^{2N}_{k=1}\mathcal{1}_{[k\neq{i}]}\exp\left(\text{sim}\left(\mathbf{z}_{i}, \mathbf{z}_{k}\right)/\tau\right)}$$
 </p>
 <p>
  where $\mathcal{1}_{[k\neq{i}]} \in ${$0, 1$} is an indicator function evaluating to $1$ iff $k\neq{i}$ and $\tau$ denotes a temperature parameter. The final loss is computed across all positive pairs, both $\left(i, j\right)$ and $\left(j, i\right)$, in a mini-batch.
 </p>
 <p>
  Source:
  <a href="https://paperswithcode.com/method/simclr">
   SimCLR
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-02_at_4.57.08_PM_sspaJAg.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FLIP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  https://developer.nvidia.com/blog/flip-a-difference-evaluator-for-alternating-images/
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GAN Hinge Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   GAN Hinge Loss
  </strong>
  is a hinge loss based loss function for
  <a href="https://paperswithcode.com/methods/category/generative-adversarial-networks">
   generative adversarial networks
  </a>
  :
 </p>
 <p>
  $$ L_{D} = -\mathbb{E}_{\left(x, y\right)\sim{p}_{data}}\left[\min\left(0, -1 + D\left(x, y\right)\right)\right] -\mathbb{E}_{z\sim{p_{z}}, y\sim{p_{data}}}\left[\min\left(0, -1 - D\left(G\left(z\right), y\right)\right)\right] $$
 </p>
 <p>
  $$ L_{G} = -\mathbb{E}_{z\sim{p_{z}}, y\sim{p_{data}}}D\left(G\left(z\right), y\right) $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_3.54.23_PM_pddc7FN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dice Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  \begin{equation}
DiceLoss\left( y, \overline{p} \right) = 1 - \dfrac{\left(  2y\overline{p} + 1 \right)} {\left( y+\overline{p } + 1 \right)}
\end{equation}
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ArcFace</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ArcFace
  </strong>
  , or
  <strong>
   Additive Angular Margin Loss
  </strong>
  , is a loss function used in face recognition tasks. The
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  is traditionally used in these tasks. However, the softmax loss function does not explicitly optimise the feature embedding to enforce higher similarity for intraclass samples and diversity for inter-class samples, which results in a performance gap for deep face recognition under large intra-class appearance variations.
 </p>
 <p>
  The ArcFace loss transforms the logits $W^{T}_{j}x_{i} = || W_{j} || \text{ } || x_{i} || \cos\theta_{j}$,
where $\theta_{j}$ is the angle between the weight $W_{j}$ and the feature $x_{i}$. The individual weight $ || W_{j} || = 1$ is fixed by $l_{2}$ normalization. The embedding feature $ ||x_{i} ||$ is fixed by $l_{2}$ normalization and re-scaled to $s$. The normalisation step on features and weights makes the predictions only depend on the angle between the feature and the weight. The learned embedding
features are thus distributed on a hypersphere with a radius of $s$. Finally, an additive angular margin penalty $m$ is added between $x_{i}$ and $W_{y_{i}}$ to simultaneously enhance the intra-class compactness and inter-class discrepancy. Since the proposed additive angular margin penalty is
equal to the geodesic distance margin penalty in the normalised hypersphere, the method is named ArcFace:
 </p>
 <p>
  $$ L_{3} = -\frac{1}{N}\sum^{N}_{i=1}\log\frac{e^{s\left(\cos\left(\theta_{y_{i}} + m\right)\right)}}{e^{s\left(\cos\left(\theta_{y_{i}} + m\right)\right)} + \sum^{n}_{j=1, j \neq y_{i}}e^{s\cos\theta_{j}}} $$
 </p>
 <p>
  The authors select face images from 8 different identities containing enough samples (around 1,500 images/class) to train 2-D feature embedding networks with the softmax and ArcFace loss, respectively. As the Figure shows, the softmax loss provides roughly separable feature embedding
but produces noticeable ambiguity in decision boundaries, while the proposed ArcFace loss can obviously enforce a more evident gap between the nearest classes.
 </p>
 <p>
  Other alternatives to enforce intra-class compactness and inter-class distance include
  <a href="https://arxiv.org/abs/2004.11362">
   Supervised Contrastive Learning
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-08-04_at_2.17.31_PM_bCJokL9.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Huber loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The Huber loss function describes the penalty incurred by an estimation procedure f. Huber (1964) defines the loss function piecewise by[1]
 </p>
 <div class="codehilite">
  <pre><span></span><code><span class="err">L δ ( a ) = { 1 2 a 2 for  | a | ≤ δ , δ ⋅ ( | a | − 1 2 δ ) , otherwise. {\displaystyle L_{\delta }(a)={\begin{cases}{\frac {1}{2}}{a^{2}}&amp;{\text{for }}|a|\leq \delta ,\\\delta \cdot \left(|a|-{\frac {1}{2}}\delta \right),&amp;{\text{otherwise.}}\end{cases}}}</span>
</code></pre>
 </div>
 <p>
  This function is quadratic for small values of a, and linear for large values, with equal values and slopes of the different sections at the two points where | a | = δ |a|=\delta . The variable a often refers to the residuals, that is to the difference between the observed and predicted values a = y − f ( x ) a=y-f(x), so the former can be expanded to[2]
 </p>
 <div class="codehilite">
  <pre><span></span><code><span class="err">L δ ( y , f ( x ) ) = { 1 2 ( y − f ( x ) ) 2 for  | y − f ( x ) | ≤ δ , δ   ⋅ ( | y − f ( x ) | − 1 2 δ ) , otherwise. {\displaystyle L_{\delta }(y,f(x))={\begin{cases}{\frac {1}{2}}(y-f(x))^{2}&amp;{\text{for }}|y-f(x)|\leq \delta ,\\\delta \ \cdot \left(|y-f(x)|-{\frac {1}{2}}\delta \right),&amp;{\text{otherwise.}}\end{cases}}}</span>
</code></pre>
 </div>
 <p>
  The Huber loss is the convolution of the absolute value function with the rectangular function, scaled and translated. Thus it "smoothens out" the former's corner at the origin.
 </p>
 <p>
  .. math::
        \ell(x, y) = L = {l_1, ..., l_N}^T
 </p>
 <div class="codehilite">
  <pre><span></span><code><span class="k">with</span>

<span class="p">..</span> <span class="n">math</span><span class="p">::</span>
    <span class="n">l_n</span> <span class="o">=</span> <span class="err">\</span><span class="k">begin</span><span class="err">{</span><span class="n">cases</span><span class="err">}</span>
    <span class="mi">0</span><span class="p">.</span><span class="mi">5</span> <span class="p">(</span><span class="n">x_n</span> <span class="o">-</span> <span class="n">y_n</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="o">&amp;</span> <span class="err">\</span><span class="nb">text</span><span class="err">{</span><span class="k">if</span> <span class="err">}</span> <span class="o">|</span><span class="n">x_n</span> <span class="o">-</span> <span class="n">y_n</span><span class="o">|</span> <span class="o">&lt;</span> <span class="n">delta</span> <span class="err">\\</span>
    <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="o">|</span><span class="n">x_n</span> <span class="o">-</span> <span class="n">y_n</span><span class="o">|</span> <span class="o">-</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">),</span> <span class="o">&amp;</span> <span class="err">\</span><span class="nb">text</span><span class="err">{</span><span class="n">otherwise</span> <span class="err">}</span>
    <span class="err">\</span><span class="k">end</span><span class="err">{</span><span class="n">cases</span><span class="err">}</span>
</code></pre>
 </div>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Supervised Contrastive Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Supervised Contrastive Loss
  </strong>
  is an alternative loss function to cross entropy that the authors argue can leverage label information more effectively. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes.
 </p>
 <p>
  $$
  \mathcal{L}^{sup}=\sum_{i=1}^{2N}\mathcal{L}_i^{sup}
  \label{eqn:total_supervised_loss}
$$
 </p>
 <p>
  $$
  \mathcal{L}_i^{sup}=\frac{-1}{2N_{\boldsymbol{\tilde{y}}_i}-1}\sum_{j=1}^{2N}\mathbf{1}_{i\neq j}\cdot\mathbf{1}_{\boldsymbol{\tilde{y}}_i=\boldsymbol{\tilde{y}}_j}\cdot\log{\frac{\exp{\left(\boldsymbol{z}_i\cdot\boldsymbol{z}_j/\tau\right)}}{\sum_{k=1}^{2N}\mathbf{1}_{i\neq k}\cdot\exp{\left(\boldsymbol{z}_i\cdot\boldsymbol{z}_k/\tau\right)}}}
$$
 </p>
 <p>
  where $N_{\boldsymbol{\tilde{y}}_i}$ is the total number of images in the minibatch that have the same label, $\boldsymbol{\tilde{y}}_i$, as the anchor, $i$. This loss has important properties well suited for supervised learning: (a) generalization to an arbitrary number of positives, (b) contrastive power increases with more negatives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-12_at_12.43.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WGAN-GP Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wasserstein Gradient Penalty Loss
  </strong>
  , or
  <strong>
   WGAN-GP Loss
  </strong>
  , is a loss used for generative adversarial networks that augments the Wasserstein loss with a gradient norm penalty for random samples $\mathbf{\hat{x}} \sim \mathbb{P}_{\hat{\mathbf{x}}}$ to achieve Lipschitz continuity:
 </p>
 <p>
  $$ L = \mathbb{E}_{\mathbf{\hat{x}} \sim \mathbb{P}_{g}}\left[D\left(\tilde{\mathbf{x}}\right)\right] - \mathbb{E}_{\mathbf{x} \sim \mathbb{P}_{r}}\left[D\left(\mathbf{x}\right)\right] + \lambda\mathbb{E}_{\mathbf{\hat{x}} \sim \mathbb{P}_{\hat{\mathbf{x}}}}\left[\left(||\nabla_{\tilde{\mathbf{x}}}D\left(\mathbf{\tilde{x}}\right)||_{2}-1\right)^{2}\right]$$
 </p>
 <p>
  It was introduced as part of the
  <a href="https://paperswithcode.com/method/wgan-gp">
   WGAN-GP
  </a>
  overall model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_12.07.57_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adaptive Loss</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The Robust Loss is a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, the loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting the loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Early exiting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Exit whenever the model is confident enough allowing early exiting from hidden layers
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>12. Normalization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Normalization
    </strong>
    layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.51.03_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Layer Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  ,
  <strong>
   Layer Normalization
  </strong>
  directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  models.
 </p>
 <p>
  We compute the layer normalization statistics over all the hidden units in the same layer as follows:
 </p>
 <p>
  $$ \mu^{l} = \frac{1}{H}\sum^{H}_{i=1}a_{i}^{l} $$
 </p>
 <p>
  $$ \sigma^{l} = \sqrt{\frac{1}{H}\sum^{H}_{i=1}\left(a_{i}^{l}-\mu^{l}\right)^{2}}  $$
 </p>
 <p>
  where $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\mu$ and $\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Batch Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Batch Normalization
  </strong>
  aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for
  <a href="https://paperswithcode.com/method/dropout">
   Dropout
  </a>
  .
 </p>
 <p>
  We apply a batch normalization layer as follows for a minibatch $\mathcal{B}$:
 </p>
 <p>
  $$ \mu_{\mathcal{B}} = \frac{1}{m}\sum^{m}_{i=1}x_{i} $$
 </p>
 <p>
  $$ \sigma^{2}_{\mathcal{B}} = \frac{1}{m}\sum^{m}_{i=1}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} $$
 </p>
 <p>
  $$ \hat{x}_{i} = \frac{x_{i} - \mu_{\mathcal{B}}}{\sqrt{\sigma^{2}_{\mathcal{B}}+\epsilon}} $$
 </p>
 <p>
  $$ y_{i} = \gamma\hat{x}_{i} + \beta = \text{BN}_{\gamma, \beta}\left(x_{i}\right) $$
 </p>
 <p>
  Where $\gamma$ and $\beta$ are learnable parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/batchnorm.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Instance Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Instance Normalization
  </strong>
  (also known as contrast normalization) is a normalization layer where:
 </p>
 <p>
  $$
    y_{tijk} =  \frac{x_{tijk} - \mu_{ti}}{\sqrt{\sigma_{ti}^2 + \epsilon}},
    \quad
    \mu_{ti} = \frac{1}{HW}\sum_{l=1}^W \sum_{m=1}^H x_{tilm},
    \quad
    \sigma_{ti}^2 = \frac{1}{HW}\sum_{l=1}^W \sum_{m=1}^H (x_{tilm} - \mu_{ti})^2.
$$
 </p>
 <p>
  This prevents instance-specific mean and covariance shift simplifying the learning process. Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_11.26.48_PM_gsLrV91.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Local Response Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local Response Normalization
  </strong>
  is a normalization layer that implements the idea of lateral inhibition. Lateral inhibition is a concept in neurobiology that refers to the phenomenon of an excited neuron inhibiting its neighbours: this leads to a peak in the form of a local maximum, creating contrast in that area and increasing sensory perception. In practice, we can either normalize within the same channel or normalize across channels when we apply LRN to convolutional neural networks.
 </p>
 <p>
  $$ b_{c} = a_{c}\left(k + \frac{\alpha}{n}\sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta} $$
 </p>
 <p>
  Where the size is the number of neighbouring channels used for normalization, $\alpha$ is multiplicative factor, $\beta$ an exponent and $k$ an additive factor
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.35.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adaptive Instance Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Instance Normalization
  </strong>
  is a normalization method that aligns the mean and variance of the content features with those of the style features.
 </p>
 <p>
  <a href="https://paperswithcode.com/method/instance-normalization">
   Instance Normalization
  </a>
  normalizes the input to a single style specified by the affine parameters. Adaptive Instance Normaliation is an extension. In AdaIN, we receive a content input $x$ and a style input $y$, and we simply align the channel-wise mean and variance of $x$ to match those of $y$. Unlike
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch Normalization
  </a>
  , Instance Normalization or
  <a href="https://paperswithcode.com/method/conditional-instance-normalization">
   Conditional Instance Normalization
  </a>
  , AdaIN has no learnable affine parameters. Instead, it adaptively computes the affine parameters from the style input:
 </p>
 <p>
  $$
\textrm{AdaIN}(x, y)= \sigma(y)\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\mu(y)
$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_10.46.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spectral Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Spectral Normalization
  </strong>
  is a normalization technique used for generative adversarial networks, used to stabilize training of the discriminator. Spectral normalization has the convenient property that the Lipschitz constant is the only hyper-parameter to be tuned.
 </p>
 <p>
  It controls the Lipschitz constant of the discriminator $f$ by constraining the spectral norm of each layer $g : \textbf{h}_{in} \rightarrow \textbf{h}_{out}$. The Lipschitz norm $\Vert{g}\Vert_{\text{Lip}}$ is equal to $\sup_{\textbf{h}}\sigma\left(\nabla{g}\left(\textbf{h}\right)\right)$, where $\sigma\left(a\right)$ is the spectral norm of the matrix $A$ ($L_{2}$ matrix norm of $A$):
 </p>
 <p>
  $$ \sigma\left(a\right) = \max_{\textbf{h}:\textbf{h}\neq{0}}\frac{\Vert{A\textbf{h}}\Vert_{2}}{\Vert\textbf{h}\Vert_{2}} = \max_{\Vert\textbf{h}\Vert_{2}\leq{1}}{\Vert{A\textbf{h}}\Vert_{2}} $$
 </p>
 <p>
  which is equivalent to the largest singular value of $A$. Therefore for a
  <a href="https://paperswithcode.com/method/linear-layer">
   linear layer
  </a>
  $g\left(\textbf{h}\right) = W\textbf{h}$ the norm is given by $\Vert{g}\Vert_{\text{Lip}} = \sup_{\textbf{h}}\sigma\left(\nabla{g}\left(\textbf{h}\right)\right) = \sup_{\textbf{h}}\sigma\left(W\right) = \sigma\left(W\right) $. Spectral normalization normalizes the spectral norm of the weight matrix $W$ so it satisfies the Lipschitz constraint $\sigma\left(W\right) = 1$:
 </p>
 <p>
  $$ \bar{W}_{\text{SN}}\left(W\right) = W / \sigma\left(W\right) $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.40.12_PM_xV7rWLA.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Weight Demodulation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Modulation
  </strong>
  is an alternative to
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  for use in generative adversarial networks, specifically it is introduced in
  <a href="https://paperswithcode.com/method/stylegan2">
   StyleGAN2
  </a>
  . The purpose of
  <a href="https://paperswithcode.com/method/instance-normalization">
   instance normalization
  </a>
  is to remove the effect of $s$ - the scales of the features maps - from the statistics of the
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  ’s output feature maps. Weight modulation tries to achieve this goal more directly. Assuming that input activations are i.i.d. random variables with unit standard deviation. After modulation and convolution, the output activations have standard deviation of:
 </p>
 <p>
  $$ \sigma_{j} = \sqrt{{\sum_{i,k}w_{ijk}'}^{2}} $$
 </p>
 <p>
  i.e., the outputs are scaled by the $L_{2}$ norm of the corresponding weights. The subsequent normalization aims to restore the outputs back to unit standard deviation. This can be achieved if we scale (“demodulate”) each output feature map $j$ by $1/\sigma_{j}$ . Alternatively, we can again bake this into the convolution weights:
 </p>
 <p>
  $$ w''_{ijk} = w'_{ijk} / \sqrt{{\sum_{i, k}w'_{ijk}}^{2} + \epsilon} $$
 </p>
 <p>
  where $\epsilon$ is a small constant to avoid numerical issues.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_10.02.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Conditional Batch Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Conditional Batch Normalization (CBN)
  </strong>
  is a class-conditional variant of
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  . The key idea is to predict the $\gamma$ and $\beta$ of the batch normalization from an embedding - e.g. a language embedding in VQA. CBN enables the linguistic embedding to manipulate entire feature maps by scaling them up or down, negating them, or shutting them off. CBN has also been used in
  <a href="https://paperswithcode.com/methods/category/generative-adversarial-networks">
   GANs
  </a>
  to allow class information to affect the batch normalization parameters.
 </p>
 <p>
  Consider a single convolutional layer with batch normalization module $\text{BN}\left(F_{i,c,h,w}|\gamma_{c}, \beta_{c}\right)$ for which pretrained scalars $\gamma_{c}$ and $\beta_{c}$ are available. We would like to directly predict these affine scaling parameters from, e.g., a language embedding $\mathbf{e_{q}}$. When starting the training procedure, these parameters must be close to the pretrained values to recover the original
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  model as a poor initialization could significantly deteriorate performance. Unfortunately, it is difficult to initialize a network to output the pretrained $\gamma$ and $\beta$. For these reasons, the authors propose to predict a change $\delta\beta_{c}$ and $\delta\gamma_{c}$ on the frozen original scalars, for which it is straightforward to initialize a neural network to produce an output with zero-mean and small variance.
 </p>
 <p>
  The authors use a one-hidden-layer MLP to predict these deltas from a question embedding $\mathbf{e_{q}}$ for all feature maps within the layer:
 </p>
 <p>
  $$\Delta\beta = \text{MLP}\left(\mathbf{e_{q}}\right)$$
 </p>
 <p>
  $$\Delta\gamma = \text{MLP}\left(\mathbf{e_{q}}\right)$$
 </p>
 <p>
  So, given a feature map with $C$ channels, these MLPs output a vector of size $C$. We then add these predictions to the $\beta$ and $\gamma$ parameters:
 </p>
 <p>
  $$ \hat{\beta}_{c} = \beta_{c} + \Delta\beta_{c} $$
 </p>
 <p>
  $$ \hat{\gamma}_{c} = \gamma_{c} + \Delta\gamma_{c} $$
 </p>
 <p>
  Finally, these updated $\hat{β}$ and $\hat{\gamma}$ are used as parameters for the batch normalization: $\text{BN}\left(F_{i,c,h,w}|\hat{\gamma_{c}}, \hat{\beta_{c}}\right)$. The authors freeze all ResNet parameters, including $\gamma$ and $\beta$, during training. A ResNet consists of
four stages of computation, each subdivided in several residual blocks. In each block, the authors apply CBN to the three convolutional layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.07.48_PM_5EXPi2t.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Weight Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Normalization
  </strong>
  is a normalization method for training neural networks. It is inspired by
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  , but it is a deterministic method that does not share batch normalization's property of adding noise to the gradients. It reparameterizes each $k$-dimentional weight vector $\textbf{w}$ in terms of a parameter vector $\textbf{v}$ and a scalar parameter $g$ and to perform stochastic gradient descent with respect to those parameters instead. Weight vectors are expressed in terms of the new parameters using:
 </p>
 <p>
  $$ \textbf{w} = \frac{g}{\Vert\textbf{v}\Vert}\textbf{v}$$
 </p>
 <p>
  where $\textbf{v}$ is a $k$-dimensional vector, $g$ is a scalar, and $\Vert\textbf{v}\Vert$ denotes the Euclidean norm of $\textbf{v}$. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\textbf{w}$: we now have $\Vert\textbf{w}\Vert = g$, independent of the parameters $\textbf{v}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_cifar10_train.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Activation Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Activation Normalization
  </strong>
  is a type of normalization used for flow-based generative models; specifically it was introduced in the
  <a href="https://paperswithcode.com/method/glow">
   GLOW
  </a>
  architecture. An ActNorm layer performs an affine transformation of the activations using a scale and bias parameter per channel, similar to
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  . These parameters are initialized such that the post-actnorm activations per-channel have zero mean and unit variance given an initial minibatch of data. This is a form of data dependent initilization. After initialization, the scale and bias are treated as regular trainable parameters that are independent of the data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_8.46.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Group Normalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Group Normalization
  </strong>
  is a normalization layer that divides channels into groups and normalizes the features within each group. GN does not exploit the batch dimension, and its computation is independent of batch sizes. In the case where the group size is 1, it is equivalent to
  <a href="https://paperswithcode.com/method/instance-normalization">
   Instance Normalization
  </a>
  .
 </p>
 <p>
  As motivation for the method, many classical features like SIFT and HOG had
  <em>
   group-wise
  </em>
  features and involved
  <em>
   group-wise normalization
  </em>
  . For example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.
 </p>
 <p>
  Formally, Group Normalization is defined as:
 </p>
 <p>
  $$ \mu_{i} = \frac{1}{m}\sum_{k\in\mathcal{S}_{i}}x_{k} $$
 </p>
 <p>
  $$ \sigma^{2}_{i} = \frac{1}{m}\sum_{k\in\mathcal{S}_{i}}\left(x_{k}-\mu_{i}\right)^{2} $$
 </p>
 <p>
  $$ \hat{x}_{i} = \frac{x_{i} - \mu_{i}}{\sqrt{\sigma^{2}_{i}+\epsilon}} $$
 </p>
 <p>
  Here $x$ is the feature computed by a layer, and $i$ is an index. Formally, a Group Norm layer computes $\mu$ and $\sigma$ in a set $\mathcal{S}_{i}$ defined as: $\mathcal{S}_{i} = ${$k \mid k_{N} = i_{N} ,\lfloor\frac{k_{C}}{C/G}\rfloor = \lfloor\frac{I_{C}}{C/G}\rfloor $}.
 </p>
 <p>
  Here $G$ is the number of groups, which is a pre-defined hyper-parameter ($G = 32$ by default). $C/G$ is the number of channels per group. $\lfloor$ is the floor operation, and the final term means that the indexes $i$ and $k$ are in the same group of channels, assuming each group of channels are stored in a sequential order along the $C$ axis.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_11.26.56_PM_BQOdMKA.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>13. Deep Tabular Learning</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Consists of tabular data learning approaches that use deep learning architectures for learning on tabular data. According to the taxonomy in
    <a href="https://paperswithcode.com/paper/deep-neural-networks-and-tabular-data-a">
     V.Borisov et al. (2021)
    </a>
    , deep learning approaches for tabular data can be categorized into:
   </p>
   <ul>
    <li>
     <strong>
      Regularization models
     </strong>
    </li>
    <li>
     <strong>
      Transformer-based models
     </strong>
     :
     <a href="/method/tabnet">
      TabNet
     </a>
     ,
     <a href="/method/tabtransformer">
      TabTransformer
     </a>
     ,
     <a href="/method/saint">
      SAINT
     </a>
     ,
     <a href="/method/arm-net">
      ARM-Net
     </a>
     ,...
    </li>
    <li>
     <strong>
      Hybrid models
     </strong>
     (fully differentiable and partly differentiable):
     <a href="/method/wide-deep">
      Wide&amp;Deep
     </a>
     ,
     <a href="/method/tabnn">
      TabNN
     </a>
     ,
     <a href="/method/non">
      NON
     </a>
     ,
     <a href="/method/boost-gnn">
      Boost-GNN
     </a>
     ,
     <a href="/method/node">
      NODE
     </a>
     ,...
    </li>
    <li>
     <strong>
      Data encoding methods
     </strong>
     (single-dimensional encoding and multi-dimensional encoding):
     <a href="/method/vime">
      VIME
     </a>
     ,
     <a href="/method/scarf">
      SCARF
     </a>
     ,...
    </li>
   </ul>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/245b7491-6db9-4c91-a7e6-412967ed0837.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>BiLSTM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Bidirectional LSTM
  </strong>
  , or
  <strong>
   biLSTM
  </strong>
  , is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow
  <em>
   and
  </em>
  precede a word in a sentence).
 </p>
 <p>
  Image Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.54.27_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NON</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Network On Network (NON) is practical tabular data classification model based on deep neural network to provide accurate predictions. Various deep methods have been proposed and promising progress has been made. However, most of them use operations like neural network and factorization machines to fuse the embeddings of different features directly, and linearly combine the outputs of those operations to get the final prediction. As a result, the intra-field information and the non-linear interactions between those operations (e.g. neural network and factorization machines) are ignored. Intra-field information is the information that features inside each field belong to the same field. NON is proposed to take full advantage of intra-field information and non-linear interactions. It consists of three components: field-wise network at the bottom to capture the intra-field information, across field network in the middle to choose suitable operations data-drivenly, and operation fusion network on the top to fuse outputs of the chosen operations deeply
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/bdd25090-df77-43f4-90b8-89088b3f1760.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NODE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neural Oblivious Decision Ensembles (NODE)
  </strong>
  is a tabular data architecture that consists of differentiable
oblivious decision trees (ODT) that are trained end-to-end by backpropagation.
 </p>
 <p>
  The core building block is a Neural Oblivious Decision Ensemble (NODE) layer. The layer is composed of $m$ differentiable oblivious decision trees (ODTs) of equal depth $d$. As an input, all $m$ trees get a common vector $x \in \mathbb{R}^{n}$, containing $n$ numeric features. Below we describe a design of a single differentiable ODT.
 </p>
 <p>
  In its essence, an ODT is a decision table that splits the data along $d$ splitting features and compares each feature to a learned threshold. Then, the tree returns one of the $2^{d}$ possible responses, corresponding to the comparisons result. Therefore, each ODT is completely determined by its splitting features $f \in \mathbb{R}^{d}$, splitting thresholds $b \in \mathbb{R}^{d}$ and a $d$-dimensional tensor of responses $R \in \mathbb{R} \underbrace{2 \times 2 \times 2}_{d}$. In this notation, the tree output is defined as:
 </p>
 <p>
  $$
h(x)=R\left[\mathbb{1}\left(f_{1}(x)-b_{1}\right), \ldots, \mathbb{1}\left(f_{d}(x)-b_{d}\right)\right]
$$
where $\mathbb{1}(\cdot)$ denotes the Heaviside function.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8a0b5f79-a0ab-45f8-a2d0-8918f527585e.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TabNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TabNet
  </strong>
  is a deep tabular data learning architecture that uses sequential attention to choose which features to reason from at each decision step.
 </p>
 <p>
  The TabNet encoder is composed of a feature transformer, an attentive transformer and feature masking. A split block
divides the processed representation to be used by the attentive transformer of the subsequent step as well as for the overall output. For each step, the feature selection mask provides interpretable information about the model’s functionality, and the masks can be aggregated to obtain global feature important attribution. The TabNet decoder is composed of a feature transformer block at each step.
 </p>
 <p>
  In the feature transformer block, a 4-layer network is used, where 2 are shared across all decision steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, BN and GLU nonlinearity. An attentive transformer block example – a single layer mapping is modulated with a prior scale information which aggregates how much each feature has been used before the current decision step. sparsemax is used for normalization of the coefficients, resulting in sparse selection of the salient features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a25d8533-a49e-488f-ae06-65c476cbcbad.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>14. Semi-Supervised Learning Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Semi-Supervised Learning
    </strong>
    methods leverage unlabelled data as well as labelled data to increase performance on machine learning tasks. Below you can find a continuously updating list of semi-supervised learning methods (this may have overlap with self-supervised methods due to evaluation protocol similarity).
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MoCo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MoCo
  </strong>
  , or
  <strong>
   Momentum Contrast
  </strong>
  , is a self-supervised learning algorithm with a contrastive loss.
 </p>
 <p>
  Contrastive loss methods can be thought of as building dynamic dictionaries. The "keys" (tokens) in the dictionary are sampled from data (e.g., images or patches) and are represented by an encoder network. Unsupervised learning trains encoders to perform dictionary look-up: an encoded “query” should be similar to its matching key and dissimilar to others. Learning is formulated as minimizing a contrastive loss.
 </p>
 <p>
  MoCo can be viewed as a way to build large and consistent dictionaries for unsupervised learning with a contrastive loss. In MoCo, we maintain the dictionary as a queue of data samples: the encoded representations of the current mini-batch are enqueued, and the oldest are dequeued. The queue decouples the dictionary size from the mini-batch size, allowing it to be large. Moreover, as the dictionary keys come from the preceding several mini-batches, a slowly progressing key encoder, implemented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-30_at_11.05.58_PM_YG15Xo7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Contrastive Predictive Coding</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contrastive Predictive Coding (CPC)
  </strong>
  learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful
to predict future samples.
 </p>
 <p>
  First, a non-linear encoder $g_{enc}$ maps the input sequence of observations $x_{t}$ to a sequence of latent representations $z_{t} = g_{enc}\left(x_{t}\right)$, potentially with a lower temporal resolution. Next, an autoregressive model $g_{ar}$ summarizes all $z\leq{t}$ in the latent space and produces a context latent representation $c_{t} = g_{ar}\left(z\leq{t}\right)$.
 </p>
 <p>
  A density ratio is modelled which preserves the mutual information between $x_{t+k}$ and $c_{t}$ as follows:
 </p>
 <p>
  $$ f_{k}\left(x_{t+k}, c_{t}\right) \propto \frac{p\left(x_{t+k}|c_{t}\right)}{p\left(x_{t+k}\right)} $$
 </p>
 <p>
  where $\propto$ stands for ’proportional to’ (i.e. up to a multiplicative constant). Note that the density ratio $f$ can be unnormalized (does not have to integrate to 1). The authors use a simple log-bilinear model:
 </p>
 <p>
  $$ f_{k}\left(x_{t+k}, c_{t}\right) = \exp\left(z^{T}_{t+k}W_{k}c_{t}\right) $$
 </p>
 <p>
  Any type of autoencoder and autoregressive can be used. An example the authors opt for is strided convolutional layers with residual blocks and GRUs.
 </p>
 <p>
  The autoencoder and autoregressive models are trained to minimize an
  <a href="https://paperswithcode.com/method/infonce">
   InfoNCE
  </a>
  loss (see components).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-02_at_4.04.47_PM_pBIVKAh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FixMatch</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  FixMatch is an algorithm that first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.
 </p>
 <p>
  Description from:
  <a href="https://paperswithcode.com/paper/fixmatch-simplifying-semi-supervised-learning">
   FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
  </a>
 </p>
 <p>
  Image credit:
  <a href="https://paperswithcode.com/paper/fixmatch-simplifying-semi-supervised-learning">
   FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-07-20_at_17.42.36_X43n8BE.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Noisy Student</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Noisy Student Training
  </strong>
  is a semi-supervised learning approach. It extends the idea of self-training
and distillation with the use of equal-or-larger student models and noise added to the student during learning. It has three main steps:
 </p>
 <ol>
  <li>
   train a teacher model on labeled images
  </li>
  <li>
   use the teacher to generate pseudo labels on unlabeled images
  </li>
  <li>
   train a student model on the combination of labeled images and pseudo labeled images.
  </li>
 </ol>
 <p>
  The algorithm is iterated a few times by treating the student as a teacher to relabel the unlabeled data and training a new student.
 </p>
 <p>
  Noisy Student Training seeks to improve on self-training and distillation in two ways. First, it makes the student larger than, or at least equal to, the teacher so the student can better learn from a larger dataset. Second, it adds noise to the student so the noised student is forced to learn harder from the pseudo labels. To noise the student, it uses input noise such as
  <a href="https://paperswithcode.com/method/randaugment">
   RandAugment
  </a>
  data augmentation, and model noise such as
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  and
  <a href="https://paperswithcode.com/method/stochastic-depth">
   stochastic depth
  </a>
  during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_11.07.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>15. Neural Architecture Search</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Neural Architecture Search
    </strong>
    methods are search methods that seek to learn architectures for machine learning tasks, including the underlying build blocks. Below you can find a continuously updating list of neural architecture search algorithms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Capsule Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A capsule is an activation vector that basically executes on its inputs some complex internal
computations. Length of these activation vectors signifies the
probability of availability of a feature. Furthermore, the condition
of the recognized element is encoded as the direction in which
the vector is pointing. In traditional, CNN uses Max pooling for
invariance activities of neurons, which is nothing except a minor
change in input and the neurons of output signal will remains
same.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DARTS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Differentiable Architecture Search
  </strong>
  (
  <strong>
   DART
  </strong>
  ) is a method for efficient architecture search. The search space is made continuous so that the architecture can be optimized with respect to its validation set performance through gradient descent.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_14.58.52_cvfBt7r.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Differentiable NAS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Neural Architecture Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neural Architecture Search (NAS)
  </strong>
  learns a modular architecture which can be transferred from a small dataset to a large dataset. The method does this by reducing the problem of learning best convolutional architectures to the problem of learning a small convolutional cell. The cell can then be stacked in series to handle larger images and more complex datasets.
 </p>
 <p>
  Note that this refers to the original method referred to as NAS - there is also a broader category of methods called "neural architecture search".
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-11_at_11.56.39_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>16. Clustering</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Clustering
    </strong>
    methods cluster a dataset so that similar datapoints are located in the same group. Below you can find a continuously updating list of clustering methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>k-Means Clustering</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   k-Means Clustering
  </strong>
  is a clustering algorithm that divides a training set into $k$ different clusters of examples that are near each other. It works by initializing $k$ different centroids {$\mu\left(1\right),\ldots,\mu\left(k\right)$} to different values, then alternating between two steps until convergence:
 </p>
 <p>
  (i) each training example is assigned to cluster $i$ where $i$ is the index of the nearest centroid $\mu^{(i)}$
 </p>
 <p>
  (ii) each centroid $\mu^{(i)}$ is updated to the mean of all training examples $x^{(j)}$ assigned to cluster $i$.
 </p>
 <p>
  Text Source: Deep Learning, Goodfellow et al
 </p>
 <p>
  Image Source:
  <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html">
   scikit-learn
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_11.45.40_PM_9iog67U.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spectral Clustering</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Spectral clustering has attracted increasing attention due to
the promising ability in dealing with nonlinearly separable datasets [15], [16]. In spectral clustering, the spectrum of the graph Laplacian is used to reveal the cluster structure. The spectral clustering algorithm mainly consists of two steps: 1) constructs the low dimensional embedded representation of the data based on the eigenvectors of the graph Laplacian, 2) applies k-means on the constructed low dimensional data to obtain the clustering result. Thus,
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-05-03_at_18.05.17_k5WgVWl.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SOM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Self-Organizing Map (SOM)
  </strong>
  , commonly also known as Kohonen network (Kohonen 1982, Kohonen 2001) is a computational method for the visualization and analysis of high-dimensional data, especially experimentally acquired information.
 </p>
 <p>
  Extracted from
  <a href="http://www.scholarpedia.org/article/Self-organizing_map">
   scholarpedia
  </a>
 </p>
 <p>
  <strong>
   Sources
  </strong>
  :
 </p>
 <p>
  Image:
  <a href="http://www.scholarpedia.org/article/File:Somnbc.png">
   scholarpedia
  </a>
 </p>
 <p>
  Paper:
  <a href="https://doi.org/10.1007/BF00337288">
   Kohonen, T. Self-organized formation of topologically correct feature maps. Biol. Cybern. 43, 59–69 (1982)
  </a>
 </p>
 <p>
  Book:
  <a href="https://doi.org/10.1007/978-3-642-56927-2">
   Self-Organizing Maps
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Somnbc_P250Scd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Coresets</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>17. Adversarial Training</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Adversarial Training
    </strong>
    methods use adversarial techniques to improve generalization (and the quality of representations learnt during training). Adversarial techniques are also sometimes used in the context of generative models with a generator and a discriminator. Below you can find a continuously updating list of adversarial training methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GAIL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Generative Adversarial Imitation Learning
  </strong>
  presents a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdvProp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdvProp
  </strong>
  is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_teaser_MlfuobC.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Singular Value Clipping</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Singular Value Clipping (SVC)
  </strong>
  is an adversarial training technique used by
  <a href="https://paperswithcode.com/method/tgan">
   TGAN
  </a>
  to enforce the 1-Lipschitz constraint of the
  <a href="https://paperswithcode.com/method/wgan">
   WGAN
  </a>
  objective. It is a constraint to all linear layers in the discriminator that satisfies the spectral norm of weight parameter $W$ is equal or less than one. This
means that the singular values of weight matrix are all one or less. Therefore singular value decomposition (SVD) is performed after a parameter update, replacing all the singular values larger than one with one, and the parameters are reconstructed with them. The same operation is applied to convolutional layers by interpreting a higher order tensor in weight parameter as a matrix $\hat{W}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_8.47.23_PM_rZZG7Ja.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DiffAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Differentiable Augmentation (DiffAugment)
  </strong>
  is a set of differentiable image transformations used to augment data during
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  training. The transformations are applied to the real and generated images. It enables the gradients to be propagated through the augmentation back to the generator, regularizes
the discriminator without manipulating the target distribution, and maintains the balance of training
dynamics. Three choices of transformation are preferred by the authors in their experiments: Translation,
  <a href="https://paperswithcode.com/method/cutout">
   CutOut
  </a>
  , and Color.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/diffaugment_UH9jHYa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>18. Feedforward Networks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Feedforward Networks
    </strong>
    are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Dense Connections</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dense Connections
  </strong>
  , or
  <strong>
   Fully Connected Connections
  </strong>
  , are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n_{\text{inputs}}*n_{\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.
 </p>
 <p>
  $$h_{l} = g\left(\textbf{W}^{T}h_{l-1}\right)$$
 </p>
 <p>
  where $g$ is an activation function.
 </p>
 <p>
  Image Source: Deep Learning by Goodfellow, Bengio and Courville
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_6.31.32_PM_xBfVMWZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Linear Layer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Linear Layer
  </strong>
  is a projection $\mathbf{XW + b}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Position-Wise Feed-Forward Layer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Position-Wise Feed-Forward Layer
  </strong>
  is a type of
  <a href="https://www.paperswithcode.com/method/category/feedforwad-networks">
   feedforward layer
  </a>
  consisting of two
  <a href="https://www.paperswithcode.com/method/dense-connections">
   dense layers
  </a>
  that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Feedforward Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Feedforward Network
  </strong>
  , or a
  <strong>
   Multilayer Perceptron (MLP)
  </strong>
  , is a neural network with solely densely connected layers. This is the classic neural network architecture of the literature. It consists of inputs $x$ passed through units $h$ (of which there can be many layers) to predict a target $y$. Activation functions are generally chosen to be non-linear to allow for flexible functional approximation.
 </p>
 <p>
  Image Source: Deep Learning, Goodfellow et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_11.52.35_PM_WzTYStT.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adapter</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HyperNetwork</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   HyperNetwork
  </strong>
  is a network that generates weights for a main network.  The behavior of the main network is the same with any usual neural network: it learns to map some raw inputs to their desired targets; whereas the hypernetwork takes a set of inputs that contain information about the structure of the weights and generates the weight for that layer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-08-16_at_2.19.58_PM_bKBqVIH.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Highway Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Highway Network
  </strong>
  is an architecture designed to ease gradient-based training of very deep networks. They allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_10.27.00_PM_RwtpL9S.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>19. Data Parallel Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed data parallel methods for deep learning. For each node we use the same model parameters to do forward propagation, but we send a small batch of different data to each node, compute the gradient normally, and send it back to the main node. Once we have all the gradients, we calculate the weighted average and use this to update the model parameters.
   </p>
   <p>
    Image credit:
    <a href="https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef">
     Jordi Torres
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2021-07-27_at_12.28.46_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Local SGD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Local SGD
  </strong>
  is a distributed training technique that runs
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  independently in parallel on different workers and averages the sequences only once in a while.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.42.41_PM_PhKkTv5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gradient Sparsification</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gradient Sparsification
  </strong>
  is a technique for distributed training that sparsifies stochastic gradients to reduce the communication cost, with minor increase in the number of iterations. The key idea behind our sparsification technique is to drop some coordinates of the stochastic gradient and appropriately amplify the remaining coordinates to ensure the unbiasedness of the sparsified stochastic gradient. The sparsification approach can significantly reduce the coding length of the stochastic gradient and only slightly increase the variance of the stochastic gradient.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_4.30.40_PM_Tuf4sM5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Zero Redundancy Optimizer (ZeRO)
  </strong>
  is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Accordion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Accordion
  </strong>
  is a gradient communication scheduling algorithm that is generic across models while imposing low computational overheads. Accordion inspects the change in the gradient norms to detect critical regimes and adjusts the communication schedule dynamically. Accordion works for both adjusting the gradient compression rate or the batch size without additional parameter tuning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_4.33.01_PM_DijEBBV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Asynchronous Data Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Crossbow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Crossbow
  </strong>
  is a single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size—however small—while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method.
  <a href="https://paperswithcode.com/method/slime-mould-algorithm-sma">
   SMA
  </a>
  , a synchronous variant of model averaging, is used in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.40.01_PM_cRpRUA0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SlowMo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Slow Momentum
  </strong>
  (SlowMo) is a distributed optimization method where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm.  Periodically, after taking some number $\tau$ of base algorithm steps, workers average their parameters using ALLREDUCE and perform a momentum update.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.37.05_PM_fSm5arz.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Wavelet Distributed Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wavelet
  </strong>
  is an asynchronous data parallel approach that interleaves waves of training tasks on the same group of GPUs, such that tasks belong to one wave can leverage on-device memory from tasks in another wave during their memory valley period, thus boost-up the training throughput. As shown in the Figure, Wavelet divides dataparallel training tasks into two waves, namely tick-wave and tock-wave. The task launching offset is achieved by delaying the launch time of tock-wave tasks for half of a whole forward-backward training cycle. Therefore, the tock-wave tasks can directly leverage GPU memory valley period of tick-wave tasks (e.g. 0.4s-0.6s in Figure 2(a)), since backward propagation of tick-wave tasks is compute-heavy but memory is often unused. Similarly, tick-wave tasks can leverage memory valley period of tock-wave tasks in the same way.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.32.39_PM_zK3TmHK.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Replicated Data Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PyTorch DDP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PyTorch DDP
  </strong>
  (Distributed Data Parallel) is a distributed data parallel implementation for PyTorch. To guarantee mathematical equivalence, all replicas start from the same initial values for model parameters and synchronize gradients to keep parameters consistent across training iterations. To minimize the intrusiveness, the implementation exposes the same forward API as the user model, allowing applications to seamlessly replace subsequent occurrences of a user model with the distributed data parallel model object with no additional code changes. Several techniques are integrated into the design to deliver high-performance training, including bucketing gradients, overlapping communication with computation, and skipping synchronization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.14.24_PM_bwiSAyG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BAGUA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BAGUA
  </strong>
  is a communication framework whose design goal is to provide a system abstraction that is both flexible and modular to support state-of-the-art system relaxation techniques of distributed training. The abstraction goes beyond parameter server and Allreduce paradigms, and provides a collection of MPI-style collective operations to facilitate communications with different precision and centralization strategies.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.42.11_AM_YssuQWS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ByteScheduler</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ByteScheduler
  </strong>
  is a generic communication scheduler for distributed DNN training acceleration. It is based on analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.46.42_AM_NJSaHRh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DABMD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Distributed Any-Batch Mirror Descent
  </strong>
  (DABMD) is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Sharded Data Parallel Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ZeRO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Zero Redundancy Optimizer (ZeRO)
  </strong>
  is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Infinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ZeRO-Infinity
  </strong>
  is a sharded data parallel system that extends
  <a href="https://paperswithcode.com/method/zero">
   ZeRO
  </a>
  with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinity to support massive model sizes on limited GPU resources by exploiting CPU and NVMe memory simultaneously. In addition, ZeRO-Infinity also introduces a novel GPU memory optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.23.58_PM_lX9OD5c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Offload</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  ZeRO-Offload is a sharded data parallel method for distributed training. It exploits both CPU memory and compute for offloading, while offering a clear path towards efficiently scaling on multiple GPUs by working with
  <a href="https://www.paperswithcode.com/method/zero">
   ZeRO-powered data parallelism
  </a>
  . The symbiosis allows ZeRO-Offload to maintain a single copy of the optimizer states on the CPU memory regardless of the data parallel degree. Furthermore, it keeps the aggregate communication volume between GPU and CPU, as well as the aggregate CPU computation a constant regardless of data parallelism, allowing ZeRO-Offload to effectively utilize the linear increase in CPU compute with the increase in the data parallelism degree.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.26.18_PM_n7gzPNO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>20. Interpretability</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Interpretability Methods
    </strong>
    seek to explain the predictions made by neural networks by introducing mechanisms to enduce or enforce interpretability. For example, LIME approximates the neural network with a locally interpretable model. Below you can find a continuously updating list of interpretability methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-08_at_7.41.45_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>SHAP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SHAP
  </strong>
  , or
  <strong>
   SHapley Additive exPlanations
  </strong>
  , is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Shapley values are approximating using Kernel SHAP, which uses a weighting kernel for the approximation, and DeepSHAP, which uses DeepLift to approximate them.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_7.39.16_PM_oCV5qld.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LIME</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LIME
  </strong>
  , or
  <strong>
   Local Interpretable Model-Agnostic Explanations
  </strong>
  , is an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. It modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. It performs the role of an "explainer" to explain predictions from each data sample. The output of LIME is a set of explanations representing the contribution of each feature to a prediction for a single sample, which is a form of local interpretability.
 </p>
 <p>
  Interpretable models in LIME can be, for instance,
  <a href="https://paperswithcode.com/method/linear-regression">
   linear regression
  </a>
  or decision trees, which are trained on small perturbations (e.g. adding noise, removing words, hiding parts of the image) of the original model to provide a good local approximation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-30_at_11.11.09_AM_QQapVxU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Class activation maps could be used to interpret the prediction decision made by the convolutional neural network (CNN).
 </p>
 <p>
  Image source:
  <a href="https://paperswithcode.com/paper/learning-deep-features-for-discriminative">
   Learning Deep Features for Discriminative Localization
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_16.27.41_2WdgjIH.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Monte Carlo Dropout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>21. Domain Adaptation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DANCE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Domain Adaptive Neighborhood Clustering via Entropy Optimization (DANCE)
  </strong>
  is a self-supervised clustering method that harnesses the cluster structure of the target domain using self-supervision. This is done with a neighborhood clustering technique that self-supervises feature learning in the target. At the same time, useful source features and class boundaries are preserved and adapted with a partial domain alignment loss that the authors refer to as entropy separation loss. This loss allows the model to either match each target example with the source, or reject it as unknown.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.11.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Source Hypothesis Transfer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Source Hypothesis Transfer
  </strong>
  , or
  <strong>
   SHOT
  </strong>
  , is a representation learning framework for unsupervised domain adaptation. SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.21.01_PM_v31rKrW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MSGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-source Sentiment Generative Adversarial Network
  </strong>
  is a multi-source domain adaptation (MDA) method for visual sentiment classification. It is composed of three pipelines, i.e., image reconstruction, image translation, and cycle-reconstruction. To handle data from multiple source domains, it learns to find a unified sentiment latent space where data from both the source and target domains share a similar distribution. This is achieved via cycle consistent adversarial learning in an end-to-end manner. Notably, thanks to the unified sentiment latent space, MSGAN requires a single classification network to handle data from different source domains.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.11.30_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SIFA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Synergistic Image and Feature Alignment
  </strong>
  is an unsupervised domain adaptation framework that conducts synergistic alignment of domains from both image and feature perspectives. In SIFA, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features by leveraging adversarial learning in multiple aspects and with a deeply supervised mechanism. The feature encoder is shared between both adaptive perspectives to leverage their mutual benefits via end-to-end learning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.37.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>22. Miscellaneous Components</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The following is a list of miscellaneous components used in neural networks.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Auxiliary Classifier</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Auxiliary Classifiers
  </strong>
  are type of architectural component that seek to improve the convergence of very deep networks. They are classifier heads we attach to layers before the end of the network. The motivation is to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combatting the vanishing gradient problem. They are notably used in the Inception family of convolutional neural networks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/GoogleNet-structure-and-auxiliary-classifier-units_CM5xsxk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Highway Layer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Highway Layer
  </strong>
  contains an information highway to other layers that helps with information flow. It is characterised by the use of a gating unit to help this information flow.
 </p>
 <p>
  A plain feedforward neural network typically consists of $L$ layers where the $l$th layer ($l \in ${$1, 2, \dots, L$}) applies a nonlinear transform $H$ (parameterized by $\mathbf{W_{H,l}}$) on its input $\mathbf{x_{l}}$ to produce its output $\mathbf{y_{l}}$. Thus, $\mathbf{x_{1}}$ is the input to the network and $\mathbf{y_{L}}$ is the network’s output. Omitting the layer index and biases for clarity,
 </p>
 <p>
  $$ \mathbf{y} = H\left(\mathbf{x},\mathbf{W_{H}}\right) $$
 </p>
 <p>
  $H$ is usually an affine transform followed by a non-linear activation function, but in general it may take other forms.
 </p>
 <p>
  For a
  <a href="https://paperswithcode.com/method/highway-network">
   highway network
  </a>
  , we additionally define two nonlinear transforms $T\left(\mathbf{x},\mathbf{W_{T}}\right)$ and $C\left(\mathbf{x},\mathbf{W_{C}}\right)$ such that:
 </p>
 <p>
  $$ \mathbf{y} = H\left(\mathbf{x},\mathbf{W_{H}}\right)·T\left(\mathbf{x},\mathbf{W_{T}}\right) + \mathbf{x}·C\left(\mathbf{x},\mathbf{W_{C}}\right)$$
 </p>
 <p>
  We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. In the original paper, the authors set $C = 1 − T$, giving:
 </p>
 <p>
  $$ \mathbf{y} = H\left(\mathbf{x},\mathbf{W_{H}}\right)·T\left(\mathbf{x},\mathbf{W_{T}}\right) + \mathbf{x}·\left(1-T\left(\mathbf{x},\mathbf{W_{T}}\right)\right)$$
 </p>
 <p>
  The authors set:
 </p>
 <p>
  $$ T\left(x\right) = \sigma\left(\mathbf{W_{T}}^{T}\mathbf{x} + \mathbf{b_{T}}\right) $$
 </p>
 <p>
  Image:
  <a href="https://towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5">
   Sik-Ho Tsang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_10.15.32_PM_QjUnnkM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel Shuffle</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Channel Shuffle
  </strong>
  is an operation to help information flow across feature channels in convolutional neural networks. It was used as part of the
  <a href="https://paperswithcode.com/method/shufflenet">
   ShuffleNet
  </a>
  architecture.
 </p>
 <p>
  If we allow a group
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  to obtain input data from different groups, the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups.
 </p>
 <p>
  The above can be efficiently and elegantly implemented by a channel shuffle operation: suppose a convolutional layer with $g$ groups whose output has $g \times n$ channels; we first reshape the output channel dimension into $\left(g, n\right)$, transposing and then flattening it back as the input of next layer. Channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_5.04.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PixelShuffle</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PixelShuffle
  </strong>
  is an operation used in super-resolution models to implement efficient sub-pixel convolutions with a stride of $1/r$. Specifically it rearranges elements in a tensor of shape $(*, C \times r^2, H, W)$ to a tensor of shape $(*, C, H \times r, W \times r)$.
 </p>
 <p>
  Image Source:
  <a href="https://www.researchgate.net/figure/The-pixel-shuffle-layer-transforms-feature-maps-from-the-LR-domain-to-the-HR-image_fig3_339531308">
   Remote Sensing Single-Image Resolution Improvement Using A Deep Gradient-Aware Network with Image-Specific Enhancement
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/pixelshuffle_Ed27NA5.pbm" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>23. Adaptive Activation Functions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Adaptive or trainable activation functions are the functions with trainable parameters that are able to adapt (change, optimize) their shape and amplitude to the target dataset.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Swish</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Swish
  </strong>
  is an activation function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, where $\beta$ a learnable parameter. Nearly all implementations do not use the learnable parameter $\beta$, in which case the activation function is $x\sigma(x)$ ("Swish-1").
 </p>
 <p>
  The function $x\sigma(x)$ is exactly the
  <a href="https://paperswithcode.com/method/silu">
   SiLU
  </a>
  , which was introduced by other authors before the swish.
See
  <a href="https://arxiv.org/abs/1606.08415">
   Gaussian Error Linear Units
  </a>
  (
  <a href="https://paperswithcode.com/method/gelu">
   GELUs
  </a>
  ) where the SiLU (Sigmoid Linear Unit) was originally coined, and see
  <a href="https://arxiv.org/abs/1702.03118">
   Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning
  </a>
  and
  <a href="https://arxiv.org/abs/1710.05941v1">
   Swish: a Self-Gated Activation Function
  </a>
  where the same activation function was experimented with later.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_2.02.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LEAF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>modReLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   modReLU
  </strong>
  is an activation that is a modification of a
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  . It is a pointwise nonlinearity, $\sigma_{modReLU}\left(z\right) : C \rightarrow C$, which affects only the absolute value of a complex number, defined as:
 </p>
 <p>
  $$ \sigma_{modReLU}\left(z\right) = \left(|z| + b\right)\frac{z}{|z|} \text{ if } |z| + b \geq 0 $$
$$ \sigma_{modReLU}\left(z\right) = 0 \text{ if } |z| + b \leq 0 $$
 </p>
 <p>
  where $b \in \mathbb{R}$ is a bias parameter of the nonlinearity. For a $n_{h}$ dimensional hidden space we learn $n_{h}$ nonlinearity bias parameters, one per dimension.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.19.15_PM_pWVAtEh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Rational Activation function</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>24. Learning Rate Schedules</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Learning Rate Schedules
    </strong>
    refer to schedules for the learning rate during the training of neural networks. Below you can find a continuously updating list of learning rate schedules.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Linear Warmup With Linear Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Linear Warmup With Linear Decay
  </strong>
  is a learning rate schedule in which we increase the learning rate linearly for $n$ updates and then linearly decay afterwards.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cosine Annealing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cosine Annealing
  </strong>
  is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a "warm restart" in contrast to a "cold restart" where a new set of small random numbers may be used as a starting point.
 </p>
 <p>
  $$\eta_{t} = \eta_{min}^{i} + \frac{1}{2}\left(\eta_{max}^{i}-\eta_{min}^{i}\right)\left(1+\cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
$$
 </p>
 <p>
  Where where $\eta_{min}^{i}$ and $ \eta_{max}^{i}$ are ranges for the learning rate, and $T_{cur}$ account for how many epochs have been performed since the last restart.
 </p>
 <p>
  Text Source:
  <a href="https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/">
   Jason Brownlee
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://www.researchgate.net/figure/Training-loss-of-100-layer-DenseNet-on-CIFAR10-using-standard-learning-rate-blue-and-M_fig2_315765130">
   Gao Huang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.46.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Linear Warmup With Cosine Annealing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Linear Warmup With Cosine Annealing
  </strong>
  is a learning rate schedule where we increase the learning rate linearly for $n$ updates and then anneal according to a cosine schedule afterwards.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inverse Square Root Schedule</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Inverse Square Root
  </strong>
  is a learning rate schedule 1 / $\sqrt{\max\left(n, k\right)}$ where
$n$ is the current training iteration and $k$ is the number of warm-up steps. This sets a constant learning rate for the first $k$ steps, then exponentially decays the learning rate until pre-training is over.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Exponential Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Exponential Decay
  </strong>
  is a learning rate schedule where we decay the learning rate with more iterations using an exponential function:
 </p>
 <p>
  $$ \text{lr} = \text{lr}_{0}\exp\left(-kt\right) $$
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1">
   Suki Lau
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-11_at_12.17.52_PM_NDIpQTo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Step Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Step Decay
  </strong>
  is a learning rate schedule that drops the learning rate by a factor every few epochs, where the number of epochs is a hyperparameter.
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1">
   Suki Lau
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-11_at_12.20.30_PM_HKOgNLn.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>25. Large Batch Optimization</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Adam</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adam
  </strong>
  is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  and
  <a href="https://paperswithcode.com/method/sgd-with-momentum">
   SGD w/th Momentum
  </a>
  . The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
 </p>
 <p>
  The weight updates are performed as:
 </p>
 <p>
  $$ w_{t} = w_{t-1} - \eta\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} + \epsilon}  $$
 </p>
 <p>
  with
 </p>
 <p>
  $$ \hat{m}_{t} = \frac{m_{t}}{1-\beta^{t}_{1}} $$
 </p>
 <p>
  $$ \hat{v}_{t} = \frac{v_{t}}{1-\beta^{t}_{2}} $$
 </p>
 <p>
  $$ m_{t} = \beta_{1}m_{t-1} + (1-\beta_{1})g_{t} $$
 </p>
 <p>
  $$ v_{t} = \beta_{2}v_{t-1} + (1-\beta_{2})g_{t}^{2}  $$
 </p>
 <p>
  $ \eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \beta_{1} $ and $ \beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_6.36.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adafactor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adafactor
  </strong>
  is a stochastic optimization method based on
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$.
 </p>
 <p>
  Instead of defining the optimization algorithm in terms of absolute step sizes {$\alpha_t$}$_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\rho_t$}$_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0.
 </p>
 <p>
  Proposed hyperparameters are: $\epsilon_{1} = 10^{-30}$, $\epsilon_{2} = 10^{-3}$, $d=1$, $p_{t} = \min\left(10^{-2}, \frac{1}{\sqrt{t}}\right)$, $\hat{\beta}_{2_{t}} = 1 - t^{-0.8}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_3.07.57_PM_m1mAIju.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LAMB</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LAMB
  </strong>
  is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  as the base algorithm and then forms an update as:
 </p>
 <p>
  $$r_{t} = \frac{m_{t}}{\sqrt{v_{t}} + \epsilon}$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }\left(r_{t}^{\left(i\right)}+\lambda{x_{t}^{\left(i\right)}}\right) $$
 </p>
 <p>
  Unlike
  <a href="https://paperswithcode.com/method/lars">
   LARS
  </a>
  , the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.23.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaGrad</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AdaGrad
  </strong>
  is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients for $\theta_{i}$:
 </p>
 <p>
  $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}}g_{t, i} $$
 </p>
 <p>
  The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.
 </p>
 <p>
  Image:
  <a href="https://twitter.com/alecrad">
   Alec Radford
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.12.49_PM_SxcrwqW.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LARS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Layer-wise Adaptive Rate Scaling
  </strong>
  , or
  <strong>
   LARS
  </strong>
  , is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as
  <a href="https://paperswithcode.com/method/adam">
   Adam
  </a>
  or
  <a href="https://paperswithcode.com/method/rmsprop">
   RMSProp
  </a>
  : first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.
 </p>
 <p>
  $$m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right)$$
$$x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_2.38.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>26. Knowledge Distillation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Knowledge Distillation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.
Source:
  <a href="https://arxiv.org/abs/1503.02531">
   Distilling the Knowledge in a Neural Network
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Ontology</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SFT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Shrink and Fine-Tune
  </strong>
  , or
  <strong>
   SFT
  </strong>
  , is a type of distillation that avoids explicit distillation by copying parameters to a student student model and then fine-tuning. Specifically it extracts a student model from the maximally spaced layers of a fine-tuned teacher. Each layer $l \in L'$ is copied fully from $L$. For example, when creating a
  <a href="https://paperswithcode.com/method/bart">
   BART
  </a>
  student with 3 decoder layers from the 12 encoder layer 12 decoder layer teacher, we copy the teacher’s full $Enc^{L}$ and decoder layers 0, 6, and 11 to the student. When deciding which layers to copy, we break ties arbitrarily; copying layers 0, 5, and 11 might work just as well. When copy only 1 decoder layer, we copy layer 0. This was found this to work better than copying layer 11. The impact of initialization on performance is measured experimentally in Section 6.1. After initialization, the student model continues to fine-tune on the summarization dataset, with the objective of minimizing $\mathcal{L}_{Data}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/148a26ba-3d3d-4764-bcf2-cc47c5ae0ccf.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>STD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Spatial-Channel Token Distillation
  </strong>
  method is proposed to improve the spatial and channel mixing from a novel knowledge distillation (KD) perspective. To be specific, we design a special KD mechanism for MLP-like Vision Models called Spatial-channel Token Distillation (STD), which improves the information mixing in both the spatial and channel dimensions of MLP blocks. Instead of modifying the mixing operations themselves, STD adds spatial and channel tokens to image patches. After forward propagation, the tokens are concatenated for distillation with the teachers’ responses as targets. Each token works as an aggregator of its dimension. The objective of them is to encourage each mixing operation to extract maximal task-related information from their specific dimension.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/95e0dc65-e194-46e4-98f0-e9e803190ab7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>27. AutoML</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     AutoML
    </strong>
    methods are used to automatically solve machine learning tasks without needing the user to specify or experiment with architectures, hyperparameters and other settings. Below you can find a continuously updating list of AutoML methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Feature Selection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HPO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In machine learning, a hyperparameter is a parameter whose value is used to control learning process, and HPO is the problem of choosing a set of optimal hyperparameters for a learning algorithm.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MDL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Minimum Description Length
  </strong>
  provides a criterion for the selection of models, regardless of their complexity, without the restrictive assumption that the data form a sample from a 'true' distribution.
 </p>
 <p>
  Extracted from
  <a href="http://scholarpedia.org/article/Minimum_description_length">
   scholarpedia
  </a>
 </p>
 <p>
  <strong>
   Source
  </strong>
  :
 </p>
 <p>
  Paper:
  <a href="https://doi.org/10.1016/0005-1098(78)90005-5">
   J. Rissanen (1978) Modeling by the shortest data description. Automatica 14, 465-471
  </a>
 </p>
 <p>
  Book:
  <a href="https://ieeexplore.ieee.org/servlet/opac?bknumber=6267274">
   P. D. Grünwald (2007) The Minimum Description Length Principle, MIT Press, June 2007, 570 pages
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AutoParsimony</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The principle of parsimony, also known as Occam's razor, elucidates the preference for the simplest explanation that provides optimal results when faced with multiple options. Thus, we can assert that the principle of parsimony is justified by "the assumption that is both the simplest and contains all the necessary information required to comprehend the experiment at hand." This principle finds application in various scenarios or events in our daily lives, including predictions in Data Science models.
 </p>
 <p>
  It is widely recognized that a less complex model will produce more stable predictions, exhibit greater resilience to noise and disturbances, and be more manageable for maintenance and analysis. Additionally, reducing the number of features can lead to further cost savings by diminishing the use of sensors, lowering energy consumption, minimizing information acquisition costs, reducing maintenance requirements, and mitigating the necessity to retrain models due to feature fluctuations caused by noise, outliers, data drift, etc.
 </p>
 <p>
  The concurrent optimization of hyperparameters (HO) and feature selection (FS) for achieving Parsimonious Model Selection (PMS) is an ongoing area of active research. Nonetheless, the effective selection of appropriate hyperparameters and feature subsets presents a challenging combinatorial problem, frequently requiring the application of efficient heuristic methods.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>28. Output Functions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Output functions
    </strong>
    are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-05-23_at_11.57.57_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Softmax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Softmax
  </strong>
  output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:
 </p>
 <p>
  $$ P(y=j \mid{x}) = \frac{e^{x^{T}w_{j}}}{\sum^{K}_{k=1}e^{x^{T}wk}} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_11.56.35_PM_yh1VO82.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Heatmap</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mixture of Logistic Distributions</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mixture of Logistic Distributions (MoL)
  </strong>
  is a type of output function, and an alternative to a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer. Discretized logistic mixture likelihood is used in
  <a href="https://paperswithcode.com/method/pixelcnn">
   PixelCNN
  </a>
  ++ and
  <a href="https://paperswithcode.com/method/wavenet">
   WaveNet
  </a>
  to predict discrete values.
 </p>
 <p>
  Image Credit:
  <a href="https://medium.com/@smallfishbigsea/an-explanation-of-discretized-logistic-mixture-likelihood-bdfe531751f0">
   Hao Gao
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_4.38.30_PM_K8oGu4K.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adaptive Softmax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Softmax
  </strong>
  is a speedup technique for the computation of probability distributions over words. The adaptive
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  is inspired by the class-based
  <a href="https://paperswithcode.com/method/hierarchical-softmax">
   hierarchical softmax
  </a>
  , where the word classes are built to minimize the computation time. Adaptive softmax achieves efficiency by explicitly taking into account the computation time of matrix-multiplication on parallel systems and combining it with a few important observations, namely keeping a shortlist of frequent words in the root node
and reducing the capacity of rare words.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_11.21.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>29. Non-Parametric Regression</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Non-Parametric Regression
    </strong>
    methods are a type of regression where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric regression methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Gaussian Process</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gaussian Processes
  </strong>
  are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.
 </p>
 <p>
  Image Source: Gaussian Processes for Machine Learning, C. E. Rasmussen &amp; C. K. I. Williams
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_3.44.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SVM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Support Vector Machine
  </strong>
  , or
  <strong>
   SVM
  </strong>
  , is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”.
 </p>
 <p>
  Source:
  <a href="https://scikit-learn.org/stable/modules/svm.html">
   scikit-learn
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/sphx_glr_plot_separating_hyperplane_0011_7LTkwNL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>k-NN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   $k$-Nearest Neighbors
  </strong>
  is a clustering-based algorithm for classification and regression. It is a a type of instance-based learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Prediction is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.
 </p>
 <p>
  Source of Description and Image:
  <a href="https://scikit-learn.org/stable/modules/neighbors.html#classification">
   scikit-learn
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/sphx_glr_plot_nca_classification_thumb_uDrSiCw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>30. Model Parallel Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed model parallel methods for scaling deep learning to very large models. For each node we assign different layers to it. During forward propagation, we start in the node with the first layers, then move onto the next, and so on. Once forward propagation is done we calculate gradients for the last node, and update model parameters for that node. Then we backpropagate onto the penultimate node, update the parameters, and so on.
   </p>
   <p>
    Image credit:
    <a href="https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef">
     Jordi Torres
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2021-07-27_at_12.30.22_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Chimera</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Chimera
  </strong>
  is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines).
 </p>
 <p>
  Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.
 </p>
 <p>
  The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active.
 </p>
 <p>
  In the down pipeline, stage$_{0}$∼stage$_{3}$ are mapped to $P_{0}∼P_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.14.50_PM_5WN2Zje.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPipe
  </strong>
  is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.08.43_PM_bzmZL2Z.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GShard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GShard
  </strong>
  is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.11.31_PM_8iMLLNb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tofu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tofu
  </strong>
  is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.14.55_PM_X6x7Lzy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Asynchronous Pipeline Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PipeDream</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  PipeDream is an asynchronous pipeline parallel strategy for training large neural networks. It adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.26.16_PM_H8uJn8L.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeDream-2BW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeDream-2BW
  </strong>
  is an asynchronous pipeline parallel method that supports memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators, and topologies and bandwidths of interconnects. PipeDream-2BW also determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint.
 </p>
 <p>
  The two main features are a double-buffered weight update (2BW) and flush mechanisms ensure high throughput. PipeDream-2BW
splits models into stages over multiple workers, and each stage is replicated an equal number of times (with data-parallel updates across replicas of the same stage).  Such parallel pipelines work well for models where each layer is repeated a fixed number of times (e.g.,
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  models).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.29.19_PM_8a0zspQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeMare</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeMare
  </strong>
  is an asynchronous (bubble-free) pipeline parallel method for training large neural networks. It involves two main techniques: learning rate rescheduling and discrepancy correction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.38.39_PM_K5cqhcI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pipelined Backpropagation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pipelined Backpropagation
  </strong>
  is an asynchronous pipeline parallel training algorithm. It was first introduced by Petrowski et al (1993). It avoids fill and drain overhead by updating the weights without draining the pipeline first. This results in weight inconsistency, the use of different weights on the forward and backward passes for a given micro-batch. The weights used to produce a particular gradient may also have been updated when the gradient is applied, resulting in stale (or delayed) gradients. For these reasons PB resembles Asynchronous
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  and is not equivalent to standard SGD. Finegrained pipelining increases the number of pipeline stages and hence increases the weight inconsistency and delay.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.40.28_PM_k1eTzBU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Intra-Layer Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GShard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GShard
  </strong>
  is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.11.31_PM_8iMLLNb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tofu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tofu
  </strong>
  is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.14.55_PM_X6x7Lzy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mesh-TensorFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mesh-TensorFlow
  </strong>
  is a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A MeshTensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.07.19_PM_YjSCU38.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Synchronous Pipeline Parallel</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Chimera</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Chimera
  </strong>
  is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines).
 </p>
 <p>
  Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.
 </p>
 <p>
  The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active.
 </p>
 <p>
  In the down pipeline, stage$_{0}$∼stage$_{3}$ are mapped to $P_{0}∼P_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.14.50_PM_5WN2Zje.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPipe
  </strong>
  is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.08.43_PM_bzmZL2Z.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>31. Hybrid Parallel Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed hybrid parallel methods for scaling deep learning to very large models. Hybrid methods combine
    <a href="https://paperswithcode.com/methods/category/data-parallel-methods">
     data parallel
    </a>
    and
    <a href="https://paperswithcode.com/methods/category/model-parallel-methods">
     model parallel
    </a>
    methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Herring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Herring
  </strong>
  is a parameter server based distributed training method. It combines AWS's Elastic Fabric
  <a href="https://paperswithcode.com/method/adapter">
   Adapter
  </a>
  (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.39.12_PM_uue6cPp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FastMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FastMoE
  </strong>
  is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  and Megatron-LM.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.14.28_AM_HRX4ubS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BytePS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BytePS
  </strong>
  is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.21.50_PM_lsD4NtJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>2D Parallel Distributed Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>FastMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FastMoE
  </strong>
  is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  and Megatron-LM.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.14.28_AM_HRX4ubS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeTransformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeTransformer
  </strong>
  is a method for automated elastic pipelining for efficient distributed training of
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  models. In PipeTransformer, an adaptive on the fly freeze algorithm is used that can identify and freeze some layers gradually during training, as well as an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.11.59_AM_kQn4WA1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>E2EAdaptiveDistTraining</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Distributed training has become a pervasive and effective approach for training a large neural network
(NN) model with processing massive data. However, it is very challenging to satisfy requirements
from various NN models, diverse computing resources, and their dynamic changes during a training
job. In this study, we design our distributed training framework in a systematic end-to-end view to
provide the built-in adaptive ability for different scenarios, especially for industrial applications and
production environments, by fully considering resource allocation, model partition, task placement,
and distributed execution. Based on the unified distributed graph and the unified cluster object,
our adaptive framework is equipped with a global cost model and a global planner, which can
enable arbitrary parallelism, resource-aware placement, multi-mode execution, fault-tolerant, and
elastic distributed training. The experiments demonstrate that our framework can satisfy various
requirements from the diversity of applications and the heterogeneity of resources with highly
competitive performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Parameter Server Methods</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Herring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Herring
  </strong>
  is a parameter server based distributed training method. It combines AWS's Elastic Fabric
  <a href="https://paperswithcode.com/method/adapter">
   Adapter
  </a>
  (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.39.12_PM_uue6cPp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BytePS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BytePS
  </strong>
  is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.21.50_PM_lsD4NtJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HetPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HetPipe
  </strong>
  is a hybrid parallel method that integrates pipelined model parallelism (PMP) with data parallelism (DP). In HetPipe, a group of multiple GPUs, called a virtual worker, processes minibatches in a pipelined manner, and multiple such virtual workers employ data parallelism for higher performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.08.59_AM_JmC5QuV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>32. Initialization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Initialization
    </strong>
    methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Kaiming Initialization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Kaiming Initialization
  </strong>
  , or
  <strong>
   He Initialization
  </strong>
  , is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activations.
 </p>
 <p>
  A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:
 </p>
 <p>
  $$\frac{1}{2}n_{l}\text{Var}\left[w_{l}\right] = 1 $$
 </p>
 <p>
  This implies an initialization scheme of:
 </p>
 <p>
  $$ w_{l} \sim \mathcal{N}\left(0,  2/n_{l}\right)$$
 </p>
 <p>
  That is, a zero-centered Gaussian with standard deviation of $\sqrt{2/{n}_{l}}$ (variance shown in equation above). Biases are initialized at $0$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_1.18.53_PM_wFylRBX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Xavier Initialization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Xavier Initialization
  </strong>
  , or
  <strong>
   Glorot Initialization
  </strong>
  , is an initialization scheme for neural networks. Biases are initialized be 0 and the weights $W_{ij}$ at each layer are initialized as:
 </p>
 <p>
  $$ W_{ij} \sim U\left[-\frac{\sqrt{6}}{\sqrt{fan_{in} + fan_{out}}}, \frac{\sqrt{6}}{\sqrt{fan_{in} + fan_{out}}}\right] $$
 </p>
 <p>
  Where $U$ is a uniform distribution and $fan_{in}$ is the size of the previous layer (number of columns in $W$) and $fan_{out}$ is the size of the current layer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_1.28.06_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SkipInit</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SkipInit
  </strong>
  is a method that aims to allow
  <a href="https://paperswithcode.com/methods/category/normalization">
   normalization
  </a>
  -free training of neural networks by downscaling
  <a href="https://paperswithcode.com/method/residual-block">
   residual branches
  </a>
  at initialization.  This is achieved by including a learnable scalar multiplier at the end of each residual branch, initialized to $\alpha$.
 </p>
 <p>
  The method is motivated by theoretical findings that
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  downscales the hidden activations on the residual branch by a factor on the order of the square root of the network depth (at initialization). Therefore, as the depth of a residual network is increased, the residual blocks are increasingly dominated by the
  <a href="https://paperswithcode.com/method/residual-connection">
   skip connection
  </a>
  , which drives the functions computed by residual blocks closer to the identity, preserving signal propagation and ensuring well-behaved gradients. This leads to the proposed method which can achieve this property through an
  <a href="https://paperswithcode.com/methods/category/initialization">
   initialization
  </a>
  strategy rather than a
  <a href="https://paperswithcode.com/methods/category/normalization">
   normalization
  </a>
  strategy.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/e172f077-9036-4c2e-ab5f-b4c5a2858724.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fixup Initialization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FixUp Initialization
  </strong>
  , or
  <strong>
   Fixed-Update Initialization
  </strong>
  , is an initialization method that rescales the standard initialization of
  <a href="https://paperswithcode.com/method/residual-block">
   residual branches
  </a>
  by adjusting for the network architecture. Fixup aims to enables training very deep
  <a href="https://paperswithcode.com/method/resnet">
   residual networks
  </a>
  stably at a maximal learning rate without
  <a href="https://paperswithcode.com/methods/category/normalization">
   normalization
  </a>
  .
 </p>
 <p>
  The steps are as follows:
 </p>
 <ol>
  <li>
   <p>
    Initialize the classification layer and the last layer of each residual branch to 0.
   </p>
  </li>
  <li>
   <p>
    Initialize every other layer using a standard method, e.g.
    <a href="https://paperswithcode.com/method/he-initialization">
     Kaiming Initialization
    </a>
    , and scale only the weight layers inside residual branches by $L^{\frac{1}{2m-2}}$.
   </p>
  </li>
  <li>
   <p>
    Add a scalar multiplier (initialized at 1) in every branch and a scalar bias (initialized at 0) before each
    <a href="https://paperswithcode.com/method/convolution">
     convolution
    </a>
    ,
    <a href="https://paperswithcode.com/method/linear-layer">
     linear
    </a>
    , and element-wise activation layer.
   </p>
  </li>
 </ol>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cd30a3ac-7556-4a78-8ee5-ee2a2aa9190b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>33. Fine-Tuning</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Fine-Tuning
    </strong>
    methods in deep learning take existing trained networks and 'fine-tune' them to a new task so that information contained in the weights can be repurposed. Below you can find a continuously updating list of fine-tuning methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Discriminative Fine-Tuning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Discriminative Fine-Tuning
  </strong>
  is a fine-tuning strategy that is used for
  <a href="https://paperswithcode.com/method/ulmfit">
   ULMFiT
  </a>
  type models. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent (
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  ) update of a model’s parameters $\theta$ at time step $t$ looks like the following (Ruder, 2016):
 </p>
 <p>
  $$ \theta_{t} = \theta_{t-1} − \eta\cdot\nabla_{\theta}J\left(\theta\right)$$
 </p>
 <p>
  where $\eta$ is the learning rate and $\nabla_{\theta}J\left(\theta\right)$ is the gradient with regard to the model’s objective function. For discriminative fine-tuning, we split the parameters $\theta$ into {$\theta_{1}, \ldots, \theta_{L}$} where $\theta_{l}$ contains the parameters of the model at the $l$-th layer and $L$ is the number of layers of the model. Similarly, we obtain {$\eta_{1}, \ldots, \eta_{L}$} where $\theta_{l}$ where $\eta_{l}$ is the learning rate of the $l$-th layer. The SGD update with discriminative finetuning is then:
 </p>
 <p>
  $$ \theta_{t}^{l} = \theta_{t-1}^{l} - \eta^{l}\cdot\nabla_{\theta^{l}}J\left(\theta\right) $$
 </p>
 <p>
  The authors find that empirically it worked well to first choose the learning rate $\eta^{L}$ of the last layer by fine-tuning only the last layer and using $\eta^{l-1}=\eta^{l}/2.6$ as the learning rate for lower layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.06.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Virtual Data Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Virtual Data Augmentation
  </strong>
  , or
  <strong>
   VDA
  </strong>
  , is a framework for robustly fine-tuning pre-trained language model. Based on the original token embeddings, a multinomial mixture for augmenting virtual data is constructed, where a masked language model guarantees the semantic relevance and the Gaussian noise provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.02.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ERNIE-GEN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ERNIE-GEN
  </strong>
  is a multi-flow sequence to sequence pre-training and fine-tuning framework which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.44.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Child-Tuning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Child-Tuning
  </strong>
  is a fine-tuning technique that updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. It decreases the hypothesis space of the model via a task-specific mask applied to the full gradients, helping to effectively adapt the large-scale pretrained model to various tasks and meanwhile aiming to maintain its original generalization ability.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_1.27.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>34. Discriminators</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Discriminators
    </strong>
    are a type of module used in architectures such as generative adversarial networks to discriminate between real and generated samples. Below you can find a continuously updating list of discriminators.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PatchGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PatchGAN
  </strong>
  is a type of discriminator for generative adversarial networks which only penalizes structure at the scale of local image patches. The PatchGAN discriminator tries to classify if each $N \times N$ patch in an image is real or fake. This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of $D$. Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. It can be understood as a type of texture/style loss.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_1.02.00_PM_FdeScgM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Projection Discriminator</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Projection Discriminator
  </strong>
  is a type of discriminator for generative adversarial networks. It is motivated by a probabilistic model in which the distribution of the conditional variable $\textbf{y}$ given $\textbf{x}$ is discrete or uni-modal continuous distributions.
 </p>
 <p>
  If we look at the original solution for the loss function $\mathcal{L}_{D}$ in the vanilla GANs, we can decompose it into the sum of two log-likelihood ratios:
 </p>
 <p>
  $$ f^{*}\left(\mathbf{x}, \mathbf{y}\right) = \log\frac{q\left(\mathbf{x}\mid{\mathbf{y}}\right)q\left(\mathbf{y}\right)}{p\left(\mathbf{x}\mid{\mathbf{y}}\right)p\left(\mathbf{y}\right)} = \log\frac{q\left(\mathbf{y}\mid{\mathbf{x}}\right)}{p\left(\mathbf{y}\mid{\mathbf{x}}\right)} + \log\frac{q\left(\mathbf{x}\right)}{p\left(\mathbf{x}\right)}  = r\left(\mathbf{y\mid{x}}\right) + r\left(\mathbf{x}\right) $$
 </p>
 <p>
  We can model the log likelihood ratio $r\left(\mathbf{y\mid{x}}\right)$ and  $r\left(\mathbf{x}\right)$ by some parametric functions $f_{1}$ and $f_{2}$ respectively. If we make a standing assumption that $p\left(y\mid{x}\right)$ and $q\left(y\mid{x}\right)$ are simple distributions like those that are Gaussian or discrete log linear on the feature space, then the parametrization of the following form becomes natural:
 </p>
 <p>
  $$ f\left(\mathbf{x}, \mathbf{y}; \theta\right) = f_{1}\left(\mathbf{x}, \mathbf{y}; \theta\right) + f_{2}\left(\mathbf{x}; \theta\right) = \mathbf{y}^{T}V\phi\left(\mathbf{x}; \theta_{\phi}\right) + \psi\left(\phi(\mathbf{x}; \theta_{\phi}); \theta_{\psi}\right) $$
 </p>
 <p>
  where $V$ is the embedding matrix of $y$, $\phi\left(·, \theta_{\phi}\right)$ is a vector output function of $x$, and $\psi\left(·, \theta_{\psi}\right)$ is a scalar function of the same $\phi\left(\mathbf{x}; \theta_{\phi}\right)$ that appears in $f_{1}$. The learned parameters $\theta = ${$V, \theta_{\phi}, \theta_{\psi}$} are trained to optimize the adversarial loss. This model of the discriminator is the projection.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-03_at_11.40.36_AM_wwYpznT.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Window-based Discriminator</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Window-based Discriminator
  </strong>
  is a type of discriminator for generative adversarial networks. It is analogous to a
  <a href="https://paperswithcode.com/method/patchgan">
   PatchGAN
  </a>
  but designed for audio. While a standard
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  discriminator learns to classify between distributions of entire audio sequences, window-based discriminator learns to classify between distribution of small audio chunks. Since the discriminator loss is computed over the overlapping windows where each window is very large (equal to the receptive field of the discriminator), the model learns to maintain coherence across patches.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_6.04.19_PM_NB5EyON.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NIMA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In the context of image enhancement, maximizing NIMA score as a prior can increase the likelihood of enhancing perceptual quality of an image.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>35. Prioritized Sampling</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Prioritized Sampling
    </strong>
    methods are used in tasks like object detection to prioritize examples (e.g. hard examples) to induce better detection performance. Below you can find a continuously updating list of prioritized sampling methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ATSS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Training Sample Selection
  </strong>
  , or
  <strong>
   ATSS
  </strong>
  , is a method to automatically select positive and negative samples according to statistical characteristics of object. It bridges the gap between anchor-based and anchor-free detectors.
 </p>
 <p>
  For each ground-truth box $g$ on the image, we first find out its candidate positive samples. As described in Line $3$ to $6$, on each pyramid level, we select $k$ anchor boxes whose center are closest to the center of $g$ based on L2 distance. Supposing there are $\mathcal{L}$ feature pyramid levels, the ground-truth box $g$ will have $k\times\mathcal{L}$ candidate positive samples. After that, we compute the IoU between these candidates and the ground-truth $g$ as $\mathcal{D}_g$ in Line $7$, whose mean and standard deviation are computed as $m_g$ and $v_g$ in Line $8$ and Line $9$. With these statistics, the IoU threshold for this ground-truth $g$ is obtained as $t_g=m_g+v_g$ in Line $10$. Finally, we select these candidates whose IoU are greater than or equal to the threshold $t_g$ as final positive samples in Line $11$ to $15$.
 </p>
 <p>
  Notably ATSS also limits the positive samples' center to the ground-truth box as shown in Line $12$. Besides, if an anchor box is assigned to multiple ground-truth boxes, the one with the highest IoU will be selected. The rest are negative samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_3.26.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PISA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PrIme Sample Attention (PISA)
  </strong>
  directs the training of object detection frameworks towards prime samples. These are samples that play a key role in driving the detection performance. The authors define Hierarchical Local Rank (HLR) as a metric of importance. Specifically, they use IoU-HLR to rank positive samples and ScoreHLR to rank negative samples in each mini-batch. This ranking strategy places the positive samples with highest IoUs around each object and the negative samples with highest scores in each cluster to the top of the ranked list and directs the focus of the training process to them via a simple re-weighting scheme. The authors also devise a classification-aware regression loss to jointly optimize the classification and regression branches. Particularly, this loss would suppress those samples with large regression loss, thus reinforcing the attention to prime samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_1.58.27_PM_2LW9YFb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>OHEM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Some object detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more
effective and efficient.
  <strong>
   OHEM
  </strong>
  , or
  <strong>
   Online Hard Example Mining
  </strong>
  , is a bootstrapping technique that modifies
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  to sample from examples in a non-uniform way depending on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution
that favors diverse, high loss instances.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_11.34.39_AM_RvmJwmo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>IoU-Balanced Sampling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IoU-Balanced Sampling
  </strong>
  is hard mining method for object detection. Suppose we need to sample $N$ negative samples from $M$ corresponding candidates. The selected probability for each sample under random sampling is:
 </p>
 <p>
  $$ p = \frac{N}{M} $$
 </p>
 <p>
  To raise the selected probability of hard negatives, we evenly split the sampling interval into $K$ bins according to IoU. $N$ demanded negative samples are equally distributed to each bin. Then we select samples from them uniformly. Therefore, we get the selected probability under IoU-balanced sampling:
 </p>
 <p>
  $$ p_{k} = \frac{N}{K}*\frac{1}{M_{k}}\text{ , } k\in\left[0, K\right)$$
 </p>
 <p>
  where $M_{k}$ is the number of sampling candidates in the corresponding interval denoted by $k$. $K$ is set to 3 by default in our experiments.
 </p>
 <p>
  The sampled histogram with IoU-balanced sampling is shown by green color in the Figure to the right. The IoU-balanced sampling can guide the distribution of training samples close to the one of hard negatives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_9.42.43_PM_DwR5Ggy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>36. Meta-Learning Algorithms</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Meta-Learning
    </strong>
    methods are methods that learn to learn. An example is few-shot meta-learning methods which aim to quickly adapt to a new task with only a few datapoints. Below you can find a continuously updating list of meta-learning methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MAML</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MAML
  </strong>
  , or
  <strong>
   Model-Agnostic Meta-Learning
  </strong>
  , is a model and task-agnostic algorithm for meta-learning that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.
 </p>
 <p>
  Consider a model represented by a parametrized function $f_{\theta}$ with parameters $\theta$. When adapting to a new task $\mathcal{T}_{i}$, the model’s parameters $\theta$ become $\theta'_{i}$. With MAML, the updated parameter vector $\theta'_{i}$ is computed using one or more gradient descent updates on task $\mathcal{T}_{i}$. For example, when using one gradient update,
 </p>
 <p>
  $$ \theta'_{i} = \theta - \alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}\left(f_{\theta}\right) $$
 </p>
 <p>
  The step size $\alpha$ may be fixed as a hyperparameter or metalearned. The model parameters are trained by optimizing for the performance of $f_{\theta'_{i}}$ with respect to $\theta$ across tasks sampled from $p\left(\mathcal{T}_{i}\right)$. More concretely the meta-objective is as follows:
 </p>
 <p>
  $$ \min_{\theta} \sum_{\mathcal{T}_{i} \sim p\left(\mathcal{T}\right)} \mathcal{L}_{\mathcal{T_{i}}}\left(f_{\theta'_{i}}\right) = \sum_{\mathcal{T}_{i} \sim p\left(\mathcal{T}\right)} \mathcal{L}_{\mathcal{T_{i}}}\left(f_{\theta - \alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}\left(f_{\theta}\right)}\right) $$
 </p>
 <p>
  Note that the meta-optimization is performed over the model parameters $\theta$, whereas the objective is computed using the updated model parameters $\theta'$. In effect MAML aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task. The meta-optimization across tasks is performed via stochastic gradient descent (
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  ), such that the model parameters $\theta$ are updated as follows:
 </p>
 <p>
  $$ \theta \leftarrow \theta - \beta\nabla_{\theta} \sum_{\mathcal{T}_{i} \sim p\left(\mathcal{T}\right)} \mathcal{L}_{\mathcal{T_{i}}}\left(f_{\theta'_{i}}\right)$$
 </p>
 <p>
  where $\beta$ is the meta step size.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-30_at_10.51.52_PM_7LozDVL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MeRL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Meta Reward Learning (MeRL)
  </strong>
  is a meta-learning method for the problem of learning from sparse and underspecified rewards. For example, an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. The key insight of MeRL in dealing with underspecified rewards is that spurious trajectories and programs that achieve accidental success are detrimental to the agent's generalization performance. For example, an agent might be able to solve a specific instance of the maze problem above. However, if it learns to perform spurious actions during training, it is likely to fail when provided with unseen instructions. To mitigate this issue, MeRL optimizes a more refined auxiliary reward function, which can differentiate between accidental and purposeful success based on features of action trajectories. The auxiliary reward is optimized by maximizing the trained agent's performance on a hold-out validation set via meta learning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/MeRL_approach_UjKR3Jw.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>OCD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Meta-augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Meta-augmentation
  </strong>
  helps generate more varied tasks for a single example in meta-learning. It can be distinguished from data augmentation in classic machine learning as follows. For data augmentation in classical machine learning, the aim is to generate more varied examples, within a single task. Meta-augmentation has the exact opposite aim: we wish to generate more varied tasks,
for a single example, to force the learner to quickly learn a new task from feedback. In meta-augmentation, adding randomness discourages the base learner and model from learning trivial solutions that do not generalize to new tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_9.47.40_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>37. Dimensionality Reduction</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Dimensionality Reduction
    </strong>
    methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PCA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Principle Components Analysis (PCA)
  </strong>
  is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.26.36_PM_2Nh5OmX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Variational Inference</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LDA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Linear discriminant analysis
  </strong>
  (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
 </p>
 <p>
  Extracted from
  <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">
   Wikipedia
  </a>
 </p>
 <p>
  <strong>
   Source
  </strong>
  :
 </p>
 <p>
  Paper:
  <a href="https://dx.doi.org/10.3233/AIC-170729">
   Linear Discriminant Analysis: A Detailed Tutorial
  </a>
 </p>
 <p>
  Public version:
  <a href="https://usir.salford.ac.uk/id/eprint/52074/">
   Linear Discriminant Analysis: A Detailed Tutorial
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An
  <strong>
   autoencoder
  </strong>
  is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.
 </p>
 <p>
  Extracted from:
  <a href="https://en.wikipedia.org/wiki/Autoencoder">
   Wikipedia
  </a>
 </p>
 <p>
  Image source:
  <a href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/800px-Autoencoder_schema_wZsxqZN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ICA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <em>
   <strong>
    Independent component analysis
   </strong>
   (ICA) is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals.
  </em>
 </p>
 <p>
  <em>
   ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA.
  </em>
 </p>
 <p>
  <em>
   ICA is superficially related to principal component analysis and factor analysis. ICA is a much more powerful technique, however, capable of finding the underlying factors or sources when these classic methods fail completely.
  </em>
 </p>
 <p>
  Extracted from (https://www.cs.helsinki.fi/u/ahyvarin/whatisica.shtml)
 </p>
 <p>
  <strong>
   Source papers
  </strong>
  :
 </p>
 <p>
  <a href="https://doi.org/10.1016/0165-1684(91)90079-X">
   Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture
  </a>
 </p>
 <p>
  <a href="https://doi.org/10.1016/0165-1684(94)90029-9">
   Independent component analysis, A new concept?
  </a>
 </p>
 <p>
  <a href="https://doi.org/10.1016/S0893-6080(00)00026-5">
   Independent component analysis: algorithms and applications
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Latent Diffusion Model</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Diffusion models applied to latent spaces, which are normally built with (Variational) Autoencoders.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>38. Robust Training</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DEQ</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A new kind of implicit models, where the output of the network is defined as the solution to an "infinite-level" fixed point equation. Thanks to this we can compute the gradient of the output without activations and therefore with a significantly reduced memory footprint.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ELR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Self-adaptive Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Self-adaptive Training
  </strong>
  is a training algorithm that dynamically corrects problematic training labels by model predictions to improve generalization of deep learning for potentially corrupted training data. Accumulated predictions are used to augment the training dynamics. The use of an exponential-moving-average scheme alleviates the instability issue of model predictions, smooths out the training target during the training process and enables the algorithm to completely change the training labels if necessary.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.00.55_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CW-ERM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A closed-loop evaluation procedure is first used in a simulator to identify training data samples that are important for practical driving performance and then we these samples to help debias the policy network.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>39. Representation Learning</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MIM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ViLBERT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Vision-and-Language BERT
  </strong>
  (
  <strong>
   ViLBERT
  </strong>
  ) is a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  -based model for learning task-agnostic joint representations of image content and natural language. ViLBERT extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-08_at_19.45.57_sFq25JV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VideoBERT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  VideoBERT adapts the powerful
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  model to learn a joint visual-linguistic representation for video. It is used in numerous tasks, including action classification and video captioning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-08_at_19.32.32_zl4NgnC.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CV-MIM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CV-MIM
  </strong>
  , or
  <strong>
   Contrastive Cross-View Mutual Information Maximization
  </strong>
  , is a representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization, which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. It further utilizes two regularization terms to ensure disentanglement and smoothness of the learned representations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3bb71ff8-3336-4f6f-994b-06f3a7c64e66.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>40. Adversarial Attacks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>G-NIA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Generalizable Node Injection Attack
  </strong>
  , or
  <strong>
   G-NIA
  </strong>
  , is an attack scenario for graph neural networks where the attacker injects malicious nodes rather than modifying original nodes or edges to affect the performance of GNNs. G-NIA generates the discrete edges also by Gumbel-Top-𝑘 following OPTI and captures the coupling effect between network structure and node features by a sophisticated designed model.
 </p>
 <p>
  G-NIA explicitly models the most critical feature propagation via jointly modeling. Specifically, the malicious attributes are adopted to guide the generation of edges, modeling the influence of attributes and edges. G-NIA also adopts a model-based framework, utilizing useful information of attacking during model training, as well as saving computational cost during inference without re-optimization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.30.35_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Morphence</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Morphence
  </strong>
  is an approach for adversarial defense that shifts the defense landscape by making a model a moving target against adversarial examples. By regularly moving the decision function of a model, Morphence makes it significantly challenging for repeated or correlated attacks to succeed. Morphence deploys a pool of models generated from a base model in a manner that introduces sufficient randomness when it responds to prediction queries. To ensure repeated or correlated attacks fail, the deployed pool of models automatically expires after a query budget is reached and the model pool is replaced by a new model pool generated in advance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.39.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fast Minimum-Norm Attack</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fast Minimum-Norm Attack
  </strong>
  , or
  <strong>
   FNM
  </strong>
  , is a type of adversarial attack that works with different $\ell_{p}$-norm perturbation models ($p=0,1,2,\infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an $\ell_{p}$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to minimize the distance of the current sample to the decision boundary.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ae1dd14c-30b8-4c60-8e32-011ad880600e.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adversarial Solarization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>41. 2D Parallel Distributed Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>FastMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FastMoE
  </strong>
  is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  and Megatron-LM.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.14.28_AM_HRX4ubS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeTransformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeTransformer
  </strong>
  is a method for automated elastic pipelining for efficient distributed training of
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  models. In PipeTransformer, an adaptive on the fly freeze algorithm is used that can identify and freeze some layers gradually during training, as well as an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.11.59_AM_kQn4WA1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>E2EAdaptiveDistTraining</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Distributed training has become a pervasive and effective approach for training a large neural network
(NN) model with processing massive data. However, it is very challenging to satisfy requirements
from various NN models, diverse computing resources, and their dynamic changes during a training
job. In this study, we design our distributed training framework in a systematic end-to-end view to
provide the built-in adaptive ability for different scenarios, especially for industrial applications and
production environments, by fully considering resource allocation, model partition, task placement,
and distributed execution. Based on the unified distributed graph and the unified cluster object,
our adaptive framework is equipped with a global cost model and a global planner, which can
enable arbitrary parallelism, resource-aware placement, multi-mode execution, fault-tolerant, and
elastic distributed training. The experiments demonstrate that our framework can satisfy various
requirements from the diversity of applications and the heterogeneity of resources with highly
competitive performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>42. Working Memory Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Working Memory Models
    </strong>
    aim to supplement neural networks with a memory module to increase their capability for memorization and allowing them to more easily perform tasks such as retrieving and copying information. Below you can find a continuously updating list of working memory models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Memory Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Memory Network
  </strong>
  provides a memory component that can be read from and written to with the inference capabilities of a neural network model. The motivation is that many neural networks lack a long-term memory component, and their existing memory component encoded by states and weights is too small and not compartmentalized enough to accurately remember facts from the past (RNNs for example, have difficult memorizing and doing tasks like copying).
 </p>
 <p>
  A memory network consists of a memory $\textbf{m}$ (an array of objects indexed by $\textbf{m}_{i}$ and four potentially learned components:
 </p>
 <ul>
  <li>
   Input feature map $I$ - feature representation of the data input.
  </li>
  <li>
   Generalization $G$ - updates old memories given the new input.
  </li>
  <li>
   Output feature map $O$ - produces new feature map given $I$ and $G$.
  </li>
  <li>
   Response $R$ - converts output into the desired response.
  </li>
 </ul>
 <p>
  Given an input $x$ (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:
 </p>
 <ol>
  <li>
   Convert $x$ to an internal feature representation $I\left(x\right)$.
  </li>
  <li>
   Update memories $m_{i}$ given the new input: $m_{i} = G\left(m_{i}, I\left(x\right), m\right)$, $\forall{i}$.
  </li>
  <li>
   Compute output features $o$ given the new input and the memory: $o = O\left(I\left(x\right), m\right)$.
  </li>
  <li>
   Finally, decode output features $o$ to give the final response: $r = R\left(o\right)$.
  </li>
 </ol>
 <p>
  This process is applied at both train and test time, if there is a distinction between such phases, that
is, memories are also stored at test time, but the model parameters of $I$, $G$, $O$ and $R$ are not updated. Memory networks cover a wide class of possible implementations. The components $I$, $G$, $O$ and $R$ can potentially use any existing ideas from the machine learning literature.
 </p>
 <p>
  Image Source:
  <a href="https://blog.acolyer.org/2016/03/10/memory-networks/">
   Adrian Colyer
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/memory-network.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Neural Turing Machine</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Neural Turing Machine
  </strong>
  is a working memory neural network model. It couples a neural network architecture with external memory resources. The whole architecture is differentiable end-to-end with gradient descent. The models can infer tasks such as copying, sorting and associative recall.
 </p>
 <p>
  A Neural Turing Machine (NTM) architecture contains two basic components: a neural
network controller and a memory bank. The Figure presents a high-level diagram of the NTM
architecture. Like most neural networks, the controller interacts with the external world via
input and output vectors. Unlike a standard network, it also interacts with a memory matrix
using selective read and write operations. By analogy to the Turing machine we refer to the
network outputs that parameterise these operations as “heads.”
 </p>
 <p>
  Every component of the architecture is differentiable. This is achieved by defining 'blurry' read and write operations that interact to a greater or lesser degree with all the elements in memory (rather
than addressing a single element, as in a normal Turing machine or digital computer). The
degree of blurriness is determined by an attentional “focus” mechanism that constrains each
read and write operation to interact with a small portion of the memory, while ignoring the
rest. Because interaction with the memory is highly sparse, the NTM is biased towards
storing data without interference. The memory location brought into attentional focus is
determined by specialised outputs emitted by the heads. These outputs define a normalised
weighting over the rows in the memory matrix (referred to as memory “locations”). Each
weighting, one per read or write head, defines the degree to which the head reads or writes
at each location. A head can thereby attend sharply to the memory at a single location or
weakly to the memory at many locations
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_12.45.35_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dynamic Memory Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Dynamic Memory Network
  </strong>
  is a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers.
 </p>
 <p>
  The DMN consists of a number of modules:
 </p>
 <ul>
  <li>
   Input Module: The input module encodes raw text inputs from the task into distributed vector representations. The input takes forms like a sentence, a long story, a movie review and so on.
  </li>
  <li>
   Question Module: The question module encodes the question of the task into a distributed
vector representation. For question answering, the question may be a sentence such as "Where did the author first fly?". The representation is fed into the episodic memory module, and forms the basis, or initial state, upon which the episodic memory module iterates.
  </li>
  <li>
   Episodic Memory Module: Given a collection of input representations, the episodic memory module chooses which parts of the inputs to focus on through the attention mechanism. It then produces a ”memory” vector representation taking into account the question as well as the previous memory. Each iteration provides the module with newly relevant information about the input. In other words,
the module has the ability to retrieve new information, in the form of input representations, which were thought to be irrelevant in previous iterations.
  </li>
  <li>
   Answer Module: The answer module generates an answer from the final memory vector of the memory module.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_2.41.30_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>End-To-End Memory Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An
  <strong>
   End-to-End Memory Network
  </strong>
  is a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of
  <a href="https://paperswithcode.com/method/memory-network">
   Memory Network
  </a>
  , but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol.
 </p>
 <p>
  The model takes a discrete set of inputs $x_{1}, \dots, x_{n}$ that are to be stored in the memory, a query $q$, and outputs an answer $a$. Each of the $x_{i}$, $q$, and $a$ contains symbols coming from a dictionary with $V$ words. The model writes all $x$ to the memory up to a fixed buffer size, and then finds a continuous representation for the $x$ and $q$. The continuous representation is then processed via multiple hops to output $a$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_memRNN4.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>43. Approximate Inference</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Approximate Inference
    </strong>
    methods are used within the context of Bayesian inference to approximate (intractable) posteriors. The most popular category were Markov Chain Monte Carlo methods; more recently variational methods have become popular. Below you can find a continuously updating list of approximate inference methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ABC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Class of methods in Bayesian Statistics where the posterior distribution is approximated over a rejection scheme on simulations because the likelihood function is intractable.
 </p>
 <p>
  Different parameters get sampled and simulated. Then a distance function is calculated to measure the quality of the simulation compared to data from real observations. Only simulations that fall below a certain threshold get accepted.
 </p>
 <p>
  Image source:
  <a href="https://www.umass.edu/nanofabrics/sites/default/files/PDF_0.pdf">
   Kulkarni et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_16.09.33_pMfevKc.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Metropolis Hastings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Metropolis-Hastings
  </strong>
  is a Markov Chain Monte Carlo (MCMC) algorithm for approximate inference. It allows for sampling from a probability distribution where direct sampling is difficult - usually owing to the presence of an intractable integral.
 </p>
 <p>
  M-H consists of a proposal distribution $q\left(\theta^{'}\mid\theta\right)$ to draw a parameter value. To decide whether $\theta^{'}$ is accepted or rejected, we then calculate a ratio:
 </p>
 <p>
  $$ \frac{p\left(\theta^{'}\mid{D}\right)}{p\left(\theta\mid{D}\right)} $$
 </p>
 <p>
  We then draw a random number $r \in \left[0, 1\right]$ and accept if it is under the ratio, reject otherwise. If we accept, we set $\theta_{i} = \theta^{'}$ and repeat.
 </p>
 <p>
  By the end we have a sample of $\theta$ values that we can use to form quantities over an approximate posterior, such as the expectation and uncertainty bounds. In practice, we typically have a period of tuning to achieve an acceptable acceptance ratio for the algorithm, as well as a warmup period to reduce bias towards initialization values.
 </p>
 <p>
  Image:
  <a href="https://static1.squarespace.com/static/52e69d46e4b05a145935f24d/t/5a7dbadcf9619a745c5b2513/1518189289690/Stan.pdf">
   Samuel Hudec
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_7.08.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>reSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  reSGLD proposes to simulate a high-temperature particle for exploration and a low-temperature particle for exploitation and allows them to swap simultaneously. Moreover, a correction term is included to avoid biases.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.01.09_PM_OCFN35j.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Simulations of multi-modal distributions can be very costly and often lead to unreliable predictions. To accelerate the computations, we propose to sample from a flattened distribution to accelerate the computations and estimate the importance weights between the original distribution and the flattened distribution to ensure the correctness of the distribution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.08.22_PM_qx9FEwa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Markov Chain Monte Carlo</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The golden standard for uncertainty quantification and Bayesian inference.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-11-04_at_9.01.09_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Metropolis Hastings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Metropolis-Hastings
  </strong>
  is a Markov Chain Monte Carlo (MCMC) algorithm for approximate inference. It allows for sampling from a probability distribution where direct sampling is difficult - usually owing to the presence of an intractable integral.
 </p>
 <p>
  M-H consists of a proposal distribution $q\left(\theta^{'}\mid\theta\right)$ to draw a parameter value. To decide whether $\theta^{'}$ is accepted or rejected, we then calculate a ratio:
 </p>
 <p>
  $$ \frac{p\left(\theta^{'}\mid{D}\right)}{p\left(\theta\mid{D}\right)} $$
 </p>
 <p>
  We then draw a random number $r \in \left[0, 1\right]$ and accept if it is under the ratio, reject otherwise. If we accept, we set $\theta_{i} = \theta^{'}$ and repeat.
 </p>
 <p>
  By the end we have a sample of $\theta$ values that we can use to form quantities over an approximate posterior, such as the expectation and uncertainty bounds. In practice, we typically have a period of tuning to achieve an acceptable acceptance ratio for the algorithm, as well as a warmup period to reduce bias towards initialization values.
 </p>
 <p>
  Image:
  <a href="https://static1.squarespace.com/static/52e69d46e4b05a145935f24d/t/5a7dbadcf9619a745c5b2513/1518189289690/Stan.pdf">
   Samuel Hudec
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_7.08.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Simulations of multi-modal distributions can be very costly and often lead to unreliable predictions. To accelerate the computations, we propose to sample from a flattened distribution to accelerate the computations and estimate the importance weights between the original distribution and the flattened distribution to ensure the correctness of the distribution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.08.22_PM_qx9FEwa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>reSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  reSGLD proposes to simulate a high-temperature particle for exploration and a low-temperature particle for exploitation and allows them to swap simultaneously. Moreover, a correction term is included to avoid biases.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.01.09_PM_OCFN35j.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>44. Active Learning</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>BASE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EWC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The methon to overcome catastrophic forgetting in neural network while continual learning
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Snapshot Ensembles</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The  overhead  cost  of  training  multiple  deep  neural networks  could  be  very  high  in  terms  of  the  training  time, hardware, and computational resource requirement and often acts  as  obstacle  for  creating  deep  ensembles.  To  overcome these barriers Huang et al. proposed a unique method to create  ensemble  which  at  the  cost  of  training  one  model, yields  multiple  constituent  model  snapshots  that  can  be ensembled together to create a strong learner. The core idea behind the concept is to make the model converge to several local minima along the optimization path and save the model parameters at these local minima points. During the training phase, a neural network would traverse through many such points. The lowest of all such local minima is known as the Global Minima. The larger the model, more are the number of parameters and larger the number of local minima points. This implies, there are discrete sets of weights and biases, at which  the  model  is  making  fewer  errors.  So,  every  such minimum  can  be  considered a  weak  but  a  potential learner model for the problem being solved. Multiple such snapshot of  weights  and  biases  are  recorded  which  can  later  be ensembled to get a better generalized model which makes the least amount of mistakes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ALDEN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALDEN
  </strong>
  , or
  <strong>
   Active Learning with DivErse iNterpretations
  </strong>
  , is an active learning approach for text classification. With local interpretations in DNNs, ALDEN identifies linearly separable regions of samples. Then, it selects samples according to their diversity of local interpretations and queries their labels.
 </p>
 <p>
  Specifically, we first calculate the local interpretations in DNN for each sample as the gradient backpropagated from the final
predictions to the input features. Then, we use the most diverse interpretation of words in a sample to measure its diverseness. Accordingly, we select unlabeled samples with the maximally diverse interpretations for labeling and retrain the model with these
labeled samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_10.05.54_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>45. Robustness Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Randomized Smoothing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Denoised Smoothing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Denoised Smoothing
  </strong>
  is a method for obtaining a provably robust classifier from a fixed pretrained one, without any additional training or fine-tuning of the latter. The basic idea is to prepend a custom-trained denoiser before the pretrained classifier, and then apply randomized smoothing. Randomized smoothing is a certified defense that converts any given classifier $f$ into a new smoothed classifier $g$ that is characterized by a non-linear Lipschitz property. When queried at a point $x$, the smoothed classifier $g$ outputs the class that is most likely to be returned by $f$ under isotropic Gaussian perturbations of its inputs. Unfortunately, randomized smoothing requires that the underlying classifier $f$ is robust to relatively large random Gaussian perturbations of the input, which is not the case for off-the-shelf pretrained models. By applying our custom-trained denoiser to the classifier $f$, we can effectively make $f$ robust to such Gaussian perturbations, thereby making it “suitable” for randomized smoothing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.32.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fishr</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fishr
  </strong>
  is a learning scheme to enforce domain invariance in the space of the gradients of the loss function: specifically, it introduces a regularization term that matches the domain-level variances of gradients across training domains. Critically, the strategy exhibits close relations with the Fisher Information and the Hessian of the loss. Forcing domain-level gradient covariances to be similar during the learning procedure eventually aligns the domain-level loss landscapes locally around the final weights.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_1.29.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Randomized Deletion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>46. Position Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Absolute Position Encodings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Absolute Position Encodings
  </strong>
  are a type of position embeddings for [
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:
 </p>
 <p>
  $$ \text{PE}\left(pos, 2i\right) = \sin\left(pos/10000^{2i/d_{model}}\right) $$
 </p>
 <p>
  $$ \text{PE}\left(pos, 2i+1\right) = \cos\left(pos/10000^{2i/d_{model}}\right) $$
 </p>
 <p>
  where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\pi$ to $10000 \dot 2\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\text{PE}_{pos+k}$ can be represented as a linear function of $\text{PE}_{pos}$.
 </p>
 <p>
  Image Source:
  <a href="https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
   D2L.ai
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/05577c08-d6ac-4b8b-9fd0-55739ba42383.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Relative Position Encodings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Relative Position Encodings
  </strong>
  are a type of position embeddings for
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  that attempts to exploit pairwise, relative positional information. Relative positional information is supplied to the model on two levels: values and keys. This becomes apparent in the two modified self-attention equations shown below. First, relative positional information is supplied to the model as an additional component to the keys
 </p>
 <p>
  $$ e_{ij} = \frac{x_{i}W^{Q}\left(x_{j}W^{K} + a^{K}_{ij}\right)^{T}}{\sqrt{d_{z}}} $$
 </p>
 <p>
  Here $a$ is an edge representation for the inputs $x_{i}$ and $x_{j}$. The
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  operation remains unchanged from vanilla self-attention. Then relative positional information is supplied again as a sub-component of the values matrix:
 </p>
 <p>
  $$ z_{i} = \sum^{n}_{j=1}\alpha_{ij}\left(x_{j}W^{V} + a_{ij}^{V}\right)$$
 </p>
 <p>
  In other words, instead of simply combining semantic embeddings with absolute positional ones, relative positional information is added to keys and values on the fly during attention calculation.
 </p>
 <p>
  Source:
  <a href="https://jaketae.github.io/study/relative-positional-encoding/">
   Jake Tae
  </a>
 </p>
 <p>
  Image Source: [Relative Positional Encoding for Transformers with Linear Complexity](https://www.youtube.com/watch?v=qajudaEHuq8
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ceefba40-152a-41b0-840c-6446df1cd89b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ALiBi</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALiBi
  </strong>
  , or
  <strong>
   Attention with Linear Biases
  </strong>
  , is a
  <a href="https://paperswithcode.com/methods/category/position-embeddings">
   positioning method
  </a>
  that allows
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  language models to consume, at inference time, sequences which are longer than the ones they were trained on.
 </p>
 <p>
  ALiBi does this without using actual position embeddings. Instead, computing the attention between a certain key and query, ALiBi penalizes the attention value that that query can assign to the key depending on how far away the key and query are. So when a key and query are close by, the penalty is very low, and when they are far away, the penalty is very high.
 </p>
 <p>
  This method was motivated by the simple reasoning that words that are close-by matter much more than ones that are  far away.
 </p>
 <p>
  This method is as fast as the sinusoidal or absolute embedding methods (the fastest positioning methods there are). It outperforms those methods and Rotary embeddings when evaluating sequences that are longer than the ones the model was trained on (this is known as extrapolation).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-31_at_9.34.28_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Rotary Embeddings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Rotary Position Embedding
  </strong>
  , or
  <strong>
   RoPE
  </strong>
  , is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of being expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and capability of equipping the linear self-attention with relative position encoding.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_10.38.41_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>47. Recommendation Systems</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>LightGCN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LightGCN
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/gcn">
   graph convolutional neural network
  </a>
  (GCN), including only the most essential component in GCN (neighborhood aggregation) for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/224887f5-1251-4b8c-a8fe-99f25d47da65.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CPE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  CPE is an effective collaborative metric learning to effectively address the problem of sparse and insufficient preference supervision from the margin distribution point-of-view.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ComiRec</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ComiRec
  </strong>
  is a multi-interest framework for sequential recommendation. The multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.30.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SmeLU</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Please enter a description about the method here
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>48. Affinity Functions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Affinity Functions
    </strong>
    are pairwise functions used to represent a relationship between two entities. They were used in the context of non-local neural networks. Below you can find a list of different affinity functions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Embedded Gaussian Affinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embedded Gaussian Affinity
  </strong>
  is a type of affinity or self-similarity function between two points $\mathbf{x_{i}}$ and $\mathbf{x_{j}}$ that uses a Gaussian function in an embedding space:
 </p>
 <p>
  $$ f\left(\mathbf{x_{i}}, \mathbf{x_{j}}\right) = e^{\theta\left(\mathbf{x_{i}}\right)^{T}\phi\left(\mathbf{x_{j}}\right)} $$
 </p>
 <p>
  Here $\theta\left(x_{i}\right) = W_{θ}x_{i}$ and $\phi\left(x_{j}\right) = W_{φ}x_{j}$ are two embeddings.
 </p>
 <p>
  Note that the self-attention module used in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model is a special case of non-local operations in the embedded Gaussian version. This can be seen from the fact that for a given $i$, $\frac{1}{\mathcal{C}\left(\mathbf{x}\right)}\sum_{\forall{j}}f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)g\left(\mathbf{x}_{j}\right)$ becomes the
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  computation along the dimension $j$. So we have $\mathbf{y} = \text{softmax}\left(\mathbf{x}^{T}W^{T}_{\theta}W_{\phi}\mathbf{x}\right)g\left(\mathbf{x}\right)$, which is the self-attention form in the Transformer model. This shows how we can relate this recent self-attention model to the classic computer vision method of non-local means.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Embedded Dot Product Affinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embedded Dot Product Affinity
  </strong>
  is a type of affinity or self-similarity function between two points $\mathbb{x_{i}}$ and $\mathbb{x_{j}}$ that uses a dot product function in an embedding space:
 </p>
 <p>
  $$ f\left(\mathbb{x_{i}}, \mathbb{x_{j}}\right) = \theta\left(\mathbb{x_{i}}\right)^{T}\phi\left(\mathbb{x_{j}}\right) $$
 </p>
 <p>
  Here $\theta\left(x_{i}\right) = W_{θ}x_{i}$ and $\phi\left(x_{j}\right) = W_{φ}x_{j}$ are two embeddings.
 </p>
 <p>
  The main difference between the dot product and
  <a href="https://paperswithcode.com/method/embedded-gaussian-affinity">
   embedded Gaussian affinity
  </a>
  functions is the presence of
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  , which plays the role of an activation function.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Concatenation Affinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Concatenation Affinity
  </strong>
  is a type of affinity or self-similarity function between two points $\mathbb{x_{i}}$ and $\mathbb{x_{j}}$ that uses a concatenation function:
 </p>
 <p>
  $$ f\left(\mathbb{x_{i}}, \mathbb{x_{j}}\right) = \text{ReLU}\left(\mathbb{w}^{T}_{f}\left[\theta\left(\mathbb{x}_{i}\right), \phi\left(\mathbb{x}_{j}\right)\right]\right)$$
 </p>
 <p>
  Here $\left[·, ·\right]$ denotes concatenation and $\mathbb{w}_{f}$ is a weight vector that projects the concatenated vector to a scalar.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Gaussian Affinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gaussian Affinity
  </strong>
  is a type of affinity or self-similarity function between two points $\mathbb{x_{i}}$ and $\mathbb{x_{j}}$ that uses a Gaussian function:
 </p>
 <p>
  $$ f\left(\mathbb{x_{i}}, \mathbb{x_{j}}\right) = e^{\mathbb{x^{T}_{i}}\mathbb{x_{j}}} $$
 </p>
 <p>
  Here $\mathbb{x^{T}_{i}}\mathbb{x_{j}}$ is dot-product similarity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>49. Fourier-related Transforms</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Fourier-related Transforms
    </strong>
    are transforms related to Fourier Analysis. Below you can find a continuously updating list of transforms.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Discrete Cosine Transform</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Discrete Cosine Transform (DCT)
  </strong>
  is an orthogonal transformation method that decomposes an
image to its spatial frequency spectrum. It expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. It is used a lot in compression tasks, e..g image compression where for example high-frequency components can be discarded. It is a type of Fourier-related Transform, similar to discrete fourier transforms (DFTs), but only using real numbers.
 </p>
 <p>
  Image Credit:
  <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform#/media/File:Example_dft_dct.svg">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/2560px-Example_dft_dct.svg_P8zzYp9.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Characteristic Functions</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MCKERNEL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  McKernel introduces a framework to use kernel approximates in the mini-batch setting with Stochastic Gradient Descent (
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  ) as an alternative to Deep Learning.
 </p>
 <p>
  The core library was developed in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. The original intend was to implement a speedup of Random Kitchen Sinks (Rahimi and Recht 2007) by writing a very efficient HADAMARD tranform, which was the main bottleneck of the construction. The code though was later expanded at ETH Zürich (in McKernel by Curtó et al. 2017) to propose a framework that could explain both Kernel Methods and Neural Networks. This manuscript and the corresponding theses, constitute one of the first usages (if not the first) in the literature of FOURIER features and Deep Learning; which later got a lot of research traction and interest in the community.
 </p>
 <p>
  More information can be found in this presentation that the first author gave at ICLR 2020
  <a href="https://www.decurto.tw/c/iclr2020_DeCurto.pdf">
   iclr2020_DeCurto
  </a>
  .
 </p>
 <p>
  [1]
  <a href="https://www.curto.hk/c/decurto.pdf">
   https://www.curto.hk/c/decurto.pdf
  </a>
 </p>
 <p>
  [2]
  <a href="https://www.zarza.hk/z/dezarza.pdf">
   https://www.zarza.hk/z/dezarza.pdf
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/De_Curto_i_DiAz_et_al_2017_GsIovhL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>timecausgabor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The time-causal and time-recursive analogue of the Gabor transform provides a way to define a Gabor-like time-frequency analysis for real-time signals, for which the future cannot be accessed. This is achieved by choosing the temporal window function in a windowed Fourier transform as the time-causal limit kernel, which is a temporal kernel that is (i) time-causal, (ii) time-recursive and (iii) obeys temporal scale covariance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>50. Hyperparameter Search</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Hyperparameter Search
    </strong>
    methods are used to search for hyperparameters during the training stage of a neural network. Below you can find a continuously updating list of (specialized) hyperparameter search methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Random Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Search
  </strong>
  replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.
 </p>
 <p>
  Extracted from
  <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search">
   Wikipedia
  </a>
 </p>
 <p>
  Source
  <a href="https://dl.acm.org/doi/10.5555/2188385.2188395">
   Paper
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://dl.acm.org/doi/pdf/10.5555/2188385.2188395">
   BERGSTRA AND BENGIO
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.51.13_mfhxuoD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Dynamic algorithm configuration (DAC) is capable of generalizing over prior optimization approaches, as well as handling optimization of hyperparameters that need to be adjusted over multiple time-steps.
 </p>
 <p>
  Image Source:
  <a href="http://ecai2020.eu/papers/1237_paper.pdf">
   Biedenkapp et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.52.52_lOWtUby.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Population Based Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Population Based Training
  </strong>
  , or
  <strong>
   PBT
  </strong>
  , is an optimization method for finding parameters and hyperparameters, and extends upon parallel search methods and sequential optimisation methods.
It leverages information sharing across a population of concurrently running optimisation processes, and allows for online propagation/transfer of parameters and hyperparameters between members of the population based on their performance. Furthermore, unlike most other adaptation schemes, the method is capable of performing online adaptation of hyperparameters -- which can be particularly important in problems with highly non-stationary learning dynamics, such as reinforcement learning settings. PBT is decentralised and asynchronous, although it could also be executed semi-serially or with partial synchrony if there is a binding budget constraint.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_12.32.38_PM_Y92E9S5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Differentiable Hyperparameter Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Differentiable simultaneous optimization of hyperparameters and neural network architecture. Also a
  <a href="https://paperswithcode.com/method/neural-architecture-search">
   Neural Architecture Search
  </a>
  (NAS) method.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/multi_channel_net_2x2_x2x2x2_5UpurvB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>51. Non-Parametric Classification</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Non-Parametric Classification
    </strong>
    methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Gaussian Process</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gaussian Processes
  </strong>
  are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.
 </p>
 <p>
  Image Source: Gaussian Processes for Machine Learning, C. E. Rasmussen &amp; C. K. I. Williams
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_3.44.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SVM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Support Vector Machine
  </strong>
  , or
  <strong>
   SVM
  </strong>
  , is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”.
 </p>
 <p>
  Source:
  <a href="https://scikit-learn.org/stable/modules/svm.html">
   scikit-learn
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/sphx_glr_plot_separating_hyperplane_0011_7LTkwNL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>k-NN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   $k$-Nearest Neighbors
  </strong>
  is a clustering-based algorithm for classification and regression. It is a a type of instance-based learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Prediction is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.
 </p>
 <p>
  Source of Description and Image:
  <a href="https://scikit-learn.org/stable/modules/neighbors.html#classification">
   scikit-learn
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/sphx_glr_plot_nca_classification_thumb_uDrSiCw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MFF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  BCI MI signal Classification Framework using Fuzzy integrals.
 </p>
 <p>
  Paper: Ko, L. W., Lu, Y. C., Bustince, H., Chang, Y. C., Chang, Y., Ferandez, J., ... &amp; Lin, C. T. (2019). Multimodal fuzzy fusion for enhancing the motor-imagery-based brain computer interface. IEEE Computational Intelligence Magazine, 14(1), 96-106.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>52. Generalized Linear Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Generalized Linear Models (GLMs)
    </strong>
    are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Logistic Regression</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Logistic Regression
  </strong>
  , despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.
 </p>
 <p>
  Source:
  <a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">
   scikit-learn
  </a>
 </p>
 <p>
  Image:
  <a href="https://commons.wikimedia.org/wiki/User:Michaelg2015">
   Michaelg2015
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Exam_pass_logistic_curve_qJ3iJKr.jpeg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Linear Regression</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Linear Regression
  </strong>
  is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is
  <em>
   least squares
  </em>
  , where we minimize the mean square error between the predicted values $\hat{y} = \textbf{X}\hat{\beta}$ and actual values $y$: $\left(y-\textbf{X}\beta\right)^{2}$.
 </p>
 <p>
  We can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\hat{\beta}$.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Linear_regression">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/2560px-Linear_regression.svg_wwqz1f3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Procrustes</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Procrustes
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>53. Asynchronous Pipeline Parallel</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PipeDream</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  PipeDream is an asynchronous pipeline parallel strategy for training large neural networks. It adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.26.16_PM_H8uJn8L.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeDream-2BW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeDream-2BW
  </strong>
  is an asynchronous pipeline parallel method that supports memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators, and topologies and bandwidths of interconnects. PipeDream-2BW also determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint.
 </p>
 <p>
  The two main features are a double-buffered weight update (2BW) and flush mechanisms ensure high throughput. PipeDream-2BW
splits models into stages over multiple workers, and each stage is replicated an equal number of times (with data-parallel updates across replicas of the same stage).  Such parallel pipelines work well for models where each layer is repeated a fixed number of times (e.g.,
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  models).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.29.19_PM_8a0zspQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PipeMare</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PipeMare
  </strong>
  is an asynchronous (bubble-free) pipeline parallel method for training large neural networks. It involves two main techniques: learning rate rescheduling and discrepancy correction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.38.39_PM_K5cqhcI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pipelined Backpropagation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pipelined Backpropagation
  </strong>
  is an asynchronous pipeline parallel training algorithm. It was first introduced by Petrowski et al (1993). It avoids fill and drain overhead by updating the weights without draining the pipeline first. This results in weight inconsistency, the use of different weights on the forward and backward passes for a given micro-batch. The weights used to produce a particular gradient may also have been updated when the gradient is applied, resulting in stale (or delayed) gradients. For these reasons PB resembles Asynchronous
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  and is not equivalent to standard SGD. Finegrained pipelining increases the number of pipeline stages and hence increases the weight inconsistency and delay.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.40.28_PM_k1eTzBU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>54. Replicated Data Parallel</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PyTorch DDP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PyTorch DDP
  </strong>
  (Distributed Data Parallel) is a distributed data parallel implementation for PyTorch. To guarantee mathematical equivalence, all replicas start from the same initial values for model parameters and synchronize gradients to keep parameters consistent across training iterations. To minimize the intrusiveness, the implementation exposes the same forward API as the user model, allowing applications to seamlessly replace subsequent occurrences of a user model with the distributed data parallel model object with no additional code changes. Several techniques are integrated into the design to deliver high-performance training, including bucketing gradients, overlapping communication with computation, and skipping synchronization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.14.24_PM_bwiSAyG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BAGUA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BAGUA
  </strong>
  is a communication framework whose design goal is to provide a system abstraction that is both flexible and modular to support state-of-the-art system relaxation techniques of distributed training. The abstraction goes beyond parameter server and Allreduce paradigms, and provides a collection of MPI-style collective operations to facilitate communications with different precision and centralization strategies.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.42.11_AM_YssuQWS.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ByteScheduler</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ByteScheduler
  </strong>
  is a generic communication scheduler for distributed DNN training acceleration. It is based on analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_10.46.42_AM_NJSaHRh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DABMD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Distributed Any-Batch Mirror Descent
  </strong>
  (DABMD) is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>55. Model Compression</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Pruning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Soups</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Compress an ensemble of models into a single one by averaging their weights (under certain pre-conditions).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NNCF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neural Network Compression Framework
  </strong>
  , or
  <strong>
   NNCF
  </strong>
  , is a Python-based framework for neural network compression with fine-tuning. It leverages recent advances of various network compression methods and implements some of them, namely quantization, sparsity, filter pruning and binarization. These methods allow producing more hardware-friendly models that can be efficiently run on general-purpose hardware computation units (CPU, GPU) or specialized deep learning accelerators.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_1.10.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CORAD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>56. Information Retrieval Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>TILDEv2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TILDEv2
  </strong>
  is a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  -based re-ranking method that stems from
  <a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462922">
   TILDE
  </a>
  but that addresses its limitations. It relies on contextualized exact term matching with expanded passages. This requires to only store in the index the score of tokens that appear in the expanded passages (rather than all the vocabulary), thus producing indexes that are 99% smaller than those of the original.
 </p>
 <p>
  Specifically, TILDE is modified in the following aspects:
 </p>
 <ul>
  <li>
   <p>
    <strong>
     Exact Term Matching
    </strong>
    . The query likelihood matching originally employed in TILDE, expands passages into the BERT vocabulary size, resulting in large indexes. To overcome this issue, estimating relevance scores is achieved with contextualized exact term matching. This allows the model to index tokens only present in the passage, thus reducing the index size. In addition to this, we replace the query likelihood loss function, with the Noise contrastive estimation (NCE) loss that allows to better leverage negative training samples.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Passage Expansion
    </strong>
    . To overcome the vocabulary mismatch problem that affects exact term matching methods, passage expansion is used to expand the original passage collection. Passages in the collection are expanded using deep LMs with a limited number of tokens. This requires TILDEv2 to only index a few extra tokens in addition to those in the original passages.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.35.46_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SERAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ReInfoSelect</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ReInfoSelect
  </strong>
  is a reinforcement weak supervision selection method for information retrieval. It learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker's performance peaks on target relevance metrics (convergence).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.39.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>57. SLAM Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DROID-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DROID-SLAM
  </strong>
  is a deep learning based SLAM system. It consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. This layer leverages geometric constraints, improves accuracy and robustness, and enables a monocular system to handle stereo or RGB-D input without retraining. It builds a dense 3D map of the environment while simultaneously localizing the camera within the map.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_1.30.38_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NICE-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NICE-SLAM, a dense RGB-D SLAM system that combines neural implicit decoders with hierarchical grid-based representations, which can be applied to large-scale scenes.
 </p>
 <p>
  Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/de7409fc-3ba3-4fe5-8c4f-f597722814e1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NeuralRecon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NeuralRecon
  </strong>
  is a framework for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, NeuralRecon proposes to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.52.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VDO-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VDO-SLAM
  </strong>
  is a feature-based stereo/RGB-D dynamic SLAM system that leverages image-based semantic information to simultaneously localise the robot, map the static and dynamic structure, and track motions of rigid objects in the scene. Input images are first pre-processed to generate instance-level object segmentation and dense optical flow. These are then used to track features on static background structure and dynamic objects. Camera poses and object motions estimated from feature tracks are then refined in a global batch optimisation, and a local map is maintained and updated with every new frame.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.57.07_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Meshing</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>NICE-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NICE-SLAM, a dense RGB-D SLAM system that combines neural implicit decoders with hierarchical grid-based representations, which can be applied to large-scale scenes.
 </p>
 <p>
  Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/de7409fc-3ba3-4fe5-8c4f-f597722814e1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NeuralRecon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NeuralRecon
  </strong>
  is a framework for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, NeuralRecon proposes to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.52.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>58. Quantum Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>gCANS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   gCANS
  </strong>
  , or
  <strong>
   Global Coupled Adaptive Number of Shots
  </strong>
  , is a variational quantum algorithm for stochastic gradient descent. It adaptively allocates shots for the measurement of each gradient component at each iteration. The optimizer uses a criterion for allocating shots that incorporates information about the overall scale of the shot cost for the iteration.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_10.44.30_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VQSVD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Variational Quantum Singular Value Decomposition
  </strong>
  is a variational quantum algorithm for singular value decomposition (VQSVD). By exploiting the variational principles for singular values and the Ky Fan Theorem, a novel loss function is designed such that two quantum neural networks (or parameterized quantum circuits) could be trained to learn the singular vectors and output the corresponding singular values.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_3.28.04_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VTDE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Variational Trace Distance Estimation
  </strong>
  , or
  <strong>
   VTDE
  </strong>
  , is a variational algorithm for trace norm estimation that only involves one ancillary qubit. Notably, the cost function in VTDE gathers information from a single-qubit observable and thus could avoid the barren plateau issue with logarithmic depth parameterized circuits.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_3.29.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Variational Entanglement Detection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Variational Entanglement Detection
  </strong>
  is a variational quantum algorithm which uses criteria based on positive maps as a bridge and works as follows. Given an unknown target bipartite quantum state, it firstly decomposes the chosen positive map into a linear combination of NISQ implementable quantum operations. Then, it variationally estimates the minimal eigenvalue of the final state, obtained by executing these quantum operations on the target state and averaging the output states. Deterministic and probabilistic methods are proposed to compute the average. At last, it asserts that the target state is entangled if the optimized minimal eigenvalue is negative. VLNE builds upon a linear decomposition of the transpose map into Pauli terms and the recently proposed trace distance estimation algorithm. It variationally estimates the well-known logarithmic negativity entanglement measure and could be applied to quantify entanglement on near-term quantum devices.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_3.31.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>59. Parameter Server Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parallax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parallax
  </strong>
  is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.
 </p>
 <p>
  Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.04.30_PM_HqxkPUa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Herring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Herring
  </strong>
  is a parameter server based distributed training method. It combines AWS's Elastic Fabric
  <a href="https://paperswithcode.com/method/adapter">
   Adapter
  </a>
  (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.39.12_PM_uue6cPp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BytePS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BytePS
  </strong>
  is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_2.21.50_PM_lsD4NtJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HetPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HetPipe
  </strong>
  is a hybrid parallel method that integrates pipelined model parallelism (PMP) with data parallelism (DP). In HetPipe, a group of multiple GPUs, called a virtual worker, processes minibatches in a pipelined manner, and multiple such virtual workers employ data parallelism for higher performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.08.59_AM_JmC5QuV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>60. Skip Connections</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Skip Connections
    </strong>
    allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Residual Connection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Connections
  </strong>
  are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}({x}):=\mathcal{H}({x})-{x}$. The original mapping is recast into $\mathcal{F}({x})+{x}$.
 </p>
 <p>
  The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/resnet-e1548261477164.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Concatenated Skip Connection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Concatenated Skip Connection
  </strong>
  is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.33.17_PM_mbtzy8R.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Zero-padded Shortcut Connection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Zero-padded Shortcut Connection
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/residual-connection">
   residual connection
  </a>
  used in the
  <a href="https://paperswithcode.com/method/pyramidnet">
   PyramidNet
  </a>
  architecture. For PyramidNets, identity mapping alone cannot be used for a shortcut because the feature map dimension differs among individual residual units. Therefore, only a zero-padded shortcut or projection shortcut can be used for all the residual units. However,  a projection shortcut can hamper information propagation and lead to optimization problems, especially for very deep networks. On the other hand, the zero-padded shortcut avoids the overfitting problem because no additional parameters exist.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_4.55.18_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deactivable Skip Connection</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Deactivable Skip Connection
  </strong>
  is a type of skip connection which, instead of concatenating the encoder features
(red) and decoder features (blue), as with
  <a href="https://paperswithcode.com/methods/category/skip-connections">
   standard skip connections
  </a>
  , it instead fuses the encoder features with part of the decoder features (light blue), to be able to deactivate this operation when needed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.44.55_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>61. Rule-based systems</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>WFST</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Symbolic rule learning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Symbolic rule learning methods find regularities in data that can be expressed in the form of 'if-then' rules based on symbolic representations of the data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAFRAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  SAFRAN is a rule application framework which aggregates rules through a scalable clustering algorithm.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>62. Structured Prediction</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Structured Prediction
    </strong>
    methods deal with structured outputs with multiple interdependent outputs. Below you can find a continuously updating list of structured prediction methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>CRF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Conditional Random Fields
  </strong>
  or
  <strong>
   CRFs
  </strong>
  are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.
 </p>
 <p>
  Image Credit:
  <a href="https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">
   Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_3.09.52_PM_1DL5Qbz.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LTLS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LTLS
  </strong>
  is a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. It tackles extreme multi-class and multi-label classification problems where the size $C$ of the output space is extremely large.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-31_at_11.30.31_AM_AUce67j.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DMVFN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>63. Markov Chain Monte Carlo</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The golden standard for uncertainty quantification and Bayesian inference.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-11-04_at_9.01.09_PM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Metropolis Hastings</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Metropolis-Hastings
  </strong>
  is a Markov Chain Monte Carlo (MCMC) algorithm for approximate inference. It allows for sampling from a probability distribution where direct sampling is difficult - usually owing to the presence of an intractable integral.
 </p>
 <p>
  M-H consists of a proposal distribution $q\left(\theta^{'}\mid\theta\right)$ to draw a parameter value. To decide whether $\theta^{'}$ is accepted or rejected, we then calculate a ratio:
 </p>
 <p>
  $$ \frac{p\left(\theta^{'}\mid{D}\right)}{p\left(\theta\mid{D}\right)} $$
 </p>
 <p>
  We then draw a random number $r \in \left[0, 1\right]$ and accept if it is under the ratio, reject otherwise. If we accept, we set $\theta_{i} = \theta^{'}$ and repeat.
 </p>
 <p>
  By the end we have a sample of $\theta$ values that we can use to form quantities over an approximate posterior, such as the expectation and uncertainty bounds. In practice, we typically have a period of tuning to achieve an acceptable acceptance ratio for the algorithm, as well as a warmup period to reduce bias towards initialization values.
 </p>
 <p>
  Image:
  <a href="https://static1.squarespace.com/static/52e69d46e4b05a145935f24d/t/5a7dbadcf9619a745c5b2513/1518189289690/Stan.pdf">
   Samuel Hudec
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_7.08.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Simulations of multi-modal distributions can be very costly and often lead to unreliable predictions. To accelerate the computations, we propose to sample from a flattened distribution to accelerate the computations and estimate the importance weights between the original distribution and the flattened distribution to ensure the correctness of the distribution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.08.22_PM_qx9FEwa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>reSGLD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  reSGLD proposes to simulate a high-temperature particle for exploration and a low-temperature particle for exploitation and allows them to swap simultaneously. Moreover, a correction term is included to avoid biases.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-04_at_9.01.09_PM_OCFN35j.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>64. Auto Parallel Methods</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed auto parallel methods for scaling deep learning to very large models. Auto parallel methods involve strategies for optimizing steps of parallelization, including hyperparameter tuning and model replication and partitioning.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>AutoSync</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AutoSync
  </strong>
  is a pipeline for automatically optimizing synchronization strategies, given model structures and resource specifications, in data-parallel distributed machine learning. By factorizing the synchronization strategy with respect to each trainable building block of a DL model, we can construct a valid and large strategy space spanned by multiple factors. AutoSync efficiently navigates the space and locates the optimal strategy. AutoSync leverages domain knowledge about synchronization systems to reduce the search space, and is equipped with a domain adaptive simulator, which combines principled communication modeling and data-driven ML models, to estimate the runtime of strategy proposals without launching real distributed execution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.17.06_AM_1erK68k.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>KungFu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   KungFu
  </strong>
  is a distributed ML library for TensorFlow that is designed to enable adaptive training. KungFu allows users to express high-level Adaptation Policies (APs) that describe how to change hyper- and system parameters during training. APs take real-time monitored metrics (e.g. signal-to-noise ratios and noise scale) as input and trigger control actions (e.g. cluster rescaling or synchronisation strategy updates). For execution, APs are translated into monitoring and control operators, which are embedded in the dataflow graph. APs exploit an efficient asynchronous collective communication layer, which ensures concurrency and consistency
of monitoring and adaptation operations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.23.04_AM_M8VhgxU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FlexFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FlexFlow
  </strong>
  is a deep learning engine that uses guided randomized search of the SOAP (Sample, Operator, Attribute, and Parameter) space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy’s performance and is three orders of magnitude faster than prior approaches that execute each strategy.
 </p>
 <p>
  FlexFlow uses two main components: a fast, incremental execution simulator to evaluate different parallelization strategies, and a Markov Chain Monte Carlo (MCMC) search algorithm that takes advantage of the incremental simulator to rapidly explore the large search space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.24.22_AM_xqSP19s.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>65. Ensembling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EMEA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Entropy Minimized Ensemble of Adapters
  </strong>
  , or
  <strong>
   EMEA
  </strong>
  , is a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. The intuition behind the method is that a good
  <a href="https://paperswithcode.com/method/adapter">
   adapter
  </a>
  weight $\alpha$ for a test input $x$ should make the model more confident in its prediction for $x$, that is, it should lead to lower model entropy over the input
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.59.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>66. Prompt Engineering</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Prompt engineering is a practice of creating a large number of prompts to more efficiently extract information from Language Models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>CoT Prompting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Chain-of-thought prompts contain a series of intermediate reasoning steps, and they are shown to significantly improve the ability of large language models to perform certain tasks that involve complex reasoning (e.g., arithmetic, commonsense reasoning, symbolic reasoning, etc.)
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CoOp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CoOp
  </strong>
  , or
  <strong>
   Context Optimization
  </strong>
  , is an automated prompt engineering method that avoids manual prompt tuning by modeling context words with continuous vectors that are end-to-end learned from data. The context could be shared among all classes or designed to be class-specific. During training, we simply minimize the prediction error using the cross-entropy loss with respect to the learnable context vectors, while keeping the pre-trained parameters fixed. The gradients can be back-propagated all the way through the text encoder, distilling the rich knowledge encoded in the parameters for learning task-relevant context.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.30.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>KnowPrompt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   KnowPrompt
  </strong>
  is a prompt-tuning approach for relational understanding. It injects entity and relation knowledge into prompt construction with learnable virtual template words as well as answer words and synergistically optimize their representation with knowledge constraints. To be specific, TYPED MARKER is utilized around entities initialized with aggregated entity-type embeddings as learnable virtual template words to inject entity type knowledge. The average embeddings of each token are leveraged in relation labels as virtual answer words to inject relation knowledge. Since there exist implicit structural constraints among entities and relations, and virtual words should be consistent with the surrounding contexts, synergistic optimization is introduced to obtain optimized virtual templates and answer words. Concretely, a context-aware prompt calibration method is used with implicit structural constraints to inject structural knowledge implications among relational triples and associate prompt embeddings with each other.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.01.45_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>67. Variational Optimization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Variational Optimization
    </strong>
    techniques are used to improve the optimization of variational based methods, e.g. variational optimization of variational autoencoders. Below you can find a continuously updating list of variational optimization methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GECO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Residual Normal Distribution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Normal Distributions
  </strong>
  are used to help the optimization of VAEs, preventing optimization from entering an unstable region. This can happen due to sharp gradients caused in situations where the encoder and decoder produce distributions far away from each other. The residual distribution parameterizes $q\left(\mathbf{z}|\mathbf{x}\right)$ relative to $p\left(\mathbf{z}\right)$. Let $p\left(z^{i}_{l}|\mathbf{z}_{&lt;l}\right) := N \left(\mu_{i}\left(\mathbf{z}_{&lt;l}\right), \sigma_{i}\left(\mathbf{z}_{&lt;l}\right)\right)$ be a Normal distribution for the $i$th variable in $\mathbf{z}_{l}$ in prior. Define $q\left(z^{i}_{l}|\mathbf{z}_{&lt;l}, x\right) := N\left(\mu_{i}\left(\mathbf{z}_{&lt;l}\right) + \Delta\mu_{i}\left(\mathbf{z}_{&lt;l}, x\right), \sigma_{i}\left(\mathbf{z}_{&lt;l}\right) \cdot \Delta\sigma_{i}\left(\mathbf{z}_{&lt;l}, x\right) \right)$, where $\Delta\mu_{i}\left(\mathbf{z}_{&lt;l}, \mathbf{x}\right)$ and $\Delta\sigma_{i}\left(\mathbf{z}_{&lt;l}, \mathbf{x}\right)$ are the relative location and scale of the approximate posterior with respect to the prior. With this parameterization, when the prior moves, the approximate posterior moves accordingly, if not changed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-13_at_1.28.32_PM_W4X3dBe.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ASVI</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Automatic Structured Variational Inference (ASVI)
  </strong>
  is a fully automated method for constructing structured variational families, inspired by the closed-form update in conjugate Bayesian models. These convex-update families incorporate the forward pass of the input probabilistic program and can therefore capture complex statistical dependencies. Convex-update families have the same space and time complexity as the input probabilistic program and are therefore tractable for a very large family of models including both continuous and discrete variables.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.31.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>68. Parameter Norm Penalties</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Parameter Norm Penalties
    </strong>
    are regularization methods that apply a penalty to the norm of parameters in the objective function of a neural network. Below you can find a continuously updating list of parameter norm penalties.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Weight Decay</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Decay
  </strong>
  , or
  <strong>
   $L_{2}$ Regularization
  </strong>
  , is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{w^{T}w}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty (encouraging smaller weights).
 </p>
 <p>
  Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).
 </p>
 <p>
  Image Source: Deep Learning, Goodfellow et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.15.13_PM_YGbJW74.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>L1 Regularization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   $L_{1}$ Regularization
  </strong>
  is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{1}$ Norm of the weights:
 </p>
 <p>
  $$L_{new}\left(w\right) = L_{original}\left(w\right) + \lambda{||w||}_{1}$$
 </p>
 <p>
  where $\lambda$ is a value determining the strength of the penalty. In contrast to
  <a href="https://paperswithcode.com/method/weight-decay">
   weight decay
  </a>
  , $L_{1}$ regularization promotes sparsity; i.e. some parameters have an optimal value of zero.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Sparsityl1.png">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.02.16_AM_iDsext7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ROME</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>69. Gated Linear Networks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GLN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Gated Linear Network
  </strong>
  , or
  <strong>
   GLN
  </strong>
  , is a type of backpropagation-free neural architecture. What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization.
 </p>
 <p>
  GLNs are feedforward networks composed of many layers of gated geometric mixing neurons as shown in the Figure . Each neuron in a given layer outputs a gated geometric mixture of the predictions from the previous layer, with the final layer consisting of just a single neuron. In a supervised learning setting, a $\mathrm{GLN}$ is trained on (side information, base predictions, label) triplets $\left(z_{t}, p_{t}, x_{t}\right)_{t=1,2,3, \ldots}$ derived from input-label pairs $\left(z_{t}, x_{t}\right)$. There are two types of input to neurons in the network: the first is the side information $z_{t}$, which can be thought of as the input features; the second is the input to the neuron, which will be the predictions output by the previous layer, or in the case of layer 0 , some (optionally) provided base predictions $p_{t}$ that typically will be a function of $z_{t} .$ Each neuron will also take in a constant bias prediction, which helps empirically and is essential for universality guarantees.
 </p>
 <p>
  Weights are learnt in a Gated Linear Network using Online Gradient Descent (OGD) locally at each neuron. They key observation is that as each neuron $(i, k)$ in layers $i&gt;0$ is itself a gated geometric mixture, all of these neurons can be thought of as individually predicting the target. Given side information $z$ , each neuron $(i, k)$ suffers a loss convex in its active weights $u:=w_{i k c_{i k}(z)}$ of
$$
\ell_{t}(u):=-\log \left(\operatorname{GEO}_{u}\left(x_{t} ; p_{i-1}\right)\right)
$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0438b267-5b4e-4aae-96b0-986f5be4f996.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>G-GLN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gaussian Gated Linear Network
  </strong>
  , or
  <strong>
   G-GLN
  </strong>
  , is a multi-variate extension to the recently proposed
  <a href="https://paperswithcode.com/method/gln">
   GLN
  </a>
  family of deep neural networks by reformulating the GLN neuron as a gated product of Gaussians. This Gaussian Gated Linear Network (G-GLN) formulation exploits the fact that exponential family densities are closed under multiplication, a property that has seen much use in
  <a href="https://paperswithcode.com/method/gaussian-process">
   Gaussian Process
  </a>
  and related literature. Similar to the Bernoulli GLN, every neuron in the G-GLN directly predicts the target distribution.
 </p>
 <p>
  Precisely, a G-GLN is a feed-forward network of data-dependent distributions. Each neuron calculates the sufficient statistics $\left(\mu, \sigma_{2}\right)$ for its associated PDF using its active weights, given those emitted by neurons in the preceding layer. It consists of consists of $L+1$ layers indexed by $i \in{0, \ldots, L}$ with $K_{i}$ neurons in each layer. The weight space for a neuron in layer $i$ is denoted by $\mathcal{W}_{i}$; the subscript is needed since the dimension of the weight space depends on $K_{i-1}$. Each neuron/distribution is indexed by its position in the network when laid out on a grid; for example, $f_{i k}$ refers to the family of PDFs defined by the $k$ th neuron in the $i$ th layer. Similarly, $c_{i k}$ refers to the context function associated with each neuron in layers $i \geq 1$, and $\mu_{i k}$ and $\sigma_{i k}^{2}$ (or $\Sigma_{i k}$ in the multivariate case) referring to the sufficient statistics for each Gaussian PDF.
 </p>
 <p>
  There are two types of input to neurons in the network. The first is the side information, which can be thought of as the input features, and is used to determine the weights used by each neuron via half-space gating. The second is the input to the neuron, which is the PDFs output by the previous layer, or in the case of layer 0, some provided base models. To apply a G-GLN in a supervised learning setting, we need to map the sequence of input-label pairs $\left(x_{t}, y_{t}\right)$ for $t=1,2, \ldots$ onto a sequence of (side information, base Gaussian PDFs, label) triplets $\left(z_{t},\left(f_{0 i}\right)_{i}, y_{t}\right)$. The side information $z_{t}$ is set to the (potentially normalized) input features $x_{t}$. The Gaussian PDFs for layer 0 will generally include the necessary base Gaussian PDFs to span the target range, and optionally some base prediction PDFs that capture domain-specific knowledge.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/430c75be-fe8a-4854-87e1-05e16a241324.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>G-GLN Neuron</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   G-GLN Neuron
  </strong>
  is a type of neuron used in the
  <a href="https://paperswithcode.com/method/g-gln">
   G-GLN
  </a>
  architecture. G-GLN. The key idea is that further representational power can be added to a weighted product of Gaussians via a contextual gating procedure. This is achieved by extending a weighted product of Gaussians model with an additional type of input called side information. The side information will be used by a neuron to select a weight vector to apply for a given example from a table of weight vectors. In typical applications to regression, the side information is defined as the (normalized) input features for an input example: i.e. $z=(x-\bar{x}) / \sigma_{x}$.
 </p>
 <p>
  More formally, associated with each neuron is a context function $c: \mathcal{Z} \rightarrow \mathcal{C}$, where $\mathcal{Z}$ is the set of possible side information and $\mathcal{C}={0, \ldots, k-1}$ for some $k \in \mathbb{N}$ is the context space. Each neuron $i$ is now parameterized by a weight matrix $W_{i}=\left[w_{i, 0} \ldots w_{i, k-1}\right]^{\top}$ with each row vector $w_{i j} \in \mathcal{W}$ for $0 \leq j&lt;k$. The context function $c$ is responsible for mapping side information $z \in \mathcal{Z}$ to a particular row $w_{i, c(z)}$ of $W_{i}$, which we then use to weight the Product of Gaussians. In other words, a G-GLN neuron can be defined by:
 </p>
 <p>
  $$
\operatorname{PoG}_{W}^{c}\left(y ; f_{1}(\cdot), \ldots, f_{m}(\cdot), z\right):=\operatorname{PoG}_{w^{c(z)}}\left(y ; f_{1}(\cdot), \ldots, f_{m}(\cdot)\right)
$$
 </p>
 <p>
  with the associated loss function $-\log \left(\operatorname{PoG}_{W}^{c}\left(y ; f_{1}(y), \ldots, f_{m}(y), z\right)\right)$ inheriting all the properties needed to apply Online Convex Programming.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0a50bba7-2a31-4e5e-b32f-8fbefeca1247.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>70. Adaptive Computation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PonderNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PonderNet
  </strong>
  is an adaptive computation method that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/99d96967-3912-44fe-9930-b0ea32ed42a3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AdaptiveBins</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PoAPL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   trainable layer
  </strong>
  that encodes feature vectors onto 2 rotational coordinates $R_y(\theta),R_z(\gamma)$ for a unit sphere.
  <em>
   Used to map data onto the Bloch Sphere surface for qubits
  </em>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3dfbb728-14cb-446c-9dc4-a2c72a5a4792.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>71. Intra-Layer Parallel</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GShard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GShard
  </strong>
  is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.11.31_PM_8iMLLNb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Tofu</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Tofu
  </strong>
  is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.14.55_PM_X6x7Lzy.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mesh-TensorFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mesh-TensorFlow
  </strong>
  is a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A MeshTensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_1.07.19_PM_YjSCU38.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>72. Sharded Data Parallel Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ZeRO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Zero Redundancy Optimizer (ZeRO)
  </strong>
  is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Infinity</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ZeRO-Infinity
  </strong>
  is a sharded data parallel system that extends
  <a href="https://paperswithcode.com/method/zero">
   ZeRO
  </a>
  with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinity to support massive model sizes on limited GPU resources by exploiting CPU and NVMe memory simultaneously. In addition, ZeRO-Infinity also introduces a novel GPU memory optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.23.58_PM_lX9OD5c.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZeRO-Offload</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  ZeRO-Offload is a sharded data parallel method for distributed training. It exploits both CPU memory and compute for offloading, while offering a clear path towards efficiently scaling on multiple GPUs by working with
  <a href="https://www.paperswithcode.com/method/zero">
   ZeRO-powered data parallelism
  </a>
  . The symbiosis allows ZeRO-Offload to maintain a single copy of the optimizer states on the CPU memory regardless of the data parallel degree. Furthermore, it keeps the aggregate communication volume between GPU and CPU, as well as the aggregate CPU computation a constant regardless of data parallelism, allowing ZeRO-Offload to effectively utilize the linear increase in CPU compute with the increase in the data parallelism degree.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.26.18_PM_n7gzPNO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>73. Feature Matching</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Good Feature Matching</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Good Feature Matching
  </strong>
  is an active map-to-frame feature matching method. Feature matching effort is tied to submatrix selection, which has combinatorial time complexity and requires choosing a scoring metric. Via simulation, the Max-logDet matrix revealing metric is shown to perform best.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.22.44_AM_7YuKNI0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MaskFlownet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MaskFlownet
  </strong>
  is an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.51.25_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Reliability Balancing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>74. Information Bottleneck</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Aggregated Learning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Aggregated Learning (AgrLearn)
  </strong>
  is a vector-quantization approach to learning neural network classifiers. It builds on an equivalence between IB learning and IB quantization and exploits the power of vector quantization, which is well known in information theory.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.08.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ReInfoSelect</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ReInfoSelect
  </strong>
  is a reinforcement weak supervision selection method for information retrieval. It learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker's performance peaks on target relevance metrics (convergence).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.39.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>75. Inference Attack</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Canvas Method</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Canvas Method
  </strong>
  is a method for inference attacks on object detection models. It draws a predicted bounding box distribution on an empty canvas for an attack model input. The canvas is initially set to an image of 300$\times$300 pixels in size, where every pixel has a value of zero and the boxes drawn on the canvas have the same center as the predicted boxes and the same intensity as the prediction scores.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.14.17_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spectral DeTuning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, Spectral DeTuning aims to recover the exact pre-fine-tuning
  <em>
   weights
  </em>
  . Spectral DeTuning can exploit this vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>76. Pruning</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Movement Pruning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Movement Pruning
  </strong>
  is a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. Magnitude pruning can be seen as utilizing zeroth-order information (absolute value) of the running model. In contrast, movement pruning methods are where importance is derived from first-order information. Intuitively, instead of selecting weights that are far from zero, we retain connections that are moving away from zero during the training process.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_1.43.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HRank</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HRank
  </strong>
  is a filter pruning method that explores the High Rank of the feature map in each layer (HRank). The proposed HRank  is inspired by the discovery that the average rank of multiple feature maps generated by a single filter is always the same, regardless of the number of image batches CNNs receive. Based on HRank, the authors develop a method that is mathematically formulated to prune filters with low-rank feature maps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_1.16.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spectral-Normalized Identity Priors</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Spectral-Normalized Identity Priors
  </strong>
  , or
  <strong>
   SNIP
  </strong>
  , is a structured pruning approach that penalizes an entire
  <a href="https://paperswithcode.com/method/residual-connection">
   residual module
  </a>
  in a
  <a href="https://paperswithcode.com/method/residual-connection">
   Transformer model
  </a>
  toward an identity mapping. It is applicable to any structured module, including a single
  <a href="https://paperswithcode.com/method/scaled">
   attention head
  </a>
  , an
  <a href="https://paperswithcode.com/method/multi-head-attention">
   entire attention block
  </a>
  , or a
  <a href="https://paperswithcode.com/method/position-wise-feed-forward-layer">
   feed-forward subnetwork
  </a>
  . The method identifies and discards unimportant non-linear mappings in the
  <a href="https://paperswithcode.com/method/residual-connection">
   residual connections
  </a>
  by applying a thresholding operator on the function norm. Furthermore,
  <a href="https://paperswithcode.com/method/spectral-normalization">
   spectral normalization
  </a>
  to stabilize the distribution of the post-activation values of the
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  layers, further improving the pruning effectiveness of the proposed methodology.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/d5ac051a-9e97-4f9f-8adc-db4faf73cd9f.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>77. Asynchronous Data Parallel</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Crossbow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Crossbow
  </strong>
  is a single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size—however small—while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method.
  <a href="https://paperswithcode.com/method/slime-mould-algorithm-sma">
   SMA
  </a>
  , a synchronous variant of model averaging, is used in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.40.01_PM_cRpRUA0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SlowMo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Slow Momentum
  </strong>
  (SlowMo) is a distributed optimization method where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm.  Periodically, after taking some number $\tau$ of base algorithm steps, workers average their parameters using ALLREDUCE and perform a momentum update.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.37.05_PM_fSm5arz.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Wavelet Distributed Training</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wavelet
  </strong>
  is an asynchronous data parallel approach that interleaves waves of training tasks on the same group of GPUs, such that tasks belong to one wave can leverage on-device memory from tasks in another wave during their memory valley period, thus boost-up the training throughput. As shown in the Figure, Wavelet divides dataparallel training tasks into two waves, namely tick-wave and tock-wave. The task launching offset is achieved by delaying the launch time of tock-wave tasks for half of a whole forward-backward training cycle. Therefore, the tock-wave tasks can directly leverage GPU memory valley period of tick-wave tasks (e.g. 0.4s-0.6s in Figure 2(a)), since backward propagation of tick-wave tasks is compute-heavy but memory is often unused. Similarly, tick-wave tasks can leverage memory valley period of tock-wave tasks in the same way.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.32.39_PM_zK3TmHK.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>78. Robotic Manipulation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>VSF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VisuoSpatial Foresight
  </strong>
  is a method for robotic fabric manipulation that leverages a combination of RGB and depth information to learn goal conditioned fabric manipulation policies for a variety of long horizon tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.46.05_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>myGym</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  We introduce myGym, a toolkit suitable for fast prototyping of neural networks in the area of robotic manipulation and navigation. Our toolbox is fully modular, enabling users to train their algorithms on different robots, environments, and tasks. We also include pretrained neural network modules for the real-time vision that allows training visuomotor tasks with sim2real transfer. The visual modules can be easily retrained using the dataset generation pipeline with domain augmentation and randomization. Moreover, myGym provides automatic evaluation methods and baselines that help the user to directly compare their trained model with the state-of-the-art algorithms. We additionally present a novel metric, called learnability, to compare the general learning capability of algorithms in different settings, where the complexity of the environment, robot, and the task is systematically manipulated. The learnability score tracks differences between the performance of algorithms in increasingly challenging setup conditions, and thus allows the user to compare different models in a more systematic fashion. The code is accessible at https://github.com/incognite-lab/myGym
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ec34d991-eb11-4e73-b941-30f5e3e2d144.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>79. Action Recognition Blocks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>TPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Temporal Pyramid Network
  </strong>
  , or
  <strong>
   TPN
  </strong>
  , is a pyramid level module for action recognition at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. The source of features and the fusion of features form a feature hierarchy for the backbone so that it can capture action instances at various tempos. In the TPN, a Backbone Network is used to extract multiple level features, a Spatial Semantic Modulation spatially downsamples features to align semantics, a Temporal Rate Modulation temporally downsamples features to adjust relative tempo among levels, Information Flow aggregates features in various directions to enhance and enrich level-wise representations and Final Prediction rescales and concatenates all levels of pyramid along channel dimension.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_11.00.37_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>G3D</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   G3D
  </strong>
  is a unified spatial-temporal graph convolutional operator that directly models cross-spacetime joint dependencies. It leverages dense cross-spacetime edges as skip connections for direct information propagation across the 3D spatial-temporal graph.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_9.52.18_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>80. Latent Variable Sampling</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Latent Variable Sampling
    </strong>
    methods are approaches for sampling from a latent variable. Below you can find a continuously updating list of latent variable sampling methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Truncation Trick</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Truncation Trick
  </strong>
  is a latent sampling procedure for generative adversarial networks, where we sample $z$ from a truncated normal (where values which fall outside a range are resampled to fall inside that range). 
The original implementation was in
  <a href="https://paperswithcode.com/paper/megapixel-size-image-creation-using">
   Megapixel Size Image Creation with GAN
  </a>
  .
In
  <a href="https://paperswithcode.com/method/biggan">
   BigGAN
  </a>
  , the authors find this provides a boost to the Inception Score and FID.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.34.17_PM_w6t5LE0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Latent Optimisation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Latent Optimisation
  </strong>
  is a technique used for generative adversarial networks to refine the sample quality of $z$. Specifically, it exploits knowledge from the discriminator $D$ to refine the latent source $z$. Intuitively, the gradient $\nabla_{z}f\left(z\right) = \delta{f}\left(z\right)\delta{z}$ points in the direction that better satisfies the discriminator $D$, which implies better samples. Therefore, instead of using the randomly sampled $z \sim p\left(z\right)$, we uses the optimised latent:
 </p>
 <p>
  $$ \Delta{z} = \alpha\frac{\delta{f}\left(z\right)}{\delta{z}} $$
 </p>
 <p>
  $$ z' = z + \Delta{z} $$
 </p>
 <p>
  Source:
  <a href="https://paperswithcode.com/method/logan">
   LOGAN
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_8.21.36_PM_0VlRNqG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Polya-Gamma Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  This method applies Polya-Gamma latent variables as a way to obtain closed form expressions for full-conditionals of posterior distributions in sampling algorithms like MCMC.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>81. Lifelong Learning</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Self-Learning</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LIMix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LIMix
  </strong>
  , or
  <strong>
   Lifelong Infinite Mixture
  </strong>
  , is a lifelong learning model which grows a mixture of models to adapt to an increasing number of tasks.  LIMix can automatically expand its network architectures or choose an appropriate component to adapt its parameters for learning a new task, while preserving its previously learnt information. Knowledge is incorporated by means of Dirichlet processes by using a gating mechanism which computes the dependence between the knowledge learnt previously and stored in each component, and a new set of data. Besides, a Student model is trained which can accumulate cross-domain representations over time and make quick inferences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-31_at_10.05.18_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>82. Augmented Reality Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>ACGPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ACGPN
  </strong>
  , or
  <strong>
   Adaptive Content Generating and Preserving Network
  </strong>
  , is a
  <a href="https://www.paperswithcode.com/method/category/generative-adversarial-network">
   generative adversarial network
  </a>
  for virtual try-on clothing applications.
 </p>
 <p>
  In Step I, the Semantic Generation Module (SGM) takes the target clothing image $\mathcal{T}_{c}$, the pose map $\mathcal{M}_{p}$, and the fused body part mask $\mathcal{M}^{F}$ as the input to predict the semantic layout and to output the synthesized body part mask $\mathcal{M}^{S}_{\omega}$ and the target clothing mask $\mathcal{M}^{S_{c}$.
 </p>
 <p>
  In Step II, the Clothes Warping Module (CWM) warps the target clothing image to $\mathcal{T}^{R}_{c}$ according to the predicted semantic layout, where a second-order difference constraint is introduced to stabilize the warping process.
 </p>
 <p>
  In Steps III and IV, the Content Fusion Module (CFM) first produces the composited body part mask $\mathcal{M}^{C}_{\omega}$ using the original clothing mask $\mathcal{M}_{c}$, the synthesized clothing mask $\mathcal{M}^{S}_{c}$, the body part mask $\mathcal{M}_{\omega}$, and the synthesized body part mask $\mathcal{M}_{\omega}^{S}$, and then exploits a fusion network to generate the try-on images $\mathcal{I}^{S}$ by utilizing the information $\mathcal{T}^{R}_{c}$, $\mathcal{M}^{S}_{c}$, and the body part image $I_{\omega}$ from previous steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-01_at_10.13.40_AM_V9osXdz.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ARShoe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ARShoe
  </strong>
  is a multi-branch network for pose estimation and segmentation tackling the "try-on" problem for augmented reality shoes. Consisting of an encoder and a decoder, the multi-branch network is trained to predict keypoints
  <a href="https://paperswithcode.com/method/heatmap">
   heatmap
  </a>
  (heatmap),
  <a href="https://paperswithcode.com/method/pafs">
   PAFs
  </a>
  heatmap (pafmap), and segmentation results (segmap) simultaneously. Post processes are then performed for a smooth and realistic virtual try-on.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_3.29.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>83. Negative Sampling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Self-Adversarial Negative Sampling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Self-Adversarial Negative Sampling
  </strong>
  is a negative sampling technique used for methods like
  <a href="https://paperswithcode.com/methods/category/word-embeddings">
   word embeddings
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/graph-embeddings">
   knowledge graph embeddings
  </a>
  . The traditional negative sampling loss from word2vec for optimizing distance-based models be written as:
 </p>
 <p>
  $$ L = −\log\sigma\left(\gamma − d_{r}\left(\mathbf{h}, \mathbf{t}\right)\right) − \sum^{n}_{i=1}\frac{1}{k}\log\sigma\left(d_{r}\left(\mathbf{h}^{'}_{i}, \mathbf{t}^{'}_{i}\right) - \gamma\right) $$
 </p>
 <p>
  where $\gamma$ is a fixed margin, $\sigma$ is the sigmoid function, and $\left(\mathbf{h}^{'}_{i}, r, \mathbf{t}^{'}_{i}\right)$ is the $i$-th negative triplet.
 </p>
 <p>
  The negative sampling loss samples the negative triplets in a uniform way. Such a uniform negative sampling suffers the problem of inefficiency since many samples are obviously false as training goes on, which does not provide any meaningful information. Therefore, the authors propose an approach called self-adversarial negative sampling, which samples negative triples according to the current embedding model. Specifically, we sample negative triples from the following distribution:
 </p>
 <p>
  $$ p\left(h^{'}_{j}, r, t^{'}_{j} | \text{set}\left(h_{i}, r_{i}, t_{i} \right) \right) = \frac{\exp\alpha{f}_{r}\left(\mathbf{h}^{'}_{j}, \mathbf{t}^{'}_{j}\right)}{\sum_{i=1}\exp\alpha{f}_{r}\left(\mathbf{h}^{'}_{i}, \mathbf{t}^{'}_{i}\right)} $$
 </p>
 <p>
  where $\alpha$ is the temperature of sampling. Moreover, since the sampling procedure may be costly, the authors treat the above probability as the weight of the negative sample. Therefore, the final negative sampling loss with self-adversarial training takes the following form:
 </p>
 <p>
  $$ L = −\log\sigma\left(\gamma − d_{r}\left(\mathbf{h}, \mathbf{t}\right)\right) − \sum^{n}_{i=1}p\left(h^{'}_{i}, r, t^{'}_{i}\right)\log\sigma\left(d_{r}\left(\mathbf{h}^{'}_{i}, \mathbf{t}^{'}_{i}\right) - \gamma\right) $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-19_at_11.05.55_AM_hzlnaGR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MBS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  To avoid the problem caused by low-frequent entity-relation pairs, our MBS uses the estimated probabilities from a trained model $\mathbf{\theta}'$ to calculate frequencies for each triplet and query. By using $\mathbf{\theta}'$, the NS loss in KGE with MBS is represented as follows:
\begin{align}
    &amp;\ell_{mbs}(\mathbf{\theta};\mathbf{\theta}') \nonumber \
=&amp;-\frac{1}{|D|}\sum_{(x,y) \in D} \Bigl[A_{mbs}(\mathbf{\theta}')\log(\sigma(s_{\mathbf{\theta}}(x,y)+\gamma))\nonumber\
    &amp;+\frac{1}{\nu}sum_{y_{i}\sim p_n(y_{i}|x)}^{\nu}B_{mbs}(\mathbf{\theta}')\log(\sigma(-s_{\mathbf{\theta}}(x,y_i)-\gamma))\Bigr],
\end{align}
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>84. Generalized Additive Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Generalized Additive Models (GAMs)
    </strong>
    are a class of methods where the response variable depends linearly on some unknown functions of predictor variables. Below you can find a continuously updating list of GAM methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>NAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neural Additive Models (NAMs)
  </strong>
  make restrictions on the structure of neural networks, which yields a family of models that are inherently interpretable while suffering little loss in prediction accuracy when applied to tabular data. Methodologically, NAMs belong to a larger model family called Generalized Additive Models (GAMs).
 </p>
 <p>
  NAMs learn a linear combination of networks that each attend to a single input feature: each $f_{i}$ in the traditional GAM formulationis parametrized by a neural network. These networks are trained jointly using backpropagation and can learn arbitrarily complex shape functions. Interpreting NAMs is easy as the impact of a feature on the prediction does not rely on the other features and can be understood by visualizing its corresponding shape function (e.g., plotting $f_{i}\left(x_{i}\right)$ vs. $x_{i}$).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/nam_xnCYK9q.jpeg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Base Boosting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In the setting of multi-target regression, base boosting permits us to incorporate prior knowledge into the learning mechanism of gradient boosting (or Newton boosting, etc.). Namely, from the vantage of statistics, base boosting is a way of building the following additive expansion in a set of elementary basis functions:
\begin{equation}
h_{j}(X ; { \alpha_{j}, \theta_{j} }) = X_{j} + \sum_{k=1}^{K_{j}} \alpha_{j,k} b(X ; \theta_{j,k}),
\end{equation}
where 
$X$ is an example from the domain $\mathcal{X},$
${\alpha_{j}, \theta_{j}} = {\alpha_{j,1},\dots, \alpha_{j,K_{j}},\theta_{j,1},\dots,\theta_{j,K_{j}}}$ collects the expansion coefficients and parameter sets,
$X_{j}$ is the image of $X$ under the $j$th coordinate function (a prediction from a user-specified model),
$K_{j}$ is the number of basis functions in the linear sum,
$b(X; \theta_{j,k})$ is a real-valued function of the example $X,$ characterized by a parameter set $\theta_{j,k}.$
 </p>
 <p>
  The aforementioned additive expansion differs from the
  <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451">
   standard  additive expansion
  </a>
  :
\begin{equation}
h_{j}(X ; { \alpha_{j}, \theta_{j}}) = \alpha_{j, 0} + \sum_{k=1}^{K_{j}} \alpha_{j,k} b(X ; \theta_{j,k}),
\end{equation}
as it replaces the constant offset value $\alpha_{j, 0}$ with a prediction from a user-specified model. In essence, this modification permits us to incorporate prior knowledge into the for loop of gradient boosting, as the for loop proceeds to build the linear sum by computing residuals that depend upon predictions from the user-specified model instead of the optimal constant model: $\mbox{argmin} \sum_{i=1}^{m_{train}} \ell_{j}(Y_{j}^{(i)}, c),$ where $m_{train}$ denotes the number of training examples, $\ell_{j}$ denotes a single-target loss function, and $c \in \mathbb{R}$ denotes a real number, e.g, $\mbox{argmin} \sum_{i=1}^{m_{train}} (Y_{j}^{(i)} - c)^{2} = \frac{\sum_{i=1}^{m_{train}} Y_{j}^{(i)}}{m_{train}}.$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/framework_wlCA2gl.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>85. Hybrid Optimization</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Hybrid Optimization
    </strong>
    methods combine various optimization methods to attempt to yield the benefits of both while minimizing or averaging out the downsides. Below you can find a continuously updating list of hybrid optimization methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MEUZZ</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MEUZZ
  </strong>
  is a machine learning-based hybrid fuzzer which employs supervised machine learning for adaptive and generalizable seed scheduling -- a prominent factor in determining the yields of hybrid fuzzing. MEUZZ determines which new seeds are expected to produce better fuzzing yields based on the knowledge learned from past seed scheduling decisions made on the same or similar programs. MEUZZ's learning is based on a series of features extracted via code reachability and dynamic analysis, which incurs negligible runtime overhead (in microseconds). Moreover, MEUZZ automatically infers the data labels by evaluating the fuzzing performance of each selected seed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.41.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HFPSO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hybrid Firefly and Particle Swarm Optimization (HFPSO)
  </strong>
  is a metaheuristic optimization algorithm that combines strong points of firefly and particle swarm optimization. HFPSO tries to determine the start of the local search process properly by checking the previous global best fitness values.
 </p>
 <p>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S156849461830084X">
   Click Here for the Paper
  </a>
 </p>
 <p>
  <a href="https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso">
   Codes (MATLAB)
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/screenshot_P5onJoX.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>86. Distributions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Distributions
    </strong>
    are functions that represent the probability of a particular value or set of values. Below you can find a continuously updating list of (specialized) distributions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Gumbel Softmax</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Gumbel-Softmax
  </strong>
  is a continuous distribution that has the property that it can be smoothly annealed into a categorical distribution, and whose parameter gradients can be easily computed via the reparameterization trick.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_1.41.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>87. Synchronous Pipeline Parallel</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Chimera</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Chimera
  </strong>
  is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines).
 </p>
 <p>
  Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.
 </p>
 <p>
  The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active.
 </p>
 <p>
  In the down pipeline, stage$_{0}$∼stage$_{3}$ are mapped to $P_{0}∼P_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.14.50_PM_5WN2Zje.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPipe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPipe
  </strong>
  is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-22_at_12.08.43_PM_bzmZL2Z.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>88. Inference Engines</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Mobile Neural Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mobile Neural Network (MNN)
  </strong>
  is a mobile inference engine tailored to mobile applications. The contributions of MNN include: (1) presenting a mechanism called pre-inference that manages to conduct runtime optimization; (2) delivering thorough kernel optimization on operators to achieve optimal computation performance; (3) introducing backend abstraction module which enables hybrid scheduling and keeps the engine lightweight.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_1.14.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>89. Kernel Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>NTK</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StreaMRAK</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StreaMRAK
  </strong>
  is a streaming version of kernel ridge regression. It divdes the problem into several levels of resolution, which allows continual refinement to the predictions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_10.41.59_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>90. Control and Decision Systems</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PPMC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Path Planning and Motion Control
  </strong>
  , or
  <strong>
   PPMC RL
  </strong>
  , is a training algorithm that teaches path planning and motion control to robots using reinforcement learning in a simulated environment. The focus is on promoting generalization where there are environmental uncertainties such as rough environments like lunar services. The algorithm is coupled with any generic reinforcement learning algorithm to teach robots how to respond to user commands and to travel to designated locations on a single neural network. The algorithm works independently of the robot structure, demonstrating that it works on a wheeled rover in addition to the past results on a quadruped walking robot.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.49.48_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GA-PID/NN-PID</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The main control tasks in autonomous vehicles are steering (lateral) and speed (longitudinal) control. PID controllers are widely used in the industry because of their simplicity and good performance, but they are difficult to tune and need additional adaptation to control nonlinear systems with varying parameters. In this paper, the longitudinal control task is addressed by implementing adaptive PID control
using two different approaches: Genetic Algorithms (GA-PID) and then Neural Networks (NN-PID) respectively. The vehicle
nonlinear longitudinal dynamics are modeled using Powertrain blockset library. Finally, simulations are performed to assess
and compare the performance of the two controllers subject to external disturbances.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/92802e22-c022-4164-bd3b-3ae1e68a0451.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>91. Manifold Disentangling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GEOMANCER</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Geomancer
  </strong>
  is a nonparametric algorithm for symmetry-based disentangling of data manifolds. It learns a set of subspaces to assign to each point in the dataset, where each subspace is the tangent space of one disentangled submanifold. This means that geomancer can be used to disentangle manifolds for which there may not be a global axis-aligned coordinate system.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_10.28.22_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>92. Leadership Inference</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Leadership is a process of individuals (leaders) who influence a group to achieve collective goals. In nature, leadership can be viewed as a process of initiation of coordinated activity. In the task of leadership inference, the main goals are to infer leaders, relationships between leaders and followers, as well as the process of leaders use to influence the group to achieve collective decisions. In machine learning, leadership inference can be viewed as a problem of finding a latent multi-agent system that governs behaviors of group of individuals from data.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>FLICA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An agreement of a group to follow a common purpose is manifested by its coalescence into a coordinated behavior. The process of initiating this behavior and the period of decision-making by the group members necessarily precedes the coordinated behavior. Given time series of group members’ behavior, the goal is to find these periods of decision-making and identify the initiating individual, if one exists.
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/pdf/1603.01570v2.pdf">
   Amornbunchornvej et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_15.10.16_xKEX7TU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>93. Distribution Approximation</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Distribution Approximation
    </strong>
    methods aim to approximate a complex distribution. Below you can find a continuously updating list of distribution approximation methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Normalizing Flows</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Normalizing Flows
  </strong>
  are a method for constructing complex distributions by transforming a
probability density through a series of invertible mappings. By repeatedly applying the rule for change of variables, the initial density ‘flows’ through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow.
 </p>
 <p>
  In the case of finite flows, the basic rule for the transformation of densities considers an invertible, smooth mapping $f : \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ with inverse $f^{-1} = g$, i.e. the composition $g \cdot f\left(z\right) = z$. If we use this mapping to transform a random variable $z$ with distribution $q\left(z\right)$, the resulting random variable $z' = f\left(z\right)$ has a distribution:
 </p>
 <p>
  $$ q\left(\mathbf{z}'\right) = q\left(\mathbf{z}\right)\bigl\vert{\text{det}}\frac{\delta{f}^{-1}}{\delta{\mathbf{z'}}}\bigr\vert = q\left(\mathbf{z}\right)\bigl\vert{\text{det}}\frac{\delta{f}}{\delta{\mathbf{z}}}\bigr\vert ^{-1} $$
?
where the last equality can be seen by applying the chain rule (inverse function theorem) and is a property of Jacobians of invertible functions. We can construct arbitrarily complex densities by composing several simple maps and successively applying the above equation. The density $q_{K}\left(\mathbf{z}\right)$ obtained by successively transforming a random variable $z_{0}$ with distribution $q_{0}$ through a chain of $K$ transformations $f_{k}$ is:
 </p>
 <p>
  $$ z_{K} = f_{K} \cdot \dots \cdot f_{2} \cdot f_{1}\left(z_{0}\right) $$
 </p>
 <p>
  $$ \ln{q}_{K}\left(z_{K}\right) = \ln{q}_{0}\left(z_{0}\right) − \sum^{K}_{k=1}\ln\vert\det\frac{\delta{f_{k}}}{\delta{\mathbf{z_{k-1}}}}\vert $$
?
The path traversed by the random variables $z_{k} = f_{k}\left(z_{k-1}\right)$ with initial distribution $q_{0}\left(z_{0}\right)$ is called the flow and the path formed by the successive distributions $q_{k}$ is a normalizing flow.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_6.49.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>QuantTree</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Given a training set drawn from an unknown $d$-variate probability distribution, QuantTree constructs a histogram by recursively splitting $\mathbb{R}^d$. The splits are defined by a stochastic process so that each bin contains a certain proportion of the training set. These histograms can be used to define test statistics (e.g., the Pearson statistic) to tell whether a batch of data is drawn from $\phi_0$ or not. The most crucial property of QuantTree is that the distribution of any statistic based on QuantTree histograms is independent of $\phi_0$, thus enabling nonparametric statistical testing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>94. Self-Training Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>STraTA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   STraTA
  </strong>
  , or
  <strong>
   Self-Training with Task Augmentation
  </strong>
  , is a self-training approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeling texts. Second, STRATA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data.
 </p>
 <p>
  In task augmentation, we train an NLI data generation model and use it to synthesize a large amount of in-domain NLI training data for each given target task, which is then used for auxiliary (intermediate) fine-tuning. The self-training algorithm iteratively learns a better model using a concatenation of labeled and pseudo-labeled examples. At each iteration, we always start with the auxiliary-task model produced by task augmentation and train on a broad distribution of pseudo-labeled data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_4.13.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>self-mem + new data</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The training contains two steps. The first step is to train data for creating the T2D and D2T models. Then, in the second step, use these
models to infer self-memory for self-training the D2T model.  Then, the training takes self-memory and new data for self-training the D2T model but not the T2D model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>95. Network Shrinking</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Network Shrinking
    </strong>
    methods aim to shrink the size of a neural network for increased efficiency, such as on mobile devices. Below you can find a continuously updating list of network shrinking methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>TinyNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  To obtain excellent deep neural architectures, a series of techniques are carefully designed in EfficientNets. The giant formula for simultaneously enlarging the resolution, depth and width provides us a Rubik's cube for neural networks. So that we can find networks with high efficiency and excellent performance by twisting the three dimensions. This paper aims to explore the twisting rules for obtaining deep neural networks with minimum model sizes and computational costs. Different from the network enlarging, we observe that resolution and depth are more important than width for tiny networks. Therefore, the original method, i.e., the compound scaling in
  <a href="https://paperswithcode.com/method/efficientnet">
   EfficientNet
  </a>
  is no longer suitable. To this end, we summarize a tiny formula for downsizing neural architectures through a series of smaller models derived from the EfficientNet-B0 with the FLOPs constraint. Experimental results on the ImageNet benchmark illustrate that our TinyNet performs much better than the smaller version of EfficientNets using the inversed giant formula. For instance, our TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is about 1.9% higher than that of the previous best
  <a href="https://paperswithcode.com/method/mobilenetv3">
   MobileNetV3
  </a>
  with similar computational cost.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NetAdapt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NetAdapt
  </strong>
  is a network shrinking algorithm to adapt a pretrained network to a mobile platform given a real resource budget. NetAdapt can incorporate direct metrics, such as latency and energy, into the optimization to maximize the adaptation performance based on the characteristics of the platform. By using empirical measurements, NetAdapt can be applied to any platform as long as we can measure the desired metrics, without any knowledge of the underlying implementation of the platform.
 </p>
 <p>
  While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using
  <em>
   empirical measurements
  </em>
  , so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.21.45_PM_UrpHX1t.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>96. Factorization Machines</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>FEFM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Field Embedded Factorization Machine
  </strong>
  , or
  <strong>
   FEFM
  </strong>
  , is a factorization machine variant. For each field pair, FEFM introduces symmetric matrix embeddings along with the usual feature vector embeddings that are present in FM. Like FM, $v_{i}$ is the vector embedding of the $i^{t h}$ feature. However, unlike Field-Aware Factorization Machines (FFMs), FEFM doesn't explicitly learn field-specific feature embeddings. The learnable symmetric matrix $W_{F(i), F(j)}$ is the embedding for the field pair $F(i)$ and $F(j) .$ The interaction between the $i^{t h}$ feature and the $j^{t h}$ feature is mediated through $W_{F(i), F(j)} .$
 </p>
 <p>
  $$
\phi(\theta, x)=\phi_{F E F M}((w, v, W), x)=w_{0}+\sum_{i=1}^{m} w_{i} x_{i}+\sum_{i=1}^{m} \sum_{j=i+1}^{m} v_{i}^{T} W_{F(i), F(j)} v_{j} x_{i} x_{j}
$$
 </p>
 <p>
  where $W_{F(i), F(j)}$ is a $k \times k$ symmetric matrix ( $k$ is the dimension of the feature vector embedding space containing feature vectors $v_{i}$ and $v_{j}$ ).
 </p>
 <p>
  The symmetric property of the learnable matrix $W_{F(i), F(j)}$ is ensured by reparameterizing $W_{F(i), F(j)}$ as $U_{F(i), F(j)}+$ $U_{F(i), F(j)}^{T}$, where $U_{F(i), F(j)}^{T}$ is the transpose of the learnable matrix $U_{F(i), F(j)} .$ Note that $W_{F(i), F(j)}$ can also be interpreted as a vector transformation matrix which transforms a feature embedding when interacting with a specific field.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/f5b50cb9-d805-4b87-87d1-78ab63c978f9.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FM with splines</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Using cubic splines to improve factorization machine accuracy with numerical features
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>97. Out-of-Distribution Example Detection</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DIME</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DIME
  </strong>
  , or
  <strong>
   Distance to Modelled Embedding
  </strong>
  , is a method for detecting out-of-distribution examples during prediction time. Given a trained neural network, the training data drawn from some high-dimensional distribution in data space $X$ is transformed into the model’s intermediate feature vector space $\mathbb{R}^{p}$. The training set embedding is linearly approximated as a hyperplane. When we then receive new observations it is difficult to assess if observations are out-of-distribution directly in data space, so we transform them into the same intermediate feature space. Finally, the Distance-to-Modelled-Embedding (DIME) can be used to assess whether new observations fit into the expected embedding covariance structure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_7.43.10_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>98. Global Context Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Hamburger</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hamburger
  </strong>
  is a global context module that employs matrix decomposition to factorize the learned representation into sub-matrices so as to recover the clean low-rank signal subspace. The key idea is, if we formulate the inductive bias like the global context into an objective function, the optimization algorithm to minimize the objective function can construct a computational graph, i.e., the architecture we need in the networks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.16.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>99. Parameter Sharing</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Parameter Sharing
    </strong>
    methods are used in neural networks to control the overall number of parameters and help guard against overfitting. Below you can find a continuously updating list of parameter sharing methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Weight Tying</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weight Tying
  </strong>
  improves the performance of language models by tying (sharing) the weights of the embedding and
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layers. This method also massively reduces the total number of parameters in the language models that it is applied to.
 </p>
 <p>
  Language models are typically comprised of an embedding layer, followed by a number of
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  or
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  layers, which are finally followed by a softmax layer. Embedding layers learn word representations, such that similar words (in meaning) are represented by vectors that are near each other (in cosine distance). [Press &amp; Wolf, 2016] showed that the softmax matrix, in which every word also has a vector representation, also exhibits this property. This leads them to propose to share the softmax and embedding matrices, which is done today in nearly all language models.
 </p>
 <p>
  This method was independently introduced by
  <a href="https://paperswithcode.com/paper/using-the-output-embedding-to-improve">
   Press &amp; Wolf, 2016
  </a>
  and
  <a href="https://paperswithcode.com/paper/tying-word-vectors-and-word-classifiers-a">
   Inan et al, 2016
  </a>
  .
 </p>
 <p>
  Additionally, the Press &amp; Wolf paper proposes Three-way Weight Tying, a method for NMT models in which the embedding matrix for the source language, the embedding matrix for the target language, and the softmax matrix for the target language are all tied. That method has been adopted by the Attention Is All You Need model and many other neural machine translation models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-11-11_at_8.57.05_PM_5FtuWCH.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>100. Genetic Algorithm</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
            
        </ul>
        
        <ul class="parent">
            <p>101. Theorem Proving Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>NeuroTactic</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NeuroTactic
  </strong>
  is a model for theorem proving which leverages
  <a href="https://paperswithcode.com/methods/category/graph-models">
   graph neural networks
  </a>
  to represent the theorem and premises, and applies graph contrastive learning for pre-training. Specifically, premise selection is designed as a pretext task for the graph contrastive learning approach. The learned representations are then used for the downstream task, tactic prediction
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_7.47.51_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>102. Playstyle</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    About the style of policy (decision making).
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Playstyle Distance</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  This method proposes first discretizing observations and calculating the action distribution distance under comparable cases (intersection states).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>103. Diffusion Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>LSDM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Our main contribution is the Guiding Points Network, where we integrate all information from the conditions to generate guiding points. By applying transformation matrices to scene entities (human/objects) with attention weighting, we can forecast the spanning of the target object.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3a3df10a-bc8f-4c66-9da2-33ff964c19fa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>104. Spreadsheet Formula Prediction Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>SpreadsheetCoder</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SpreadsheetCoder
  </strong>
  is a neural network architecture for spreadsheet formula prediction. It is a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  -based model architecture to represent the tabular context in both row-based and column-based formats. A
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  encoder computes an embedding vector for each input token, incorporating the contextual information from nearby rows and columns. The BERT encoder is initialized from the weights pre-trained on English text corpora, which is beneficial for encoding table headers. To handle cell references, a two-stage decoding process is used inspired by sketch learning for program synthesis. The decoder first generates a formula sketch, which does not include concrete cell references, and then predicts the corresponding cell ranges to generate the complete formula
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.29.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>105. Momentum Rules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Demon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Decaying Momentum
  </strong>
  , or
  <strong>
   Demon
  </strong>
  , is a stochastic optimizer motivated by decaying the total contribution of a gradient to all future updates. By decaying the momentum parameter, the total contribution of a gradient to all future updates is decayed. A particular gradient term $g_{t}$ contributes a total of  $\eta\sum_{i}\beta^{i}$ of its "energy" to all future gradient updates, and this results in the geometric sum, $\sum^{\infty}_{i=1}\beta^{i} = \beta\sum^{\infty}_{i=0}\beta^{i} = \frac{\beta}{\left(1-\beta\right)}$. Decaying this sum results in the Demon algorithm. Letting $\beta_{init}$ be the initial $\beta$; then at the current step $t$ with total $T$ steps, the decay routine is given by solving the below for $\beta_{t}$:
 </p>
 <p>
  $$ \frac{\beta_{t}}{\left(1-\beta_{t}\right)} =  \left(1-t/T\right)\beta_{init}/\left(1-\beta_{init}\right)$$
 </p>
 <p>
  Where $\left(1-t/T\right)$ refers to the proportion of iterations remaining. Note that Demon typically requires no hyperparameter tuning as it is usually decayed to $0$ or a small negative value at time 
$T$. Improved performance is observed by delaying the decaying. Demon can be applied to any gradient descent algorithm with a momentum parameter.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_9.45.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>106. Tabular Data Generation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>CTAB-GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CTAB-GAN
  </strong>
  is a model for conditional tabular data generation. The generator and discriminator utilize the
  <a href="https://paperswithcode.com/method/dcgan">
   DCGAN
  </a>
  architecture. An
  <a href="https://paperswithcode.com/method/auxiliary-classifier">
   auxiliary classifier
  </a>
  is also used with an MLP architecture.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_9.54.11_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>107. Distributed Communication</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    This section contains a compilation of distributed communication methods for scaling deep learning to very large models. Communication methods aim to minimize communication overhead between nodes in a distributed system, and link nodes in an optimal way.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Blink Communication</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blink
  </strong>
  is a communication library for inter-GPU parameter exchange that achieves near-optimal link utilization. To handle topology heterogeneity from hardware generations or partial allocations from cluster schedulers, Blink dynamically generates optimal communication primitives for a given topology. Blink probes the set of links available for a given job at runtime and builds a topology with appropriate link capacities. Given the topology, Blink achieves the optimal communication rate by packing spanning trees, that can utilize more links (Lovasz, 1976; Edmonds, 1973) when compared to rings. The authors use a multiplicative-weight update based approximation algorithm to quickly compute the maximal packing and extend the algorithm to further minimize the number of trees generated. Blink’s collectives extend across multiple machines effectively utilizing all available network interfaces.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_9.32.52_AM_wlZxXmR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>108. Bijective Transformation</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Bijective Transformations
    </strong>
    are transformations that are bijective, i.e. they can be reversed. They are used within the context of normalizing flow models. Below you can find a continuously updating list of bijective transformation methods.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Affine Coupling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Affine Coupling
  </strong>
  is a method for implementing a normalizing flow (where we stack a sequence of invertible bijective transformation functions). Affine coupling is one of these bijective transformation functions. Specifically, it is an example of a reversible transformation where the forward function, the reverse function and the log-determinant are computationally efficient. For the forward function, we split the input dimension into two parts:
 </p>
 <p>
  $$ \mathbf{x}_{a}, \mathbf{x}_{b} = \text{split}\left(\mathbf{x}\right) $$
 </p>
 <p>
  The second part stays the same $\mathbf{x}_{b} = \mathbf{y}_{b}$, while the first part  $\mathbf{x}_{a}$ undergoes an affine transformation, where the parameters for this transformation are learnt using the second part $\mathbf{x}_{b}$ being put through a neural network. Together we have:
 </p>
 <p>
  $$ \left(\log{\mathbf{s}, \mathbf{t}}\right) = \text{NN}\left(\mathbf{x}_{b}\right) $$
 </p>
 <p>
  $$ \mathbf{s} = \exp\left(\log{\mathbf{s}}\right) $$
 </p>
 <p>
  $$ \mathbf{y}_{a} = \mathbf{s} \odot \mathbf{x}_{a} + \mathbf{t}  $$
 </p>
 <p>
  $$ \mathbf{y}_{b} = \mathbf{x}_{b} $$
 </p>
 <p>
  $$ \mathbf{y} = \text{concat}\left(\mathbf{y}_{a}, \mathbf{y}_{b}\right) $$
 </p>
 <p>
  Image:
  <a href="https://paperswithcode.com/method/glow">
   GLOW
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_8.53.47_PM_j0OdFyF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>109. Distillation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>SFT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Shrink and Fine-Tune
  </strong>
  , or
  <strong>
   SFT
  </strong>
  , is a type of distillation that avoids explicit distillation by copying parameters to a student student model and then fine-tuning. Specifically it extracts a student model from the maximally spaced layers of a fine-tuned teacher. Each layer $l \in L'$ is copied fully from $L$. For example, when creating a
  <a href="https://paperswithcode.com/method/bart">
   BART
  </a>
  student with 3 decoder layers from the 12 encoder layer 12 decoder layer teacher, we copy the teacher’s full $Enc^{L}$ and decoder layers 0, 6, and 11 to the student. When deciding which layers to copy, we break ties arbitrarily; copying layers 0, 5, and 11 might work just as well. When copy only 1 decoder layer, we copy layer 0. This was found this to work better than copying layer 11. The impact of initialization on performance is measured experimentally in Section 6.1. After initialization, the student model continues to fine-tune on the summarization dataset, with the objective of minimizing $\mathcal{L}_{Data}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/148a26ba-3d3d-4764-bcf2-cc47c5ae0ccf.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>110. Geometric Matching</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>CHM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Convolutional Hough Matching
  </strong>
  , or
  <strong>
   CHM
  </strong>
  , is a geometric matching algorithm that distributes similarities of candidate matches over a geometric transformation space and evaluates them in a convolutional manner. It is casted into a trainable neural layer with a  semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.48.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>111. Sample Re-Weighting</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Fast Sample Re-Weighting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fast Sample Re-Weighting
  </strong>
  , or
  <strong>
   FSR
  </strong>
  , is a sample re-weighting strategy to tackle problems such as dataset biases, noisy labels and imbalanced classes. It leverages a dictionary (essentially an extra buffer) to monitor the training history reflected by the model updates during meta optimization periodically, and utilises a valuation function to discover meaningful samples from training data as the proxy of reward data. The unbiased dictionary keeps being updated and provides reward signals to optimize sample weights. Additionally, instead of maintaining model states for both model and sample weight updates separately, feature sharing is enabled for saving the computation cost used for maintaining respective states.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.39.38_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>112. Cache Replacement Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Parrot</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Parrot
  </strong>
  is an imitation learning approach to automatically learn cache access patterns by leveraging Belady’s optimal policy. Belady’s optimal policy is an oracle policy that computes the theoretically optimal cache eviction decision based on knowledge of future cache accesses, which Parrot approximates with a policy that only conditions on the past accesses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/b4997e0d-2c52-4d3c-bb1b-b3312e7fc21b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>113. Generalization</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Distributional Generalization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Distributional Generalization
  </strong>
  is a type of generalization that roughly states that outputs of a classifier at train and test time are close as distributions, as opposed to close in just their average error. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/25bb84be-6ca2-4287-b307-47949ee83cfe.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>114. Confidence Calibration</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>CCAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Confidence Calibration with an Auxiliary Class
  </strong>
  , or
  <strong>
   CCAC
  </strong>
  , is a post-hoc confidence calibration method for DNN classifiers on OOD datasets. The key feature of CCAC is an auxiliary class in the calibration model which separates mis-classified samples from correctly classified ones, thus effectively mitigating the target DNN’s being confidently wrong. It also reduces the number of free parameters in CCAC to reduce free parameters and facilitate transfer to a new unseen dataset.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/47b63418-9357-47b9-9b48-506ca8875638.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>115. Confidence Estimators</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MACEst</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Model Agnostic Confidence Estimator
  </strong>
  , or
  <strong>
   MACEst
  </strong>
  , is a model-agnostic confidence estimator. Using a set of nearest neighbours, the algorithm differs from other methods by estimating confidence independently as a local quantity which explicitly accounts for both aleatoric and epistemic uncertainty. This approach differs from standard calibration methods that use a global point prediction model as a starting point for the confidence estimate.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.08.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>116. Long-Range Interaction Layers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Lambda Layer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Lambda layers
  </strong>
  are a building block for modeling long-range dependencies in data. They consist of long-range interactions between a query and a structured set of context elements at a reduced memory cost. Lambda layers transform each available context into a linear function, termed a lambda, which is then directly applied to the corresponding query. Whereas self-attention defines a similarity kernel between the query and the context elements, a lambda layer instead summarizes contextual information into a fixed-size linear function (i.e. a matrix), thus bypassing the need for memory-intensive attention maps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-20_at_2.20.33_PM_wviWxYp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>117. Hybrid Fuzzing</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MEUZZ</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MEUZZ
  </strong>
  is a machine learning-based hybrid fuzzer which employs supervised machine learning for adaptive and generalizable seed scheduling -- a prominent factor in determining the yields of hybrid fuzzing. MEUZZ determines which new seeds are expected to produce better fuzzing yields based on the knowledge learned from past seed scheduling decisions made on the same or similar programs. MEUZZ's learning is based on a series of features extracted via code reachability and dynamic analysis, which incurs negligible runtime overhead (in microseconds). Moreover, MEUZZ automatically infers the data labels by evaluating the fuzzing performance of each selected seed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.41.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>118. Binary Neural Networks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>BiDet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BiDet
  </strong>
  is a binarized neural network learning method for efficient object detection. Conventional network binarization methods directly quantize the weights and activations in one-stage or two-stage detectors with constrained representational capacity, so that the information redundancy in the networks causes numerous false positives and degrades the performance significantly. On the contrary, BiDet fully utilizes the representational capacity of the binary neural networks for object detection by redundancy removal, through which the detection precision is enhanced with alleviated false positives. Specifically, the information bottleneck (IB) principle is generalized to object detection, where the amount of information in the high-level feature maps is constrained and the mutual information between the feature maps and object detection is maximized.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.21.38_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>119. Bot Detection</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>BIMAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BIMAN
  </strong>
  , or
  <strong>
   Bot Identification by commit Message, commit Association, and author Name
  </strong>
  , is a technique to detect bots that commit code. It is comprised of three methods that consider independent aspects of the commits made by a particular author: 1) Commit Message: Identify if commit messages are being generated from templates; 2) Commit Association: Predict if an author is a bot using a random forest model, with features related to files and projects associated with the commits as predictors; and 3) Author Name: Match author’s name and email to common bot patterns.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.36.46_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>120. Rule Learners</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GPFL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Graph Path Feature Learning
  </strong>
  is a probabilistic rule learner optimized to mine instantiated first-order logic rules from knowledge graphs. Instantiated rules contain constants extracted from KGs. Compared to abstract rules that contain no constants, instantiated rules are capable of explaining and expressing concepts in more detail. GPFL utilizes a novel two-stage rule generation mechanism that first generalizes extracted paths into templates that are acyclic abstract rules until a certain degree of template saturation is achieved, then specializes the generated templates into instantiated rules.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.53.07_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>121. Learning to Rank Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DCN-V2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DCN-V2
  </strong>
  is an architecture for learning-to-rank that improves upon the original
  <a href="https://paperswithcode.com/method/dcn">
   DCN
  </a>
  model. It first learns explicit feature interactions of the inputs (typically the embedding layer) through cross layers, and then combines with a deep network to learn complementary implicit interactions. The core of DCN-V2 is the cross layers, which inherit the simple structure of the cross network from DCN, however it is significantly more expressive at learning explicit and bounded-degree cross features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_10.34.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>122. Sparsity</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Neural Tangent Transfer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neural Tangent Transfer
  </strong>
  , or
  <strong>
   NTT
  </strong>
  , is a method for finding trainable sparse networks in a label-free manner. Specifically, NTT finds sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_9.24.11_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>123. Position Recovery Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>PRNet+</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PRNet+
  </strong>
  is a multi-task neural network for outdoor position recovery from measurement record (MR) data. PRNet+ develops a feature extraction module to learn common local-, short- and long-term spatio-temporal locality from heterogeneous MR samples, with a convolutional neural network (CNN), long short-term memory cells (
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  ), and attention mechanisms. Specifically, PRNet+ 1) allows the various-length sequences of MR samples, such that the two components (CNN and LSTM) are able to capture spatial locality from the samples within each MR sequence, 2) exploits two attention mechanisms for the time-interval between neighbouring MR samples, together with the one between neighbouring MR sequences, to capture temporal locality, and 3) incorporates the detected transportation modes and predicted locations of heterogeneous MR data into a joint loss for better result.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_1.12.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>124. Table Parsing Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Cycle-CenterNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cycle-CenterNet
  </strong>
  is a table structure parsing approach built on
  <a href="https://paperswithcode.com/method/centernet">
   CenterNet
  </a>
  that uses a cycle-pairing module to simultaneously detect and group tabular cells into structured tables. It also utilizes a pairing loss which enables the grouping of discrete cells into the structured tables.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.51.00_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>125. Mixture-of-Experts</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>DSelect-k</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DSelect-k
  </strong>
  is a continuously differentiable and sparse gate for Mixture-of-experts (MoE), based on a novel binary encoding formulation. Given a user-specified parameter $k$, the gate selects at most $k$ out of the $n$ experts. The gate can be trained using first-order methods, such as stochastic gradient descent, and offers explicit control over the number of experts to select. This explicit control over sparsity leads to a cardinality-constrained optimization problem, which is computationally challenging. To circumvent this challenge, the authors use a unconstrained reformulation that is equivalent to the original problem. The reformulated problem uses a binary encoding scheme to implicitly enforce the cardinality constraint. By carefully smoothing the binary encoding variables, the reformulated problem can be effectively optimized using first-order methods such as
  <a href="https://paperswithcode.com/method/sgd">
   SGD
  </a>
  .
 </p>
 <p>
  The motivation for this method is that  existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to convergence and statistical performance issues when training with gradient-based methods.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_3.56.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>126. Mesh-Based Simulation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>MeshGraphNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MeshGraphNet
  </strong>
  is a framework for learning mesh-based simulations using
  <a href="https://paperswithcode.com/methods/category/graph-models">
   graph neural networks
  </a>
  . The model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. The model uses an Encode-Process-Decode architecture trained with one-step supervision, and can be applied iteratively to generate long trajectories at inference time. The encoder transforms the input mesh $M^{t}$ into a graph, adding extra world-space edges. The processor performs several rounds of message passing along mesh edges and world edges, updating all node and edge embeddings. The decoder extracts the acceleration for each node, which is used to update the mesh to produce $M^{t+1}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/198113d0-643b-421f-b67e-5e58c6e085ed.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>127. Statistical Inference</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Estimation Statistics</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. The primary aim of estimation methods is to report an effect size (a point estimate) along with its confidence interval, the latter of which is related to the precision of the estimate. The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a P value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>128. Air Quality Forecasting</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GAGNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GAGNN
  </strong>
  , or
  <strong>
   Group-aware Graph Neural Network
  </strong>
  , is a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-31_at_9.54.47_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>129. Twin Networks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Twin Networks
    </strong>
    are a type of neural network architecture where we use two of the same network architecture to perform a task. For example, Siamese Networks are used to learn representations that differentiate between inputs (learning their similarity). Below you can find a continuously updating list of twin network architectures.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Siamese Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Siamese Network
  </strong>
  consists of twin networks which accept distinct inputs but are joined by an energy function at the top. This function computes a metric between the highest level feature representation on each side. The parameters between the twin networks are tied.
  <a href="https://paperswithcode.com/method/weight-tying">
   Weight tying
  </a>
  guarantees that two extremely similar images are not mapped by each network to very different locations in feature space because each network computes the same function. The network is symmetric, so that whenever we present two distinct images to the twin networks, the top conjoining layer will compute the same metric as if we were to we present the same two images but to the opposite twins.
 </p>
 <p>
  Intuitively instead of trying to classify inputs, a siamese network learns to differentiate between inputs, learning their similarity. The loss function used is usually a form of contrastive loss.
 </p>
 <p>
  Source:
  <a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">
   Koch et al
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_9.48.12_PM_aUqN5WU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>130. Incident Aggregation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>GRLIA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GRLIA
  </strong>
  is an incident aggregation framework for online service systems based on graph representation learning over the cascading graph of cloud failures. A representation vector is learned for each unique type of incident in an unsupervised and unified manner, which is able to simultaneously encode the topological and temporal correlations among incidents.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-31_at_9.59.47_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>131. Label Correction</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Label Quality Model</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Label Quality Model
  </strong>
  is an intermediate supervised task aimed at predicting the clean labels from noisy labels by leveraging rater features and a paired subset for supervision. The LQM technique assumes the existence of rater features and a subset of training data with both noisy and clean labels, which we call paired-subset. In real world scenarios, some level of label noise may be unavoidable. The LQM approach still works as long as the clean(er) label is less noisy than a label from a rater that is randomly selected from the pool, e.g., clean labels can be from either expert raters or aggregation of multiple raters. LQM is trained on the paired-subset using rater features and noisy label as input, and inferred on the entire training corpus. The output of LQM is used during model training as a more accurate alternative to the noisy labels.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/996d28f3-d3c8-4dc7-aba2-96640a7c4251.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>132. Ternarization</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>Ternary Weight Splitting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Ternary Weight Splitting
  </strong>
  is a ternarization approach used in
  <a href="https://www.paperswithcode.com/method/binarybert">
   BinaryBERT
  </a>
  that exploits the flatness of ternary loss landscape as the optimization proxy of the binary model. We first train the half-sized ternary BERT to convergence, and then split both the latent full-precision weight $\mathbf{w}^{t}$ and quantized $\hat{\mathbf{w}}^{t}$ to their binary counterparts $\mathbf{w}_{1}^{b}, \mathbf{w}_{2}^{b}$ and $\hat{\mathbf{w}}_{1}^{b}, \hat{\mathbf{w}}_{2}^{b}$ via the TWS operator. To inherit the performance of the ternary model after splitting, the TWS operator requires the splitting equivalency (i.e., the same output given the same input):
 </p>
 <p>
  $$
\mathbf{w}^{t}=\mathbf{w}_{1}^{b}+\mathbf{w}_{2}^{b}, \quad \hat{\mathbf{w}}^{t}=\hat{\mathbf{w}}_{1}^{b}+\hat{\mathbf{w}}_{2}^{b}
$$
 </p>
 <p>
  While solution to the above equation is not unique, we constrain the latent full-precision weights after splitting $\mathbf{w}_{1}^{b}, \mathbf{w}_{2}^{b}$ to satisfy $\mathbf{w}^{t}=\mathbf{w}_{1}^{b}+\mathbf{w}_{2}^{b}$. See the paper for more details.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/05d620ad-a6d8-4a0f-b68f-db8be90d1264.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>133. Fuzzy Logic</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li>
            <details class="method">
            <summary>NFN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Neo-fuzzy-neuron
  </strong>
  is a type of artificial neural network that combines the characteristics of both fuzzy logic and neural networks. It uses a fuzzy inference system to model non-linear relationships between inputs and outputs, and a feedforward neural network to learn the parameters of the fuzzy system. The combination of these two approaches provides a flexible and powerful tool for solving a wide range of problems in areas such as pattern recognition, control, and prediction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        </div></body></html>