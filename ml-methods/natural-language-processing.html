<!DOCTYPE html>
<html>

<head>
	<title>PWC Categories</title>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	<link rel="stylesheet" href="styles.css">
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="script.js"></script>
</head>

<body>
	<div class="lightbox" id="lightbox" style="display: none">
		<img src="" id="lightbox-image" alt="Enlarged Image">
	</div>
	<div class="navbar">
		<a class="nav-btn" href="general.html">General</a>
		<a class="nav-btn" href="computer-vision.html">Computer Vision</a>
		<a class="nav-btn" href="natural-language-processing.html">NLP</a>
		<a class="nav-btn" href="reinforcement-learning.html">Reinforcement Learning</a>
		<a class="nav-btn" href="audio.html">Audio</a>
		<a class="nav-btn" href="sequential.html">Sequential</a>
		<a class="nav-btn" href="graphs.html">Graphs</a>
	</div>
	<div class="bottom-right-buttons">
        <button id="toggle-cats-btn" class="bottom-right-button">Toggle categories</button>
        <button id="close-methods-btn" class="bottom-right-button">Collapse methods</button>
    </div>
	<div class="container mx-auto">
        <ul class="parent cat-importance-1">
            <p>Language Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Language Models
    </strong>
    are models for predicting the next word or character in a document. Below you can find a continuously updating list of language models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/BERT.jpeg" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Diffusion (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Diffusion models generate samples by gradually
removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound (https://arxiv.org/abs/2006.11239).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Focus (2023)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-4 (2023)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-4
  </strong>
  is a transformer based model pre-trained to predict the next token in a document.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/b51f32e3-d2f1-4592-abb0-ba232ec24555.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>LLaMA (2023)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LLaMA
  </strong>
  is a collection of foundation language models ranging from 7B to 65B parameters. It is based on the transformer architecture with various improvements that were subsequently proposed. The main difference with the original architecture are listed below.
 </p>
 <ul>
  <li>
   RMSNorm normalizing function is used to improve the training stability, by normalizing the input of each transformer sub-layer, instead of normalizing the output.
  </li>
  <li>
   The ReLU non-linearity is replaced by the SwiGLU activation function to improve performance.
  </li>
  <li>
   Absolute positional embeddings are removed and instead rotary positional embeddings (RoPE) are added at each layer of the network.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>BART (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BART
  </strong>
  is a
  <a href="https://paperswithcode.com/method/denoising-autoencoder">
   denoising autoencoder
  </a>
  for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ) and a left-to-right decoder (like
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  ). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT2
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ELMo (2018)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embeddings from Language Models
  </strong>
  , or
  <strong>
   ELMo
  </strong>
  , is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.
 </p>
 <p>
  A biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\textbf{ELMO}^{task}_k$ with $\textbf{x}_k$ and pass the ELMO enhanced representation $[\textbf{x}_k; \textbf{ELMO}^{task}_k]$ into the task RNN. Here $\textbf{x}_k$ is a context-independent token representation for each token position.
 </p>
 <p>
  Image Source:
  <a href="https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.41.43_PM_DuQFJHG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>CAM (2015) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Class activation maps could be used to interpret the prediction decision made by the convolutional neural network (CNN).
 </p>
 <p>
  Image source:
  <a href="https://paperswithcode.com/paper/learning-deep-features-for-discriminative">
   Learning Deep Features for Discriminative Localization
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-10_at_16.27.41_2WdgjIH.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mBERT (2018)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  mBERT
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>OPT (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   OPT
  </strong>
  is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The model uses an AdamW optimizer and weight decay of 0.1. It follows a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in the smaller models, and decaying down to 10% of the maximum LR over 300B tokens. The batch sizes range from 0.5M to 4M depending on the model size and is kept constant throughout the course of training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLM-R (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  XLM-R
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>RAG (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Retriever-Augmented Generation
  </strong>
  , or
  <strong>
   RAG
  </strong>
  , is a type of language generation model that combines pre-trained parametric and non-parametric memory for language generation. Specifically, the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.  For query $x$, Maximum Inner Product Search (MIPS) is used to find the top-K documents $z_{i}$. For final prediction $y$, we treat $z$ as a latent variable and marginalize over seq2seq predictions given different documents.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ef5dbabb-23bd-4a09-9a0c-e9c1478fb5b3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>PaLM (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PaLM
  </strong>
  (
  <strong>
   Pathways Language Model
  </strong>
  ) uses a standard Transformer model architecture (Vaswani et al., 2017) in a decoder-only setup (i.e., each timestep can only attend to itself and past timesteps), with several modifications. PaLM is trained as a 540 billion parameter, densely activated, autoregressive Transformer on 780 billion tokens. PaLM leverages Pathways (Barham et al., 2022), which enables highly efficient training of very large neural networks across thousands of accelerator chips.
 </p>
 <p>
  Image credit:
  <a href="https://paperswithcode.com/paper/palm-scaling-language-modeling-with-pathways-1">
   PaLM: Scaling Language Modeling with Pathways
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/edc6477d-9970-4e24-a7a2-93bbf5e37be2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Mamba (2023)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>BLOOM (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BLOOM
  </strong>
  is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of
sources in 46 natural and 13 programming languages (59 in total).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Performer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Performer
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architectures which can estimate regular (
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  ) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. To approximate softmax attention-kernels, Performers use a Fast Attention Via positive Orthogonal Random features approach (FAVOR+), leveraging new methods for approximating softmax and Gaussian kernels.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.30.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Flan-T5 (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Flan-T5
  </strong>
  is the instruction fine-tuned version of
  <strong>
   T5
  </strong>
  or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  Language Model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a04cb14e-e6b8-449e-9487-bc4262911d74.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li class="cat-importance-0">
            <details class="category depth1">
            <summary>Transformers</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Transformers
    </strong>
    are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/trans.jpeg" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Focus (2023)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>BART (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BART
  </strong>
  is a
  <a href="https://paperswithcode.com/method/denoising-autoencoder">
   denoising autoencoder
  </a>
  for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ) and a left-to-right decoder (like
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  ). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT2
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>RAG (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Retriever-Augmented Generation
  </strong>
  , or
  <strong>
   RAG
  </strong>
  , is a type of language generation model that combines pre-trained parametric and non-parametric memory for language generation. Specifically, the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.  For query $x$, Maximum Inner Product Search (MIPS) is used to find the top-K documents $z_{i}$. For final prediction $y$, we treat $z$ as a latent variable and marginalize over seq2seq predictions given different documents.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ef5dbabb-23bd-4a09-9a0c-e9c1478fb5b3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>PaLM (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PaLM
  </strong>
  (
  <strong>
   Pathways Language Model
  </strong>
  ) uses a standard Transformer model architecture (Vaswani et al., 2017) in a decoder-only setup (i.e., each timestep can only attend to itself and past timesteps), with several modifications. PaLM is trained as a 540 billion parameter, densely activated, autoregressive Transformer on 780 billion tokens. PaLM leverages Pathways (Barham et al., 2022), which enables highly efficient training of very large neural networks across thousands of accelerator chips.
 </p>
 <p>
  Image credit:
  <a href="https://paperswithcode.com/paper/palm-scaling-language-modeling-with-pathways-1">
   PaLM: Scaling Language Modeling with Pathways
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/edc6477d-9970-4e24-a7a2-93bbf5e37be2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Performer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Performer
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architectures which can estimate regular (
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  ) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. To approximate softmax attention-kernels, Performers use a Fast Attention Via positive Orthogonal Random features approach (FAVOR+), leveraging new methods for approximating softmax and Gaussian kernels.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.30.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
        <li class="cat-importance-0">
            <details class="category depth2">
            <summary>Autoencoding Transformers</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li class="cat-importance-0">
            <details class="category depth2">
            <summary>Autoregressive Transformers</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li class="cat-importance-0">
            <details class="category depth2">
            <summary>Rendezvous</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    A transformer-based approach with a mixture of several types of attentions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MHMA (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The multi-head of mixed attention combines both self- and cross-attentions, encouraging high-level learning of interactions between entities captured in the various attention features. It is build with several attention heads, each of the head can implement either self or cross attention. A self attention is when the key and query features are the same or come from the same domain features. A cross attention is when the key and query features are generated from different features. Modeling MHMA allows a model to identity the relationship between features of different domains. This is very useful in tasks involving relationship modeling such as human-object interaction, tool-tissue interaction, man-machine interaction, human-computer interface, etc.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6ccc765a-9fa3-48d9-809d-80e6da7be14a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent cat-importance-1">
            <p>Transformers</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Transformers
    </strong>
    are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/trans.jpeg" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Focus (2023)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>BART (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BART
  </strong>
  is a
  <a href="https://paperswithcode.com/method/denoising-autoencoder">
   denoising autoencoder
  </a>
  for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ) and a left-to-right decoder (like
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  ). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT2
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>RAG (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Retriever-Augmented Generation
  </strong>
  , or
  <strong>
   RAG
  </strong>
  , is a type of language generation model that combines pre-trained parametric and non-parametric memory for language generation. Specifically, the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.  For query $x$, Maximum Inner Product Search (MIPS) is used to find the top-K documents $z_{i}$. For final prediction $y$, we treat $z$ as a latent variable and marginalize over seq2seq predictions given different documents.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ef5dbabb-23bd-4a09-9a0c-e9c1478fb5b3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>PaLM (2022)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PaLM
  </strong>
  (
  <strong>
   Pathways Language Model
  </strong>
  ) uses a standard Transformer model architecture (Vaswani et al., 2017) in a decoder-only setup (i.e., each timestep can only attend to itself and past timesteps), with several modifications. PaLM is trained as a 540 billion parameter, densely activated, autoregressive Transformer on 780 billion tokens. PaLM leverages Pathways (Barham et al., 2022), which enables highly efficient training of very large neural networks across thousands of accelerator chips.
 </p>
 <p>
  Image credit:
  <a href="https://paperswithcode.com/paper/palm-scaling-language-modeling-with-pathways-1">
   PaLM: Scaling Language Modeling with Pathways
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/edc6477d-9970-4e24-a7a2-93bbf5e37be2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Performer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Performer
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architectures which can estimate regular (
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  ) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. To approximate softmax attention-kernels, Performers use a Fast Attention Via positive Orthogonal Random features approach (FAVOR+), leveraging new methods for approximating softmax and Gaussian kernels.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_2.30.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-0">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li class="cat-importance-1">
            <details class="category depth1">
            <summary>Autoencoding Transformers</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li class="cat-importance-1">
            <details class="category depth1">
            <summary>Autoregressive Transformers</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li class="cat-importance-0">
            <details class="category depth1">
            <summary>Rendezvous</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    A transformer-based approach with a mixture of several types of attentions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MHMA (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The multi-head of mixed attention combines both self- and cross-attentions, encouraging high-level learning of interactions between entities captured in the various attention features. It is build with several attention heads, each of the head can implement either self or cross attention. A self attention is when the key and query features are the same or come from the same domain features. A cross attention is when the key and query features are generated from different features. Modeling MHMA allows a model to identity the relationship between features of different domains. This is very useful in tasks involving relationship modeling such as human-object interaction, tool-tissue interaction, man-machine interaction, human-computer interface, etc.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6ccc765a-9fa3-48d9-809d-80e6da7be14a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Autoencoding Transformers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BERT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BERT
  </strong>
  , or Bidirectional Encoder Representations from Transformers, improves upon standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformers
  </a>
  by removing the unidirectionality constraint by using a
  <em>
   masked language model
  </em>
  (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a
  <em>
   next sentence prediction
  </em>
  task that jointly pre-trains text-pair representations.
 </p>
 <p>
  There are two steps in BERT:
  <em>
   pre-training
  </em>
  and
  <em>
   fine-tuning
  </em>
  . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they
are initialized with the same pre-trained parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>RoBERTa (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RoBERTa
  </strong>
  is an extension of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  with changes to the pretraining procedure. The modifications include:
 </p>
 <ul>
  <li>
   training the model longer, with bigger batches, over more data
  </li>
  <li>
   removing the next sentence prediction objective
  </li>
  <li>
   training on longer sequences
  </li>
  <li>
   dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.41.28_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>T5 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   T5
  </strong>
  , or
  <strong>
   Text-to-Text Transfer Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  include:
 </p>
 <ul>
  <li>
   adding a
   <em>
    causal
   </em>
   decoder to the bidirectional architecture.
  </li>
  <li>
   replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_text_to_text.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ALBERT (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALBERT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture based on
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.
 </p>
 <p>
  Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.20.56_PM_sqj0MAw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DistilBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DistilBERT
  </strong>
  is a small, fast, cheap and light
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model based on the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/distilbert_COTEMhF.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Electric (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Electric
  </strong>
  is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.
 </p>
 <p>
  Specifically, like BERT, Electric also models $p_{\text {data }}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\mathbf{x}=\left[x_{1}, \ldots, x_{n}\right]$ into contextualized vector representations $\mathbf{h}(\mathbf{x})=\left[\mathbf{h}_{1}, \ldots, \mathbf{h}_{n}\right]$ using a transformer network. The model assigns a given position $t$ an energy score
 </p>
 <p>
  $$
E(\mathbf{x})_{t}=\mathbf{w}^{T} \mathbf{h}(\mathbf{x})_{t}
$$
 </p>
 <p>
  using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as
 </p>
 <p>
  $$
p_{\theta}\left(x_{t} \mid \mathbf{x}_{\backslash t}\right)=\exp \left(-E(\mathbf{x})_{t}\right) / Z\left(\mathbf{x}_{\backslash t}\right) 
$$
 </p>
 <p>
  $$
=\frac{\exp \left(-E(\mathbf{x})_{t}\right)}{\sum_{x^{\prime} \in \mathcal{V}} \exp \left(-E\left(\operatorname{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)\right)_{t}\right)}
$$
 </p>
 <p>
  where $\text{REPLACE}\left(\mathbf{x}, t, x^{\prime}\right)$ denotes replacing the token at position $t$ with $x^{\prime}$ and $\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\prime}$ using a softmax layer, a candidate $x^{\prime}$ is passed in as input to the transformer. As a result, computing $p_{\theta}$ is prohibitively expensive because the partition function $Z_{\theta}\left(\mathbf{x}_{\backslash t}\right)$ requires running the transformer $|\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\theta}(\mathbf{x} \backslash t)$ is more due to the expensive scoring function rather than having a large sample space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/863602e8-31d8-41ed-b119-4ff8a66cabb0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ELECTRA (2020)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ELECTRA
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_10.21.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mT5 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mt5
  </strong>
  is a multilingual variant of
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that was pre-trained on a new Common Crawl-based dataset covering $101$ languages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9deb264b-f062-4992-b875-8635a3c5796a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Longformer (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Longformer
  </strong>
  is a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture. Traditional
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer-based models
  </a>
  are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this,
  <strong>
   Longformer
  </strong>
  uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
 </p>
 <p>
  The attention patterns utilised include:
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   sliding window attention
  </a>
  ,
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated sliding window attention
  </a>
  and global + sliding window. These can be viewed in the components section of this page.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.04.33_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DeBERTa (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeBERTa
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based neural language model that aims to improve the
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  models with two techniques: a
  <a href="https://paperswithcode.com/method/disentangled-attention-mechanism">
   disentangled attention mechanism
  </a>
  and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cdd53959-f9f1-4f79-b604-3f531e431abd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>mBART (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   mBART
  </strong>
  is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the
  <a href="https://paperswithcode.com/method/bart">
   BART objective
  </a>
  . The input texts are noised by masking phrases and permuting sentences, and a single
  <a href="https://paperswithcode.com/method/transformer">
   Transformer model
  </a>
  is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.01.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLM (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLM
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based architecture that is pre-trained using one of three language modelling objectives:
 </p>
 <ol>
  <li>
   Causal Language Modeling - models the probability of a word given the previous words in a sentence.
  </li>
  <li>
   Masked Language Modeling - the masked language modeling objective of
   <a href="https://paperswithcode.com/method/bert">
    BERT
   </a>
   .
  </li>
  <li>
   Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.
  </li>
 </ol>
 <p>
  The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_XLM.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-1">
            <p>Word Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>GloVe (2014)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GloVe Embeddings
  </strong>
  are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:
 </p>
 <p>
  $$ J=\sum_{i, j=1}^{V}f\left(𝑋_{i j}\right)(w^{T}_{i}\tilde{w}_{j} + b_{i} + \tilde{b}_{j} - \log{𝑋}_{ij})^{2} $$
 </p>
 <p>
  where $w_{i}$ and $b_{i}$ are the word vector and bias respectively of word $i$, $\tilde{w}_{j}$ and $b_{j}$ are the context word vector and bias respectively of word $j$, $X_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_1.48.38_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ELMo (2018)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embeddings from Language Models
  </strong>
  , or
  <strong>
   ELMo
  </strong>
  , is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.
 </p>
 <p>
  A biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\textbf{ELMO}^{task}_k$ with $\textbf{x}_k$ and pass the ELMO enhanced representation $[\textbf{x}_k; \textbf{ELMO}^{task}_k]$ into the task RNN. Here $\textbf{x}_k$ is a context-independent token representation for each token position.
 </p>
 <p>
  Image Source:
  <a href="https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.41.43_PM_DuQFJHG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>fastText (2016)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   fastText
  </strong>
  embeddings exploit subword information to construct word embeddings. Representations are learnt of character $n$-grams, and words represented as the sum of the $n$-gram vectors. This extends the word2vec type models with subword information. This helps the embeddings understand suffixes and prefixes. Once a word is represented using character $n$-grams, a skipgram model is trained to learn the embeddings.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.40.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>UNITER (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  UNITER or UNiversal Image-TExt Representation model is a large-scale pre-trained model for joint multimodal embedding. It is pre-trained using four image-text datasets COCO, Visual Genome, Conceptual Captions, and SBU Captions. It can power heterogeneous downstream V+L tasks with joint multimodal embeddings. 
UNITER takes the visual regions of the image and textual tokens of the sentence as inputs. A faster R-CNN is used in Image Embedder to extract the visual features of each region and a Text Embedder is used to tokenize the input sentence into WordPieces.
 </p>
 <p>
  It proposes WRA via the Optimal Transport to provide more fine-grained alignment between word tokens and image regions that is effective in calculating the minimum cost of transporting the contextualized image embeddings to word embeddings and vice versa.
 </p>
 <p>
  Four pretraining tasks were designed for this model. They are Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). This model is different from the previous models because it uses conditional masking on pre-training tasks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6bf9dc19-6dfb-4c31-a71a-450dd32d9850.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        <li class="cat-importance-1">
            <details class="category depth1">
            <summary>Contextualized Word Embeddings</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>ELMo (2018)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embeddings from Language Models
  </strong>
  , or
  <strong>
   ELMo
  </strong>
  , is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.
 </p>
 <p>
  A biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\textbf{ELMO}^{task}_k$ with $\textbf{x}_k$ and pass the ELMO enhanced representation $[\textbf{x}_k; \textbf{ELMO}^{task}_k]$ into the task RNN. Here $\textbf{x}_k$ is a context-independent token representation for each token position.
 </p>
 <p>
  Image Source:
  <a href="https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.41.43_PM_DuQFJHG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>USE (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CoVe (2017) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CoVe
  </strong>
  , or
  <strong>
   Contextualized Word Vectors
  </strong>
  , uses a deep
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  encoder from an attentional sequence-to-sequence model trained for machine translation to contextualize word vectors. $\text{CoVe}$ word embeddings are therefore a function of the entire input sequence. These word embeddings can then be used in downstream tasks by concatenating them with $\text{GloVe}$ embeddings:
 </p>
 <p>
  $$ v = \left[\text{GloVe}\left(x\right), \text{CoVe}\left(x\right)\right]$$
 </p>
 <p>
  and then feeding these in as features for the task-specific models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_1.22.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Cross-View Training (2018) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cross View Training
  </strong>
  , or
  <strong>
   CVT
  </strong>
  , is a semi-supervised algorithm for training distributed word representations that makes use of unlabelled and labelled examples.
 </p>
 <p>
  CVT adds $k$ auxiliary prediction modules to the model, a Bi-
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  encoder, which are used when learning on unlabeled examples. A prediction module is usually a small neural network (e.g., a hidden layer followed by a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer). Each one takes as input an intermediate representation $h^j(x_i)$ produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model). It outputs a distribution over labels $p_{j}^{\theta}\left(y\mid{x_{i}}\right)$.
 </p>
 <p>
  Each $h^j$ is chosen such that it only uses a part of the input $x_i$; the particular choice can depend on the task and model architecture. The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces $p_\theta$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_4.37.39_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li class="cat-importance-1">
            <details class="category depth1">
            <summary>Static Word Embeddings</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>GloVe (2014)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GloVe Embeddings
  </strong>
  are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:
 </p>
 <p>
  $$ J=\sum_{i, j=1}^{V}f\left(𝑋_{i j}\right)(w^{T}_{i}\tilde{w}_{j} + b_{i} + \tilde{b}_{j} - \log{𝑋}_{ij})^{2} $$
 </p>
 <p>
  where $w_{i}$ and $b_{i}$ are the word vector and bias respectively of word $i$, $\tilde{w}_{j}$ and $b_{j}$ are the context word vector and bias respectively of word $j$, $X_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_1.48.38_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>fastText (2016)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   fastText
  </strong>
  embeddings exploit subword information to construct word embeddings. Representations are learnt of character $n$-grams, and words represented as the sum of the $n$-gram vectors. This extends the word2vec type models with subword information. This helps the embeddings understand suffixes and prefixes. Once a word is represented using character $n$-grams, a skipgram model is trained to learn the embeddings.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.40.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Skip-gram Word2Vec (2013) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Skip-gram Word2Vec
  </strong>
  is an architecture for computing word embeddings. Instead of using surrounding words to predict the center word, as with CBow Word2Vec, Skip-gram Word2Vec uses the central word to predict the surrounding words.
 </p>
 <p>
  The skip-gram objective function sums the log probabilities of the surrounding $n$ words to the left and right of the target word $w_{t}$ to produce the following objective:
 </p>
 <p>
  $$J_\theta = \frac{1}{T}\sum^{T}_{t=1}\sum_{-n\leq{j}\leq{n}, \neq{0}}\log{p}\left(w_{j+1}\mid{w_{t}}\right)$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_2.04.55_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CBoW Word2Vec (2013) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Continuous Bag-of-Words Word2Vec
  </strong>
  is an architecture for creating word embeddings that uses $n$ future words as well as $n$ past words to create a word embedding. The objective function for CBOW is:
 </p>
 <p>
  $$ J_\theta = \frac{1}{T}\sum^{T}_{t=1}\log{p}\left(w_{t}\mid{w}_{t-n},\ldots,w_{t-1}, w_{t+1},\ldots,w_{t+n}\right) $$
 </p>
 <p>
  In the CBOW model, the distributed representations of context are used to predict the word in the middle of the window. This contrasts with
  <a href="https://paperswithcode.com/method/skip-gram-word2vec">
   Skip-gram Word2Vec
  </a>
  where the distributed representation of the input word is used to predict the context.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_2.04.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Autoregressive Transformers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer (2017)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Transformer
  </strong>
  is a model architecture that eschews recurrence and instead relies entirely on an
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanism
  </a>
  to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of
  <a href="https://paperswithcode.com/methods/category/attention-mechanisms-1">
   attention mechanisms
  </a>
  allows for significantly more parallelization than methods like
  <a href="https://paperswithcode.com/methods/category/recurrent-neural-networks">
   RNNs
  </a>
  and
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-3 (2020)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-3
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/methods/category/transformers">
   transformer
  </a>
  model with 175 billion
parameters. It uses the same architecture/model as
  <a href="https://paperswithcode.com/method/gpt-2">
   GPT-2
  </a>
  , including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  , similar to the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_10.27.17_PM_WRE3tj7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>GPT-2 (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GPT-2
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a
  <a href="https://paperswithcode.com/dataset/webtext">
   WebText dataset
  </a>
  - text from 45 million website links. It largely follows the previous
  <a href="https://paperswithcode.com/method/gpt">
   GPT
  </a>
  architecture with some modifications:
 </p>
 <ul>
  <li>
   <p>
    <a href="https://paperswithcode.com/method/layer-normalization">
     Layer normalization
    </a>
    is moved to the input of each sub-block, similar to a
pre-activation residual network and an additional layer normalization was added after the final self-attention block.
   </p>
  </li>
  <li>
   <p>
    A modified initialization which accounts for the accumulation on the residual path with model depth
is used. Weights of residual layers are scaled at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers.
   </p>
  </li>
  <li>
   <p>
    The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and
a larger batch size of 512 is used.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_2.01.27_PM_YPL4rHk.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>XLNet (2019)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XLNet
  </strong>
  is an autoregressive
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.
 </p>
 <p>
  Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of
  <a href="https://paperswithcode.com/method/transformer-xl">
   Transformer-XL
  </a>
  into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_dwpaEuNcwmPQUOKi_COw4cmZ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Transformer-XL (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Transformer-XL
  </strong>
  (meaning extra long) is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_10.49.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Attention Patterns</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    The original
    <a href="https://paperswithcode.com/method/scaled">
     self-attention component
    </a>
    in the
    <a href="https://paperswithcode.com/method/transformer">
     Transformer
    </a>
    architecture has a $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Attention pattern methods look to reduce this complexity by looking at a subset of the space.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/strided.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Strided Attention (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Strided Attention
  </strong>
  is a factorized attention pattern that has one head attend to the previous
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\sqrt{n}$. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Strided Attention, $A^{(1)}_{i} = ${$t, t + 1, ..., i$} for $t = \max\left(0, i − l\right)$, and $A^{(2)}_{i} = ${$j : (i − j) \mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_3.19.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Fixed Factorized Attention (2019)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fixed Factorized Attention
  </strong>
  is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the
  <a href="https://paperswithcode.com/method/sparse-transformer">
   Sparse Transformer
  </a>
  architecture.
 </p>
 <p>
  A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \text{set}\left(S_{1}, \dots, S_{n}\right)$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:
 </p>
 <p>
  $$ \text{Attend}\left(X, S\right) = \left(a\left(\mathbf{x}_{i}, S_{i}\right)\right)_{i\in\text{set}\left(1,\dots,n\right)}$$
 </p>
 <p>
  $$ a\left(\mathbf{x}_{i}, S_{i}\right) = \text{softmax}\left(\frac{\left(W_{q}\mathbf{x}_{i}\right)K^{T}_{S_{i}}}{\sqrt{d}}\right)V_{S_{i}} $$
 </p>
 <p>
  $$ K_{Si} = \left(W_{k}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  $$ V_{Si} = \left(W_{v}\mathbf{x}_{j}\right)_{j\in{S_{i}}} $$
 </p>
 <p>
  Here $W_{q}$, $W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $x_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.
 </p>
 <p>
  Full self-attention for autoregressive models defines $S_{i} = \text{set}\left(j : j \leq i\right)$, allowing every element to attend to all previous positions and its own position.
 </p>
 <p>
  Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A_{i}^{(m)} ⊂ \text{set}\left(j : j \leq i\right)$ and lets $S_{i} = A_{i}^{(m)}$. The goal with the Sparse
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  was to find efficient choices for the subset $A$.
 </p>
 <p>
  Formally for Fixed Factorized Attention, $A^{(1)}_{i} = ${$j : \left(\lfloor{j/l\rfloor}=\lfloor{i/l\rfloor}\right)$}, where the brackets denote the floor operation, and $A^{(2)}_{i} = ${$j : j \mod l \in ${$t, t+1, \ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}_{i}$ or $A^{(2)}_{i}$. This pattern can be visualized in the figure to the right.
 </p>
 <p>
  If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.
 </p>
 <p>
  A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \in ${$8, 16, 32$} for typical values of $l \in
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the
  <a href="https://paperswithcode.com/method/strided-attention">
   strided attention
  </a>
  .
 </p>
 <p>
  Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.19.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Sliding Window Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.
 </p>
 <p>
  More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\left(n×w\right)$,
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)
 </p>
 <p>
  In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Global and Sliding Window Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global and Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Since
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   windowed
  </a>
  and
  <a href="https://paperswithcode.com/method/dilated-sliding-window-attention">
   dilated
  </a>
  attention patterns are not flexible enough to learn task-specific representations, the authors of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Dilated Sliding Window Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Sliding Window Attention
  </strong>
  is an attention pattern for attention-based models. It was proposed as part of the
  <a href="https://paperswithcode.com/method/longformer">
   Longformer
  </a>
  architecture. It is motivated by the fact that non-sparse attention in the original
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  formulation has a
  <a href="https://paperswithcode.com/method/scaled">
   self-attention component
  </a>
  with $O\left(n^{2}\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs.
 </p>
 <p>
  Compared to a
  <a href="https://paperswithcode.com/method/sliding-window-attention">
   Sliding Window Attention
  </a>
  pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to
  <a href="https://paperswithcode.com/method/dilated-convolution">
   dilated CNNs
  </a>
  where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Sentence Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MTS () </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>SBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>SimCSE (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SimCSE
  </strong>
  is a contrastive learning framework for generating sentence embeddings. It utilizes an unsupervised approach, which takes an input sentence and predicts itself in contrastive objective, with only standard
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  used as noise. The authors find that dropout acts as minimal “data augmentation” of hidden representations, while removing it leads to a representation collapse. Afterwards a supervised approach is used, which incorporates annotated pairs from natural language inference datasets into the contrastive framework, by using “entailment” pairs as positives and “contradiction
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_1.19.35_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Mirror-BERT (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Mirror-BERT converts pretrained language models into effective universal text encoders without any supervision, in 20-30 seconds. It is an extremely simple, fast, and effective contrastive learning technique. It relies on fully identical
  <em>
   or
  </em>
  slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cd18d6ac-ca08-4fdb-bc69-69e4551372d1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Static Word Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>GloVe (2014)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GloVe Embeddings
  </strong>
  are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:
 </p>
 <p>
  $$ J=\sum_{i, j=1}^{V}f\left(𝑋_{i j}\right)(w^{T}_{i}\tilde{w}_{j} + b_{i} + \tilde{b}_{j} - \log{𝑋}_{ij})^{2} $$
 </p>
 <p>
  where $w_{i}$ and $b_{i}$ are the word vector and bias respectively of word $i$, $\tilde{w}_{j}$ and $b_{j}$ are the context word vector and bias respectively of word $j$, $X_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_1.48.38_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>fastText (2016)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   fastText
  </strong>
  embeddings exploit subword information to construct word embeddings. Representations are learnt of character $n$-grams, and words represented as the sum of the $n$-gram vectors. This extends the word2vec type models with subword information. This helps the embeddings understand suffixes and prefixes. Once a word is represented using character $n$-grams, a skipgram model is trained to learn the embeddings.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.40.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Skip-gram Word2Vec (2013) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Skip-gram Word2Vec
  </strong>
  is an architecture for computing word embeddings. Instead of using surrounding words to predict the center word, as with CBow Word2Vec, Skip-gram Word2Vec uses the central word to predict the surrounding words.
 </p>
 <p>
  The skip-gram objective function sums the log probabilities of the surrounding $n$ words to the left and right of the target word $w_{t}$ to produce the following objective:
 </p>
 <p>
  $$J_\theta = \frac{1}{T}\sum^{T}_{t=1}\sum_{-n\leq{j}\leq{n}, \neq{0}}\log{p}\left(w_{j+1}\mid{w_{t}}\right)$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_2.04.55_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CBoW Word2Vec (2013) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Continuous Bag-of-Words Word2Vec
  </strong>
  is an architecture for creating word embeddings that uses $n$ future words as well as $n$ past words to create a word embedding. The objective function for CBOW is:
 </p>
 <p>
  $$ J_\theta = \frac{1}{T}\sum^{T}_{t=1}\log{p}\left(w_{t}\mid{w}_{t-n},\ldots,w_{t-1}, w_{t+1},\ldots,w_{t+n}\right) $$
 </p>
 <p>
  In the CBOW model, the distributed representations of context are used to predict the word in the middle of the window. This contrasts with
  <a href="https://paperswithcode.com/method/skip-gram-word2vec">
   Skip-gram Word2Vec
  </a>
  where the distributed representation of the input word is used to predict the context.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_2.04.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Contextualized Word Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>ELMo (2018)  <span title=">25 papers/year"> &#10548;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Embeddings from Language Models
  </strong>
  , or
  <strong>
   ELMo
  </strong>
  , is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.
 </p>
 <p>
  A biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\textbf{ELMO}^{task}_k$ with $\textbf{x}_k$ and pass the ELMO enhanced representation $[\textbf{x}_k; \textbf{ELMO}^{task}_k]$ into the task RNN. Here $\textbf{x}_k$ is a context-independent token representation for each token position.
 </p>
 <p>
  Image Source:
  <a href="https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.41.43_PM_DuQFJHG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>USE (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CoVe (2017) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CoVe
  </strong>
  , or
  <strong>
   Contextualized Word Vectors
  </strong>
  , uses a deep
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  encoder from an attentional sequence-to-sequence model trained for machine translation to contextualize word vectors. $\text{CoVe}$ word embeddings are therefore a function of the entire input sequence. These word embeddings can then be used in downstream tasks by concatenating them with $\text{GloVe}$ embeddings:
 </p>
 <p>
  $$ v = \left[\text{GloVe}\left(x\right), \text{CoVe}\left(x\right)\right]$$
 </p>
 <p>
  and then feeding these in as features for the task-specific models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_1.22.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Cross-View Training (2018) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cross View Training
  </strong>
  , or
  <strong>
   CVT
  </strong>
  , is a semi-supervised algorithm for training distributed word representations that makes use of unlabelled and labelled examples.
 </p>
 <p>
  CVT adds $k$ auxiliary prediction modules to the model, a Bi-
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  encoder, which are used when learning on unlabeled examples. A prediction module is usually a small neural network (e.g., a hidden layer followed by a
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer). Each one takes as input an intermediate representation $h^j(x_i)$ produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model). It outputs a distribution over labels $p_{j}^{\theta}\left(y\mid{x_{i}}\right)$.
 </p>
 <p>
  Each $h^j$ is chosen such that it only uses a part of the input $x_i$; the particular choice can depend on the task and model architecture. The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces $p_\theta$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_4.37.39_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Text Classification Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MixText (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MixText
  </strong>
  is a semi-supervised learning method for text classification, which uses a new data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. The technique leverages advances in data augmentation to guess low-entropy labels for unlabeled data, making them as easy to use as labeled data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.02.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Lbl2Vec (2022) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ALDEN (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALDEN
  </strong>
  , or
  <strong>
   Active Learning with DivErse iNterpretations
  </strong>
  , is an active learning approach for text classification. With local interpretations in DNNs, ALDEN identifies linearly separable regions of samples. Then, it selects samples according to their diversity of local interpretations and queries their labels.
 </p>
 <p>
  Specifically, we first calculate the local interpretations in DNN for each sample as the gradient backpropagated from the final
predictions to the input features. Then, we use the most diverse interpretation of words in a sample to measure its diverseness. Accordingly, we select unlabeled samples with the maximally diverse interpretations for labeling and retrain the model with these
labeled samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_10.05.54_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>DualCL (2022) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Contrastive learning has achieved remarkable success in representation learning via self-supervision in unsupervised settings. However, effectively adapting contrastive learning to supervised learning tasks remains as a challenge in practice. In this work, we introduce a dual contrastive learning (DualCL) framework that simultaneously learns the features of input samples and the parameters of classifiers in the same space. Specifically, DualCL regards the parameters of the classifiers as augmented samples associating to different labels and then exploits the contrastive learning between the input samples and the augmented samples. Empirical studies on five benchmark text classification datasets and their low-resource version demonstrate the improvement in classification accuracy and confirm the capability of learning discriminative representations of DualCL.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-1">
            <p>Subword Segmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BPE (2015)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Byte Pair Encoding
  </strong>
  , or
  <strong>
   BPE
  </strong>
  , is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).
 </p>
 <p>
  <a href="https://leimao.github.io/blog/Byte-Pair-Encoding/">
   Lei Mao
  </a>
  has a detailed blog post that explains how this works.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.39.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>WordPiece (2016)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   WordPiece
  </strong>
  is a subword segmentation algorithm used in natural language processing.  The vocabulary is initialized with individual characters in the language, then the most frequent combinations of symbols in the vocabulary are iteratively added to the vocabulary. The process is:
 </p>
 <ol>
  <li>
   Initialize the word unit inventory with all the characters in the text.
  </li>
  <li>
   Build a language model on the training data using the inventory from 1.
  </li>
  <li>
   Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.
  </li>
  <li>
   Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.
  </li>
 </ol>
 <p>
  Text:
  <a href="https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944">
   Source
  </a>
 </p>
 <p>
  Image: WordPiece as used in
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.57.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Gradient-Based Subword Tokenization (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GBST
  </strong>
  , or
  <strong>
   Gradient-based Subword Tokenization Module
  </strong>
  , is a soft gradient-based subword tokenization module that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network.
 </p>
 <p>
  GBST learns a position-wise soft selection over candidate subword blocks by scoring them with a scoring network. In contrast to prior tokenization-free methods, GBST learns interpretable latent subwords, which enables easy inspection of lexical representations and is more efficient than other byte-based models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-07-05_at_14.40.01_Oxgsqqn.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Unigram Segmentation (2018) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Unigram Segmentation
  </strong>
  is a subword segmentation algorithm based on a unigram language model. It provides multiple segmentations with probabilities. The language model allows for emulating the noise generated during the segmentation of actual data.
 </p>
 <p>
  The unigram language model makes an assumption that each subword occurs independently, and consequently, the probability of a subword sequence $\mathbf{x} = (x_1,\ldots,x_M)$ is
formulated as the product of the subword occurrence probabilities
$p(x_i)$:
 </p>
 <p>
  $$
  P(\mathbf{x}) = \prod_{i=1}^{M} p(x_i), \\
  \forall i\,\, x_i \in \mathcal{V},\,\,\,
  \sum_{x \in \mathcal{V}} p(x) = 1, \nonumber
$$
 </p>
 <p>
  where $\mathcal{V}$ is a pre-determined vocabulary.  The most probable
segmentation $\mathbf{x}^*$ for the input sentence $X$ is then given by:
 </p>
 <p>
  $$
  \mathbf{x}^{*} = \text{argmax}_{\mathbf{x} \in \mathcal{S}(X)} P(\mathbf{x}),
$$
 </p>
 <p>
  where $\mathcal{S}(X)$ is a set of segmentation candidates built from
the input sentence $X$.  $\mathbf{x}^*$ is obtained with the Viterbi
algorithm.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_hyperparameter.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Document Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Contextualized Topic Models (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Contextualized Topic Models are based on the Neural-ProdLDA variational autoencoding approach by Srivastava and Sutton (2017).
 </p>
 <p>
  This approach trains an encoding neural network to map pre-trained contextualized word embeddings (e.g.,
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ) to latent representations. Those latent representations are sampled variationally from a Gaussian distribution $N(\mu, \sigma^2)$ and passed to a decoder network that has to reconstruct the document bag-of-word representation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/logo_e0vXbiC.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>lda2vec (2016) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   lda2vec
  </strong>
  builds representations over both words and documents by mixing word2vec’s skipgram architecture with Dirichlet-optimized sparse topic mixtures.
 </p>
 <p>
  The Skipgram Negative-Sampling (SGNS) objective of word2vec is modified to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The total loss term $L$ is the sum of the Skipgram Negative Sampling Loss (SGNS) $L^{neg}_{ij}$ with the addition of a Dirichlet-likelihood term over document weights, $L_{d}$. The loss is conducted using a context vector, $\overrightarrow{c_{j}}$ , pivot word vector $\overrightarrow{w_{j}}$, target word vector $\overrightarrow{w_{i}}$, and negatively-sampled word vector $\overrightarrow{w_{l}}$:
 </p>
 <p>
  $$ L = L^{d} + \Sigma_{ij}L^{neg}_{ij} $$
 </p>
 <p>
  $$L^{neg}_{ij} = \log\sigma\left(c_{j}\cdot\overrightarrow{w_{i}}\right) + \sum^{n}_{l=0}\sigma\left(-\overrightarrow{c_{j}}\cdot\overrightarrow{w_{l}}\right)$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.18.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>EDLPS (2000) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of obtaining word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder. This discriminator is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validate our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and provide competitive results on the paraphrase generation and sentiment analysis task on standard dataset. These results are also shown to be statistically significant.
 </p>
 <p>
  Github Link:https://github.com/dev-chauhan/PQG-pytorch.
 </p>
 <p>
  2
The PQG dataset is available on this link: https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs.
 </p>
 <p>
  3
website: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs.
 </p>
 <p>
  4
we report same baseline results as mentioned in [10]
 </p>
 <p>
  5
website: www.kaggle.com/c/sentiment-analysis-on-movie-reviews.
 </p>
 <p>
  6
Code: https://github.com/dev-chauhan/PQG-pytorch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Synthesized Attention Mechanisms</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Dense Synthesized Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dense Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is a type of synthetic attention mechanism that replaces the notion of
  <a href="https://paperswithcode.com/method/scaled">
   query-key-values
  </a>
  in the self-attention module and directly synthesizes the alignment matrix instead. Dense attention is conditioned on each input token. The method accepts an input $X \in \mathbb{R}^{l\text{ x }d}$ and produces an output of $Y \in \mathbb{R}^{l\text{ x }d}$. Here $l$ refers to the sequence length and $d$ refers to the dimensionality of the model. We first adopt $F\left(.\right)$, a parameterized function, for projecting input $X_{i}$ from $d$ dimensions to $l$ dimensions.
 </p>
 <p>
  $$B_{i} = F\left(X_{i}\right)$$
 </p>
 <p>
  where $F\left(.\right)$ is a parameterized function that maps $\mathbb{R}^{d}$ to $\mathbb{R}^{l}$ and $i$ is the $i$-th token of $X$. Intuitively, this can be interpreted as learning a token-wise projection to the sequence length $l$. Essentially, with this model, each token predicts weights for each token in the input sequence. In practice, a simple two layered feed-forward layer with
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activations for $F\left(.\right)$ is adopted:
 </p>
 <p>
  $$ F\left(X\right) = W\left(\sigma_{R}\left(W(X) + b\right)\right) + b$$
 </p>
 <p>
  where $\sigma_{R}$ is the ReLU activation function. Hence, $B$ is now of $\mathbb{R}^{l\text{ x }d}$. Given $B$, we now compute:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(B\right)G\left(X\right) $$
 </p>
 <p>
  where $G\left(.\right)$ is another parameterized function of $X$ that is analogous to $V$ (value) in the standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model. This approach eliminates the
  <a href="https://paperswithcode.com/method/scaled">
   dot product
  </a>
  altogether by replacing $QK^{T}$ in standard Transformers with the synthesizing function $F\left(.\right)$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Random Synthesized Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Synthesized Attention
  </strong>
  is a form of synthesized attention where the attention weights are not conditioned on any input tokens. Instead, the attention weights are initialized to random values. It was introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. Random Synthesized Attention contrasts with
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   Dense Synthesized Attention
  </a>
  which conditions on each token independently, as opposed to pairwise token interactions in the vanilla
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  model.
 </p>
 <p>
  Let $R$ be a randomly initialized matrix. Random Synthesized Attention is defined as:
 </p>
 <p>
  $$Y = \text{Softmax}\left(R\right)G\left(X\right) $$
 </p>
 <p>
  where $R \in \mathbb{R}^{l \text{ x } l}$. Notably, each head adds 2 parameters to the overall network. The basic idea of the Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples. This is a direct generalization of the recently proposed fixed self-attention patterns of
  <a href="https://arxiv.org/abs/2002.10260">
   Raganato et al (2020)
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Factorized Random Synthesized Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Random Synthesized Attention
  </strong>
  , introduced with the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture, is similar to
  <a href="https://paperswithcode.com/method/factorized-dense-synthesized-attention">
   factorized dense synthesized attention
  </a>
  but for random synthesizers. Letting $R$ being a randomly initialized matrix, we factorize $R$ into low rank matrices $R_{1}, R_{2} \in \mathbb{R}^{l\text{ x}k}$ in the attention function:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(R_{1}R_{2}^{T}\right)G\left(X\right) . $$
 </p>
 <p>
  Here $G\left(.\right)$ is a parameterized function that is equivalent to $V$ in
  <a href="https://paperswithcode.com/method/scaled">
   Scaled Dot-Product Attention
  </a>
  .
 </p>
 <p>
  For each head, the factorization reduces the parameter costs from $l^{2}$ to $2\left(lk\right)$ where
$k &lt;&lt; l$ and hence helps prevent overfitting. In practice, we use a small value of $k = 8$.
 </p>
 <p>
  The basic idea of a  Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-02_at_12.06.20_AM_PkacRfG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Factorized Dense Synthesized Attention (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Factorized Dense Synthesized Attention
  </strong>
  is a synthesized attention mechanism, similar to
  <a href="https://paperswithcode.com/method/dense-synthesized-attention">
   dense synthesized attention
  </a>
  , but we factorize the outputs to reduce parameters and prevent overfitting. It was proposed as part of the
  <a href="https://paperswithcode.com/method/synthesizer">
   Synthesizer
  </a>
  architecture. The factorized variant of the dense synthesizer can be expressed as follows:
 </p>
 <p>
  $$A, B = F_{A}\left(X_{i}\right), F_{B}\left(X_{i}\right)$$
 </p>
 <p>
  where $F_{A}\left(.\right)$ projects input $X_{i}$ into $a$ dimensions, $F_B\left(.\right)$ projects $X_{i}$ to $b$ dimensions, and $a \text{ x } b = l$. The output of the factorized module is now written as:
 </p>
 <p>
  $$ Y = \text{Softmax}\left(C\right)G\left(X\right) $$
 </p>
 <p>
  where $C = H_{A}\left(A\right) * H_{B}\left(B\right)$, where $H_{A}$, $H_{B}$ are tiling functions and $C \in \mathbb{R}^{l \text{ x } l}$. The tiling function simply duplicates the vector $k$ times, i.e., $\mathbb{R}^{l} \rightarrow \mathbb{R}^{lk}$. In this case, $H_{A}\left(\right)$ is a projection of $\mathbb{R}^{a} \rightarrow \mathbb{R}^{ab}$ and $H_{B}\left(\right)$ is a projection of $\mathbb{R}^{b} \rightarrow \mathbb{R}^{ba}$. To avoid having similar values within the same block, we compose the outputs of $H_{A}$ and $H_{B}$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_11.54.21_PM_52J3Q9s.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Language Model Pre-Training</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MPNet (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MPNet
  </strong>
  is a pre-training method for language models that combines masked language modeling (MLM) and permuted language modeling (PLM) in one view. It takes the dependency among the predicted tokens into consideration through permuted language modeling and thus avoids the issue of
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  . On the other hand, it takes position information of all tokens as input to make the model see the position information of all the tokens and thus alleviates the position discrepancy of
  <a href="https://paperswithcode.com/method/xlnet">
   XLNet
  </a>
  .
 </p>
 <p>
  The training objective of MPNet is:
 </p>
 <p>
  $$ \mathbb{E}_{z\in{\mathcal{Z}_{n}}} \sum^{n}_{t=c+1}\log{P}\left(x_{z_{t}}\mid{x_{z_{&lt;t}}}, M\_{z\_{{&gt;}{c}}}; \theta\right) $$
 </p>
 <p>
  As can be seen, MPNet conditions on ${x_{z_{&lt;t}}}$ (the tokens preceding the current predicted token $x_{z_{t}}$) rather than only the non-predicted tokens ${x_{z_{&lt;=c}}}$ in MLM; comparing with PLM, MPNet takes more information (i.e., the mask symbol $[M]$ in position $z_{&gt;c}$) as inputs. Although the objective seems simple, it is challenging to implement the model efficiently. For details, see the paper.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_3.25.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ERNIE-GEN (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ERNIE-GEN
  </strong>
  is a multi-flow sequence to sequence pre-training and fine-tuning framework which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.44.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>ReasonBERT (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ReasonBERT
  </strong>
  is a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid, contexts. It utilizes distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases.
 </p>
 <p>
  Specifically, given a query sentence containing an entity pair, if we mask one of the entities, another sentence or table that contains the same pair of entities can likely be used as evidence to recover the masked entity. Moreover, to encourage deeper reasoning, multiple pieces of evidence are collected that are jointly used to recover the masked entities in the query sentence, allowing for the scattering of the masked entities among different pieces of evidence to mimic different types of reasoning.
 </p>
 <p>
  The Figure illustrates several examples using such distant supervision. In Ex. 1, a model needs to check multiple constraints (i.e., intersection reasoning type) and find “the beach soccer competition that is established in 1998.” In Ex. 2, a model needs to find “the type of the band that released Awaken the Guardian,” by first inferring the name of the band “Fates Warning” (i.e., bridging reasoning type).
 </p>
 <p>
  The masked entities in a query sentence are replaced with the [QUESTION] tokens. The new pre-training objective, span reasoning, then extracts the masked entities from the provided evidence. Existing LMs like
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  and
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  are augmented by continuing to train them with the new objective, which leads to ReasonBERT. Then query sentence and textual evidence are encoded via the LM. When tabular evidence is present, the structure-aware
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  <a href="https://paperswithcode.com/method/tapas">
   TAPAS
  </a>
  is used as the encoder to capture the table structure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.43.15_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>K3M (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   K3M
  </strong>
  is a multi-modal pretraining method for e-commerce product data that introduces knowledge modality to correct the noise and supplement the missing of image and text modalities. The modal-encoding layer extracts the features of each modality. The modal-interaction layer is capable of effectively modeling the interaction of multiple modalities, where an initial-interactive feature fusion model is designed to maintain the independence of image modality and text modality, and a structure aggregation module is designed to fuse the information of image, text, and knowledge modalities. K3M is pre-trained with three pretraining tasks, including masked object modeling (MOM), masked language modeling (MLM), and link prediction modeling (
  <a href="https://paperswithcode.com/method/local-prior-matching">
   LPM
  </a>
  ).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.46.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Code Generation Transformers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>CodeBERT (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CodeBERT
  </strong>
  is a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. CodeBERT is developed with a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based neural architecture, and is trained with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables the utilization of both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_1.03.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CodeT5 (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CodeT5
  </strong>
  is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based model for code understanding and generation based on the
  <a href="https://paperswithcode.com/method/t5">
   T5 architecture
  </a>
  . It utilizes an identifier-aware pre-training objective that considers the crucial token type information (identifiers) from code. Specifically, the denoising
  <a href="https://paperswithcode.com/method/seq2seq">
   Seq2Seq
  </a>
  objective of T5 is extended with two identifier tagging and prediction tasks to enable the model to better leverage the token type information from programming languages, which are the identifiers assigned by developers. To improve the natural language-programming language alignment, a bimodal dual learning objective is used for a bidirectional conversion between natural language and programming language.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.10.30_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>PICARD (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CuBERT (2019) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CuBERT
  </strong>
  , or
  <strong>
   Code Understanding BERT
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  based model for code understanding. In order to achieve this, the authors curate a massive corpus of Python programs collected from GitHub. GitHub projects are known to contain a large amount of duplicate code. To avoid biasing the model to such duplicated code, authors perform deduplication using the method of
  <a href="https://arxiv.org/abs/1812.06469">
   Allamanis (2018)
  </a>
  . The resulting corpus has 7.4 million files with a total of 9.3 billion tokens (16 million unique).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/da945303-09fa-49c9-b889-9cd626c639e3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Table Question Answering Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>TAPAS (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TAPAS
  </strong>
  is a weakly supervised question answering model that reasons over tables without generating logical forms. TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ’s architecture with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_12.58.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>MATE (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MATE
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture designed to model the structure of web tables. It uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. Each attention head reorders the tokens by either column or row index and then applies a windowed attention mechanism. Unlike traditional self-attention, Mate scales linearly in the sequence length.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.23.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-1">
            <p>Machine Translation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Seq2Seq (2014)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Seq2Seq
  </strong>
  , or
  <strong>
   Sequence To Sequence
  </strong>
  , is a model used in sequence prediction tasks, such as language modelling and machine translation. The idea is to use one
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  , the
  <em>
   encoder
  </em>
  , to read the input sequence one timestep at a time, to obtain a large fixed dimensional vector representation (a context vector), and then to use another LSTM, the
  <em>
   decoder
  </em>
  , to extract the output sequence
from that vector. The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.
 </p>
 <p>
  (Note that this page refers to the original seq2seq not general sequence-to-sequence models)
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.47.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>PCT (2016) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Question Answering Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Macaw (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Macaw
  </strong>
  is a generative question-answering (QA) system that is built on UnifiedQA, itself built on
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  . Macaw has three interesting features. First, it often produces high-quality answers to questions far outside the domain it was trained on, sometimes surprisingly so. Second, Macaw allows different permutations (“an gles”) of inputs and outputs to be used. For example, we can give it a question and get an answer; or give it an answer and get a question; or give it a question and answer and get a set of multiple-choice (MC) options for that question. This multi-angle QA capability allows versatility in the way Macaw can be used, include recursively using outputs as new inputs to the system. Finally, Macaw also generates explanations as an optional output (or even input) element.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.09.58_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>TransferQA (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TransferQA
  </strong>
  is a transferable generative QA model, built upon
  <a href="https://paperswithcode.com/method/t5">
   T5
  </a>
  that combines extractive QA and multi-choice QA via a text-to-text
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  framework, and tracks both categorical slots and non-categorical slots in DST. In addition, it introduces two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable the model to handle “none” value slots in the zero-shot DST setting.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.57.44_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>EMQAP (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EMQAP
  </strong>
  , or
  <strong>
   E-Manual Question Answering Pipeline
  </strong>
  , is an approach for answering questions pertaining to electronics devices. Built upon the pretrained
  <a href="https://paperswithcode.com/method/roberta">
   RoBERTa
  </a>
  , it harbors a supervised multi-task learning framework which efficiently performs the dual tasks of identifying the section in the E-manual where the answer can be found and the exact answer span within that section.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.24.14_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Relation Extraction Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Partition Filter Network (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Partition Filter Network
  </strong>
  is a framework designed specifically for joint entity and relation extraction. The framework consists of three components: partition filter encoder, NER unit and RE unit. In task units, we use table-filling for word pair prediction. Orange, yellow and green represents NER-related, shared and RE-related component or features. (b) Detailed depiction of partition filter encoder in one single time step. We decompose feature encoding into two steps: partition and filter (shown in the gray area). In partition, we first segment neurons into two task partitions and one shared partition. Then in filter, partitions are selected and combined to form task-specific features and shared features, filtering out information irrelevant to each task.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.47.28_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>HEGCN (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HEGCN
  </strong>
  , or
  <strong>
   Hierarchical Entity Graph Convolutional Network
  </strong>
  is a model for multi-hop relation extraction across documents. Documents in a document chain are encoded using a bi-directional long short-term memory (
  <a href="https://paperswithcode.com/method/bilstm">
   BiLSTM
  </a>
  ) layer. On top of the BiLSTM layer, two graph convolutional networks (
  <a href="https://paperswithcode.com/method/gcn">
   GCN
  </a>
  ) are used, one after another in a hierarchy.
 </p>
 <p>
  In the first level of the GCN hierarchy, a separate entity mention graph is constructed on each document of the chain using all the entities mentioned in that document. Each mention of an entity in a document is considered as a separate node in the graph. A graph convolutional network (GCN) is used to represent the entity mention graph of each document to capture the relations among the entity mentions in the document. A unified entity-level graph is then constructed across all the documents in the chain. Each node of this entity-level graph represents a unique entity in the document chain. Each common entity between two documents in the chain is represented by a single node in the graph. A GCN is used to represent this entity-level graph to capture the relations among the entities across the documents.
 </p>
 <p>
  The representations of the nodes of the subject entity and object entity are concatenated and passed to a feed-forward layer with
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  for relation classification.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_9.48.38_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>CubeRE (2022) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Our model known as CubeRE first encodes each input sentence using a language model encoder to obtain the contextualized sequence representation. We then capture the interaction between each possible head and tail entity as a pair representation for predicting the entity-relation label scores. To reduce the computational cost, each sentence is pruned to retain only words that have higher entity scores. Finally, we capture the interaction between each possible relation triplet and qualifier to predict the qualifier label scores and decode the outputs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-1">
            <p>Tokenizers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>WordPiece (2016)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   WordPiece
  </strong>
  is a subword segmentation algorithm used in natural language processing.  The vocabulary is initialized with individual characters in the language, then the most frequent combinations of symbols in the vocabulary are iteratively added to the vocabulary. The process is:
 </p>
 <ol>
  <li>
   Initialize the word unit inventory with all the characters in the text.
  </li>
  <li>
   Build a language model on the training data using the inventory from 1.
  </li>
  <li>
   Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.
  </li>
  <li>
   Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.
  </li>
 </ol>
 <p>
  Text:
  <a href="https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944">
   Source
  </a>
 </p>
 <p>
  Image: WordPiece as used in
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_12.57.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>SentencePiece (2018)  <span title=">500 papers"> &#9733;</span></summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SentencePiece
  </strong>
  is a subword tokenizer and detokenizer for natural language processing. It performs subword segmentation, supporting the byte-pair-encoding (
  <a href="https://paperswithcode.com/method/bpe">
   BPE
  </a>
  ) algorithm and unigram language model, and then converts this text into an id sequence guarantee perfect reproducibility of the normalization and subword segmentation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_1.12.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Topic Embeddings</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Contextualized Topic Models (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Contextualized Topic Models are based on the Neural-ProdLDA variational autoencoding approach by Srivastava and Sutton (2017).
 </p>
 <p>
  This approach trains an encoding neural network to map pre-trained contextualized word embeddings (e.g.,
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  ) to latent representations. Those latent representations are sampled variationally from a Gaussian distribution $N(\mu, \sigma^2)$ and passed to a decoder network that has to reconstruct the document bag-of-word representation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/logo_e0vXbiC.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>lda2vec (2016) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   lda2vec
  </strong>
  builds representations over both words and documents by mixing word2vec’s skipgram architecture with Dirichlet-optimized sparse topic mixtures.
 </p>
 <p>
  The Skipgram Negative-Sampling (SGNS) objective of word2vec is modified to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The total loss term $L$ is the sum of the Skipgram Negative Sampling Loss (SGNS) $L^{neg}_{ij}$ with the addition of a Dirichlet-likelihood term over document weights, $L_{d}$. The loss is conducted using a context vector, $\overrightarrow{c_{j}}$ , pivot word vector $\overrightarrow{w_{j}}$, target word vector $\overrightarrow{w_{i}}$, and negatively-sampled word vector $\overrightarrow{w_{l}}$:
 </p>
 <p>
  $$ L = L^{d} + \Sigma_{ij}L^{neg}_{ij} $$
 </p>
 <p>
  $$L^{neg}_{ij} = \log\sigma\left(c_{j}\cdot\overrightarrow{w_{i}}\right) + \sum^{n}_{l=0}\sigma\left(-\overrightarrow{c_{j}}\cdot\overrightarrow{w_{l}}\right)$$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.18.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Entity Recognition Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Partition Filter Network (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Partition Filter Network
  </strong>
  is a framework designed specifically for joint entity and relation extraction. The framework consists of three components: partition filter encoder, NER unit and RE unit. In task units, we use table-filling for word pair prediction. Orange, yellow and green represents NER-related, shared and RE-related component or features. (b) Detailed depiction of partition filter encoder in one single time step. We decompose feature encoding into two steps: partition and filter (shown in the gray area). In partition, we first segment neurons into two task partitions and one shared partition. Then in filter, partitions are selected and combined to form task-specific features and shared features, filtering out information irrelevant to each task.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.47.28_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Textual Meaning</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>SIRM (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Skim and Intensive Reading Model
  </strong>
  , or
  <strong>
   SIRM
  </strong>
  , is a deep neural network for figuring out implied textual meaning. It consists of two main components, namely the skim reading component and intensive reading component. N-gram features are quickly extracted from the skim reading component, which is a combination of several convolutional neural networks, as skim (entire) information. An intensive reading component enables a hierarchical investigation for both local (sentence) and global (paragraph) representation, which encapsulates the current embedding and the contextual information with a dense connection.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.34.12_AM_xt2D4sY.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Document Summary Evaluation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BLANC (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BLANC
  </strong>
  is an automatic estimation approach for document summary quality. The goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. BLANC achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document's text.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.16.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Conversational Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>SRS (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sticker Response Selector
  </strong>
  , or
  <strong>
   SRS
  </strong>
  , is a model for multi-turn dialog that automatically selects a sticker response. SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.31.26_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Meena (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Meena
  </strong>
  is a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. A seq2seq model is used with the Evolved
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  as the main architecture. The model is trained on multi-turn conversations where the input sequence is all turns of the context and the output sequence is the response.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.11.32_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Dialog Adaptation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>DAPO (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dialogue-Adaptive Pre-training Objective (DAPO)
  </strong>
  is a pre-training objective for dialogue adaptation, which is designed to measure qualities of dialogues from multiple important aspects, like Readability, Consistency and Fluency which have already been focused on by general LM pre-training objectives, and those also significant for assessing dialogues but ignored by general LM pre-training objectives, like Diversity and Specificity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_1.38.46_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Text Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MixText (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MixText
  </strong>
  is a semi-supervised learning method for text classification, which uses a new data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. The technique leverages advances in data augmentation to guess low-entropy labels for unlabeled data, making them as easy to use as labeled data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.02.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Augmented SBERT (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Augmented SBERT
  </strong>
  is a data augmentation strategy for pairwise sentence scoring that uses a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  cross-encoder to improve the performance for the
  <a href="https://paperswithcode.com/method/sbert">
   SBERT
  </a>
  bi-encoders. Given a pre-trained, well-performing crossencoder, we sample sentence pairs according to a certain sampling strategy and label these using the cross-encoder. We call these weakly labeled examples the silver dataset and they will be merged with the gold training dataset. We then train the bi-encoder on this extended training dataset.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ae0b6827-d70b-4af4-b6b9-6f5582b601ad.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Language Model Components</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Neural Cache (2016) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Neural Cache
  </strong>
  , or a
  <strong>
   Continuous Cache
  </strong>
  , is a module for language modelling which stores previous hidden states in memory cells. They are then used as keys to retrieve their corresponding word, that is the next word. There is no transformation applied to the storage during writing and reading.
 </p>
 <p>
  More formally it exploits the hidden representations $h_{t}$ to define a probability distribution over the words in the cache. As
illustrated in the Figure, the cache stores pairs $\left(h_{i}, x_{i+1}\right)$ of a hidden representation, and the word which was generated based on this representation (the vector $h_{i}$ encodes the history $x_{i}, \dots, x_{1}$). At time $t$, we then define a probability distribution over words stored in the cache based on the stored hidden representations and the current one $h_{t}$ as:
 </p>
 <p>
  $$ p_{cache}\left(w | h_{1\dots{t}}, x_{1\dots{t}}\right) \propto \sum^{t-1}_{i=1}\mathcal{1}_{\text{set}\left(w=x_{i+1}\right)} \exp\left(θ_{h}&gt;h_{t}^{T}h_{i}\right) $$
 </p>
 <p>
  where the scalar $\theta$ is a parameter which controls the flatness of the distribution. When $\theta$ is equal to zero, the probability distribution over the history is uniform, and the model is equivalent to a unigram cache model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-08-28_at_3.07.14_PM_8kAVYve.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Sequence Decoding Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Discriminative Adversarial Search (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Discriminative Adversarial Search
  </strong>
  , or
  <strong>
   DAS
  </strong>
  , is a sequence decoding approach which aims to alleviate the effects of exposure bias and to optimize on the data distribution itself rather than for external metrics. Inspired by generative adversarial networks (GANs), wherein a discriminator is used to improve the generator, DAS differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/15565d42-0ee8-4546-84ce-24b7f433b1f2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li class="method-importance-1">
            <details class="method">
            <summary>Vulnerability-constrained Decoding (2023) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Vulnerability-constrained Decoding
  </strong>
  , is a sequence decoding approach that aims to avoid generating vulnerabilities in generated code.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Input Embedding Factorization</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Adaptive Input Representations (2018) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Input Embeddings
  </strong>
  extend the
  <a href="https://paperswithcode.com/method/adaptive-softmax">
   adaptive softmax
  </a>
  to input word representations. The factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_1.34.39_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Reading Comprehension Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Deep LSTM Reader (2015) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Deep LSTM Reader
  </strong>
  is a neural network for reading comprehension. We feed documents one word at a time into a Deep
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  encoder, after a delimiter we then also feed the query into the encoder. The model therefore processes each document query pair as a single long sequence. Given the embedded document and query the network predicts which token in the document answers the query.
 </p>
 <p>
  The model consists of a Deep LSTM cell with skip connections from each input $x\left(t\right)$ to every hidden layer, and from every hidden layer to the output $y\left(t\right)$:
 </p>
 <p>
  $$x'\left(t, k\right) = x\left(t\right)||y'\left(t, k - 1\right) \text{,  } y\left(t\right) = y'\left(t, 1\right)|| \dots ||y'\left(t, K\right) $$
 </p>
 <p>
  $$ i\left(t, k\right) =  \left(W_{kxi}x'\left(t, k\right) + W_{khi}h(t - 1, k) + W_{kci}c\left(t - 1, k\right) + b_{ki}\right) $$
 </p>
 <p>
  $$ f\left(t, k\right) =  \left(W_{kxf}x\left(t\right) + W_{khf}h\left(t - 1, k\right) + W_{kcf}c\left(t - 1, k\right) + b_{kf}\right) $$
 </p>
 <p>
  $$ c\left(t, k\right) = f\left(t, k\right)c\left(t - 1, k\right) + i\left(t, k\right)\text{tanh}\left(W_{kxc}x'\left(t, k\right) + W_{khc}h\left(t -  1, k\right) + b_{kc}\right) $$
 </p>
 <p>
  $$ o\left(t, k\right) =  \left(W_{kxo}x'\left(t, k\right) + W_{kho}h\left(t - 1, k\right) + W_{kco}c\left(t, k\right) + b_{ko}\right) $$
 </p>
 <p>
  $$ h\left(t, k\right) = o\left(t, k\right)\text{tanh}\left(c\left(t, k\right)\right) $$
 </p>
 <p>
  $$ y'\left(t, k\right) = W_{kyh}\left(t, k\right) + b_{ky} $$
 </p>
 <p>
  where || indicates vector concatenation, $h\left(t, k\right)$ is the hidden state for layer $k$ at time $t$, and $i$, $f$, $o$ are the input, forget, and output gates respectively. Thus our Deep LSTM Reader is defined by $g^{\text{LSTM}}\left(d, q\right) = y\left(|d|+|q|\right)$ with input $x\left(t\right)$ the concatenation of $d$ and $q$ separated by the delimiter |||.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-19_at_6.10.33_PM_TbsJ3Qj.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Passage Re-Ranking Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>TILDEv2 (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TILDEv2
  </strong>
  is a
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  -based re-ranking method that stems from
  <a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462922">
   TILDE
  </a>
  but that addresses its limitations. It relies on contextualized exact term matching with expanded passages. This requires to only store in the index the score of tokens that appear in the expanded passages (rather than all the vocabulary), thus producing indexes that are 99% smaller than those of the original.
 </p>
 <p>
  Specifically, TILDE is modified in the following aspects:
 </p>
 <ul>
  <li>
   <p>
    <strong>
     Exact Term Matching
    </strong>
    . The query likelihood matching originally employed in TILDE, expands passages into the BERT vocabulary size, resulting in large indexes. To overcome this issue, estimating relevance scores is achieved with contextualized exact term matching. This allows the model to index tokens only present in the passage, thus reducing the index size. In addition to this, we replace the query likelihood loss function, with the Noise contrastive estimation (NCE) loss that allows to better leverage negative training samples.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Passage Expansion
    </strong>
    . To overcome the vocabulary mismatch problem that affects exact term matching methods, passage expansion is used to expand the original passage collection. Passages in the collection are expanded using deep LMs with a limited number of tokens. This requires TILDEv2 to only index a few extra tokens in addition to those in the original passages.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.35.46_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Exaggeration Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MT-PET (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MT-PET
  </strong>
  is a multi-task version of
  <a href="https://arxiv.org/abs/2001.07676">
   Pattern Exploiting Training
  </a>
  (PET) for exaggeration detection, which leverages knowledge from complementary cloze-style QA tasks to improve few-shot learning. It defines pairs of complementary pattern-verbalizer pairs for a main task and auxiliary task. These PVPs are then used to train PET on data from both tasks.
 </p>
 <p>
  PET uses the masked language modeling objective of pretrained language models to transform a task into one or more cloze-style question answering tasks.  In the original PET implementation, PVPs are defined for a single target task. MT-PET extends this by allowing for auxiliary PVPs from related tasks, adding complementary cloze-style QA tasks during training. The motivation for the multi-task approach is two-fold: 1) complementary cloze-style tasks can potentially help the model to learn different aspects of the main task, i.e. the similar tasks of exaggeration detection and claim strength prediction; 2) data on related tasks can be utilized during training, which is important in situations where data for the main task is limited.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.41.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Reading Order Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>LayoutReader (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LayoutReader
  </strong>
  is a sequence-to-sequence model for reading order detection that uses both textual and layout information, where the layout-aware language model
  <a href="https://paperswithcode.com/method/layoutlmv2">
   LayoutLM
  </a>
  is leveraged as an encoder. The generation step in the encoder-decoder structure tis modified to generate the reading order sequence.
 </p>
 <p>
  In the encoding stage, LayoutReader packs the pair of source and target segments into a contiguous input sequence of LayoutLM and carefully designs the
  <a href="https://paperswithcode.com/methods/category/factorized-attention">
   self-attention mask
  </a>
  to control the visibility between tokens. As shown in the Figure, LayoutReader allows the tokens in the source segment to attend to each other while preventing the tokens in the target segment from attending to the rightward context. If 1 means allowing and 0 means preventing, the detail of the mask $M$ is as follows:
 </p>
 <p>
  $$ M_{i, j}= \begin{cases}1, &amp; \text { if } i&lt;j \text { or } i, j \in \operatorname{src} \ 0, &amp; \text { otherwise }\end{cases} $$
 </p>
 <p>
  where $i, j$ are the indices in the packed input sequence, so they may be from source or target segments; $i, j \in$ src means both tokens are from source segment.
 </p>
 <p>
  In the decoding stage, since the source and target are reordered sequences, the prediction candidates can be constrained to the source segment. Therefore, we ask the model to predict the indices in the source sequence. The probability is calculated as follows:
 </p>
 <p>
  $$
\mathcal{P}\left(x_{k}=i \mid x_{&lt;k}\right)=\frac{\exp \left(e_{i}^{T} h_{k}+b_{k}\right)}{\sum_{j} \exp \left(e_{j}^{T} h_{k}+b_{k}\right)}
$$
 </p>
 <p>
  where $i$ is an index in the source segment; $e_{i}$ and $e_{j}$ are the $\mathrm{i}$-th and $\mathrm{j}$-th input embeddings of the source segment; $h_{k}$ is the hidden states at the $\mathrm{k}$-th time step; $b_{k}$ is the bias at the $\mathrm{k}$-th time step.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/82f68593-7263-40ea-91b1-1c8f48456c0b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Span Representations</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Packed Levitated Markers (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Packed Levitated Markers
  </strong>
  , or
  <strong>
   PL-Marker
  </strong>
  , is a span representation approach for
  <a href="https://paperswithcode.com/task/named-entity-recognition-ner">
   named entity recognition
  </a>
  that considers the dependencies between spans (pairs) by strategically packing the markers in the encoder. A pair of Levitated Markers, emphasizing a span, consists of a start marker and an end marker which share the same position embeddings with span’s start and end tokens respectively. In addition, both levitated markers adopt a restricted attention, that is, they are visible to each other, but not to the text token and other pairs of markers. sBased on the above features, the levitated marker would not affect the attended context of the original text tokens, which allows us to flexibly pack a series of related spans with their levitated markers in the encoding phase and thus model their dependencies.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.45.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Taxonomy Expansion Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>TaxoExpan (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TaxoExpan
  </strong>
  is a self-supervised taxonomy expansion framework. It automatically generates a set of &lt;query concept, anchor concept&gt; pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. TaxoExpan features: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.14.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Open-Domain Chatbots</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Meena (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Meena
  </strong>
  is a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. A seq2seq model is used with the Evolved
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  as the main architecture. The model is trained on multi-turn conversations where the input sequence is all turns of the context and the output sequence is the response.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.11.32_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Textual Inference Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>RAHP (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Review-guided Answer Helpfulness Prediction
  </strong>
  (RAHP) is a textual inference model for identifying helpful answers in e-commerce. It not only considers the interactions between QA pairs, but also investigates the opinion coherence between the answer and crowds' opinions reflected in the reviews, which is another important factor to identify helpful answers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.43.06_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Dialog System Evaluation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>ENIGMA (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ENIGMA
  </strong>
  is an evaluation framework for dialog systems based on Pearson and Spearman's rank correlations between the estimated rewards and the true rewards.  ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data (see details in Section 2), which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_9.50.13_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Inference Extrapolation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>ALiBi (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ALiBi
  </strong>
  , or
  <strong>
   Attention with Linear Biases
  </strong>
  , is a
  <a href="https://paperswithcode.com/methods/category/position-embeddings">
   positioning method
  </a>
  that allows
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  language models to consume, at inference time, sequences which are longer than the ones they were trained on.
 </p>
 <p>
  ALiBi does this without using actual position embeddings. Instead, computing the attention between a certain key and query, ALiBi penalizes the attention value that that query can assign to the key depending on how far away the key and query are. So when a key and query are close by, the penalty is very low, and when they are far away, the penalty is very high.
 </p>
 <p>
  This method was motivated by the simple reasoning that words that are close-by matter much more than ones that are  far away.
 </p>
 <p>
  This method is as fast as the sinusoidal or absolute embedding methods (the fastest positioning methods there are). It outperforms those methods and Rotary embeddings when evaluating sequences that are longer than the ones the model was trained on (this is known as extrapolation).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-31_at_9.34.28_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Copy Mechanisms</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>TopK Copy (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TopK Copy
  </strong>
  is a cross-attention guided copy mechanism for entity extraction where only the Top-$k$ important attention heads are used for computing copy distributions. The motivation is that that attention heads may not equally important, and that some heads can be pruned out with a marginal decrease in overall performance. Attention probabilities produced by insignificant attention heads may be noisy. Thus, computing copy distributions without these heads could improve the model’s ability to infer the importance of each token in the input document.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.51.12_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Entity Retrieval Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>MuVER (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-View Entity Representations
  </strong>
  , or
  <strong>
   MuVER
  </strong>
  , is an approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. It matches a mention to the appropriate entity by comparing it with entity descriptions. Motivated by the fact that mentions with different contexts correspond to different parts in descriptions, multi-view representations are constructed for each description. Specifically, we segment a description into several sentences. We refer to each sentence as a view $v$, which contains partial information, to form a view set $\mathcal{V}$ of the entity $e$. The Figure illustrates an example that constructs a view set $\mathcal{V}$ for “Kobe Bryant”.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.52.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Text Data Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>AEDA (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AEDA
  </strong>
  , or
  <strong>
   An Easier Data Augmentation
  </strong>
  , is a type of data augmentation technique for text classification which includes only the insertion of various punctuation marks into the input sequence. AEDA preserves all the input information and does not mislead the network since it keeps the word order intact while changing their positions in that the words are shifted to the right.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.57.56_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Paraphrase Generation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>BTmPG (2021) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BTmPG
  </strong>
  , or
  <strong>
   Back-Translation guided multi-round Paraphrase Generation
  </strong>
  , is a multi-round paraphrase generation method that leverages back-translation to guide paraphrase model during training and generates paraphrases in a multiround process. The model regards paraphrase generation as a monolingual translation task. Given a paraphrase pair $\left(S_{0}, P\right)$, which $S_{0}$ is the original/source sentence and $P$ is the target paraphrase given in the dataset. In the first round generation, we send $S_{0}$ into a paraphrase model to generate a paraphrase $S_{1}$. In the second round generation, we use the $S_{1}$ as the input of the model to generate a new paraphrase $S_{2}$. And so forth, in the $i$-th round generation, we send $S_{i−1}$ into the paraphrase model to generate $S_{i}$.
.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_1.56.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Sequence Editing Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>Seq2Edits (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Seq2Edits
  </strong>
  is an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. For text normalization, sentence fusion, sentence splitting &amp; rephrasing, text simplification, and grammatical error correction, the approach improves explainability by associating each edit operation with a human-readable tag.
 </p>
 <p>
  Rather than generating the target sentence as a series of tokens, the model predicts a sequence of edit operations that, when applied to the source sentence, yields the target sentence. Each edit operates on a span in the source sentence and either copies, deletes, or replaces it with one or more target tokens. Edits are generated auto-regressively from left to right using a modified
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture to facilitate learning of long-range dependencies.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/b472d8d4-c65b-4005-a946-b257e5732b5f.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Dialogue State Trackers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>FastSGT (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fast Schema Guided Tracker
  </strong>
  , or
  <strong>
   FastSGT
  </strong>
  , is a fast and robust
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  -based model for state tracking in goal-oriented dialogue systems. The model employs carry-over mechanisms for transferring the values between slots, enabling switching between services and accepting the values offered by the system during dialogue. It also uses
  <a href="https://paperswithcode.com/method/multi-head-attention">
   multi-head attention
  </a>
  projections in some of the decoders to have a better modelling of the encoder outputs.
 </p>
 <p>
  The model architecture is illustrated in the Figure. It consists of four main modules: 1-Utterance Encoder, 2-Schema Encoder, 3-State Decoder, and 4-State Tracker. The first three modules constitute the NLU component and are based on neural networks, whereas the state tracker is a rule-based module.
  <a href="https://paperswithcode.com/method/bert">
   BERT
  </a>
  was used for both encoders in the model.
 </p>
 <p>
  The Utterance Encoder is a BERT model which encodes the user and system utterances at each turn. The Schema Encoder is also a BERT model which encodes the schema descriptions of intents, slots, and values into schema embeddings. These schema embeddings help the decoders to transfer or share knowledge between different services by having some language understanding of each slot, intent, or value. The schema and utterance embeddings are passed to the State Decoder - a multi-task module. This module consists of five sub-modules producing the information necessary to track the state of the dialogue. Finally, the State Tracker module takes the previous state along with the current outputs of the State Decoder and predicts the current state of the dialogue by aggregating and summarizing the information across turns.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/dc2b3fbc-7fcb-45bc-9c6f-33cc90c20b01.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Document Understanding Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>LayoutLMv2 (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LayoutLMv2
  </strong>
  is an architecture and pre-training method for document understanding. The model is pre-trained with a great number of unlabeled scanned document images from the IIT-CDIP dataset, where some images in the text-image pairs are randomly replaced with another document image to make the model learn whether the image and OCR texts are correlated or not. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks.
 </p>
 <p>
  Specifically, an enhanced Transformer architecture is used, i.e. a multi-modal Transformer asisthe backbone of LayoutLMv2. The multi-modal Transformer accepts inputs of three modalities: text, image, and layout. The input of each modality is converted to an embedding sequence and fused by the encoder. The model establishes deep interactions within and between modalities by leveraging the powerful Transformer layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/fb35941e-31e2-49e3-b8c4-70e56d5e6222.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        
        <ul class="parent cat-importance-0">
            <p>Dependency Parsers</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <ul>
                
        <li class="method-importance-1">
            <details class="method">
            <summary>DDParser (2020) </summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DDParser
  </strong>
  , or
  <strong>
   Baidu Dependency Parser
  </strong>
  , is a Chinese dependency parser trained on a large-scale manually labeled dataset called Baidu Chinese Treebank (DuCTB).
 </p>
 <p>
  For inputs, for the $i$ th word, its input vector $e_{i}$ is the concatenation of the word embedding and character-level representation:
 </p>
 <p>
  $$
e_{i}=e_{i}^{w o r d} \oplus C h a r L S T M\left(w_{i}\right)
$$
 </p>
 <p>
  Where $\operatorname{CharLSTM}\left(w_{i}\right)$ is the output vectors after feeding the character sequence into a
  <a href="https://paperswithcode.com/method/bilstm">
   BiLSTM
  </a>
  layer. The experimental results on DuCTB dataset show that replacing POS tag embeddings with $\operatorname{CharLSTM}\left(w_{i}\right)$ leads to the improvement.
 </p>
 <p>
  For the BiLSTM encoder, three BiLSTM layers are employed over the input vectors for context encoding. Denote $r_{i}$ the output vector of the top-layer BiLSTM for $w_{i}$
 </p>
 <p>
  The dependency parser of
  <a href="https://arxiv.org/abs/1611.01734">
   Dozat and Manning
  </a>
  is used. Dimension-reducing MLPs are applied to each recurrent output vector $r_{i}$ before applying the biaffine transformation. Applying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision. Then biaffine attention is used both in the dependency arc classifier and relation classifier. The computations of all symbols in the Figure are shown below:
 </p>
 <p>
  $$
h_{i}^{d-a r c}=M L P^{d-a r c}\left(r_{i}\right)
$$
$$
h_{i}^{h-a r c}=M L P^{h-a r c}\left(r_{i}\right) \
$$
$$
h_{i}^{d-r e l}=M L P^{d-r e l}\left(r_{i}\right) \
$$
$$
h_{i}^{h-r e l}=M L P^{h-r e l}\left(r_{i}\right) \
$$
$$
S^{a r c}=\left(H^{d-a r c} \oplus I\right) U^{a r c} H^{h-a r c} \
$$
$$
S^{r e l}=\left(H^{d-r e l} \oplus I\right) U^{r e l}\left(\left(H^{h-r e l}\right)^{T} \oplus I\right)^{T}
$$
 </p>
 <p>
  For the decoder, the first-order Eisner algorithm is used to ensure that the output is a projection tree. Based on the dependency tree built by biaffine parser, we get a word sequence through the in-order traversal of the tree. The output is a projection tree only if the word sequence is in order.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/317fc17f-eab7-4afd-a885-3b55810398d2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
        </li>
    
            
        </ul>
        </div></body></html>