<!DOCTYPE html>
<html>

<head>
	<title>PWC Categories</title>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
	<link rel="stylesheet" href="styles.css">
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="script.js"></script>
</head>

<body>
	<div class="lightbox" id="lightbox" style="display: none">
		<img src="" id="lightbox-image" alt="Enlarged Image">
	</div>
	<div class="navbar">
		<a class="nav-btn" href="general.html">General</a>
		<a class="nav-btn" href="computer-vision.html">Computer Vision</a>
		<a class="nav-btn" href="natural-language-processing.html">NLP</a>
		<a class="nav-btn" href="reinforcement-learning.html">Reinforcement Learning</a>
		<a class="nav-btn" href="audio.html">Audio</a>
		<a class="nav-btn" href="sequential.html">Sequential</a>
		<a class="nav-btn" href="graphs.html">Graphs</a>
	</div>
	<div class="bottom-right-buttons">
        <button id="toggle-cats-btn" class="bottom-right-button">Toggle categories</button>
        <button id="close-methods-btn" class="bottom-right-button">Collapse methods</button>
    </div>
	<div class="container mx-auto">
        <ul class="parent">
            <p>1. Image Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Image Models
    </strong>
    are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Networks
  </strong>
  , or
  <strong>
   ResNets
  </strong>
  , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack
  <a href="https://paperswithcode.com/method/residual-block">
   residual blocks
  </a>
  ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original mapping is recast into $\mathcal{F}(x)+x$.
 </p>
 <p>
  There is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Vision Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Vision Transformer
  </strong>
  , or
  <strong>
   ViT
  </strong>
  , is a model for image classification that employs a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -like architecture over patches of the image.  An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VGG
  </strong>
  is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.
 </p>
 <p>
  Image:
  <a href="https://www.cs.toronto.edu/frossard/post/vgg16/">
   Davi Frossard
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/vgg_7mT4DML.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DenseNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   DenseNet
  </strong>
  is a type of convolutional neural network that utilises
  <a href="https://paperswithcode.com/method/dense-connections">
   dense connections
  </a>
  between layers, through
  <a href="http://www.paperswithcode.com/method/dense-block">
   Dense Blocks
  </a>
  , where we connect
  <em>
   all layers
  </em>
  (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.35.53_PM_KroVKVL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-16</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV2
  </strong>
  is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer with 32 filters, followed by 19 residual bottleneck layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.37.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Swin Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Swin Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  . It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_11.42.58_AM_jkrbxmo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AlexNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AlexNet
  </strong>
  is a classic convolutional neural network architecture. It consists of convolutions,
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-53
  </strong>
  is a convolutional neural network that acts as a backbone for the
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  object detection approach. The improvements upon its predecessor
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  include the use of residual connections, as well as more layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.53.56_PM_QQoF5AO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EfficientNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EfficientNet
  </strong>
  is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a
  <em>
   compound coefficient
  </em>
  . Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\alpha ^ N$,  width by $\beta ^ N$, and image size by $\gamma ^ N$, where $\alpha, \beta, \gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\phi$ to uniformly scales network width, depth, and resolution in a  principled way.
 </p>
 <p>
  The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
 </p>
 <p>
  The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of
  <a href="https://paperswithcode.com/method/mobilenetv2">
   MobileNetV2
  </a>
  , in addition to squeeze-and-excitation blocks.
 </p>
 <p>
  EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.45.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Xception</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Xception
  </strong>
  is a convolutional neural network architecture that relies solely on
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise separable convolution
  </a>
  layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Detr</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Detr
  </strong>
  , or
  <strong>
   Detection Transformer
  </strong>
  , is a set-based object detector using a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  on top of a convolutional backbone. It uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class
and bounding box) or a “no object” class.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-20_at_9.17.39_PM_ZHS2kmV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GoogLeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GoogLeNet
  </strong>
  is a type of convolutional neural network based on the
  <a href="https://paperswithcode.com/method/inception-module">
   Inception
  </a>
  architecture. It utilises Inception modules, which allow the network to choose between multiple convolutional filter sizes in each block. An Inception network stacks these modules on top of each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.28.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ResNeXt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   ResNeXt
  </strong>
  repeats a building block that aggregates a set of transformations with the same topology. Compared to a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  , it exposes a new dimension,
  <em>
   cardinality
  </em>
  (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width.
 </p>
 <p>
  Formally, a set of aggregated transformations can be represented as: $\mathcal{F}(x)=\sum_{i=1}^{C}\mathcal{T}_i(x)$, where $\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSPDarknet53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CSPDarknet53
  </strong>
  is a convolutional neural network and backbone for object detection that uses
  <a href="https://paperswithcode.com/method/darknet-53">
   DarkNet-53
  </a>
  . It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.
 </p>
 <p>
  This CNN is used as the backbone for
  <a href="https://paperswithcode.com/method/yolov4">
   YOLOv4
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_3.55.20_PM_fTGbeXg.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DINO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DINO
  </strong>
  (self-distillation with no labels) is a self-supervised learning method that directly predicts the output of a teacher network - built with a momentum encoder - using a standard cross-entropy loss.
 </p>
 <p>
  In the example to the right, DINO is illustrated in the case of one single pair of views $\left(x_{1}, x_{2}\right)$ for simplicity.
The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but other parameters.
The output of the teacher network is centered with a mean computed over the batch. Each network outputs a $K$ dimensional feature normalized with a temperature
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  over the feature dimension.
Their similarity is then measured with a cross-entropy loss.
A stop-gradient (sg) operator is applied to the teacher to propagate gradients only through the student.
The teacher parameters are updated with the student parameters' exponential moving average (ema).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.14.59_PM_fumVop5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inception-v3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Inception-v3
  </strong>
  is a convolutional neural network architecture from the Inception family that makes several improvements including using
  <a href="https://paperswithcode.com/method/label-smoothing">
   Label Smoothing
  </a>
  , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  for layers in the sidehead).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/inceptionv3onc--oview_vjAbOfw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SqueezeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SqueezeNet
  </strong>
  is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that "squeeze" parameters using 1x1 convolutions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DeiT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Data-Efficient Image Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  for image classification tasks. The model is trained using a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-12-24_at_1.16.17_PM_4ybxEGe.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV3
  </strong>
  is a convolutional neural network that is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the
  <a href="https://paperswithcode.com/method/netadapt">
   NetAdapt
  </a>
  algorithm, and then subsequently improved through novel architecture advances. Advances include (1) complementary search techniques, (2) new efficient versions of nonlinearities practical for the mobile setting, (3) new efficient network design.
 </p>
 <p>
  The network design includes the use of a
  <a href="https://paperswithcode.com/method/hard-swish">
   hard swish
  </a>
  activation and squeeze-and-excitation modules in the MBConv blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.03.15_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MLP-Mixer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   MLP-Mixer
  </strong>
  architecture (or “Mixer” for short) is an image architecture that doesn't use convolutions or self-attention. Instead, Mixer’s architecture is based entirely on multi-layer perceptrons (MLPs) that are repeatedly applied across either spatial locations or feature channels. Mixer relies only on basic matrix multiplication routines, changes to data layout (reshapes and transpositions), and scalar nonlinearities.
 </p>
 <p>
  It accepts a sequence of linearly projected image patches (also referred to as tokens) shaped as a “patches × channels” table as an input, and maintains this dimensionality. Mixer makes use of two types of MLP layers: channel-mixing MLPs and token-mixing MLPs. The channel-mixing MLPs allow communication between different channels; they operate on each token independently and take individual rows of the table as inputs. The token-mixing MLPs allow communication between different spatial locations (tokens); they operate on each channel independently and take individual columns of the table as inputs. These two types of layers are interleaved to enable interaction of both input dimensions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.09.16_PM_aLnxO7E.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV1</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNet
  </strong>
  is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_4.26.15_PM_ko4FqXD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HRNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HRNet
  </strong>
  , or
  <strong>
   High-Resolution Net
  </strong>
  , is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and
the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_11.03.39_PM_64dSeMm.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WideResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wide Residual Networks
  </strong>
  are a variant on
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.18.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-19
  </strong>
  is a convolutional neural network that is used as the backbone of
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  .  Similar to the
  <a href="https://paperswithcode.com/method/vgg">
   VGG
  </a>
  models it mostly uses $3 \times 3$ filters and doubles the number of channels after every pooling step. Following the work on Network in Network (NIN) it uses
  <a href="https://paperswithcode.com/method/global-average-pooling">
   global average pooling
  </a>
  to make predictions as well as $1 \times 1$ filters to compress the feature representation between $3 \times 3$ convolutions.
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch Normalization
  </a>
  is used to stabilize training, speed up convergence, and regularize the model batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.38.12_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Convolutional Neural Networks</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Convolutional Neural Networks
    </strong>
    are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/cnn.jpeg" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Networks
  </strong>
  , or
  <strong>
   ResNets
  </strong>
  , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack
  <a href="https://paperswithcode.com/method/residual-block">
   residual blocks
  </a>
  ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original mapping is recast into $\mathcal{F}(x)+x$.
 </p>
 <p>
  There is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VGG
  </strong>
  is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.
 </p>
 <p>
  Image:
  <a href="https://www.cs.toronto.edu/frossard/post/vgg16/">
   Davi Frossard
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/vgg_7mT4DML.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DenseNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   DenseNet
  </strong>
  is a type of convolutional neural network that utilises
  <a href="https://paperswithcode.com/method/dense-connections">
   dense connections
  </a>
  between layers, through
  <a href="http://www.paperswithcode.com/method/dense-block">
   Dense Blocks
  </a>
  , where we connect
  <em>
   all layers
  </em>
  (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.35.53_PM_KroVKVL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-16</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV2
  </strong>
  is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer with 32 filters, followed by 19 residual bottleneck layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.37.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AlexNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AlexNet
  </strong>
  is a classic convolutional neural network architecture. It consists of convolutions,
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-53
  </strong>
  is a convolutional neural network that acts as a backbone for the
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  object detection approach. The improvements upon its predecessor
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  include the use of residual connections, as well as more layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.53.56_PM_QQoF5AO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EfficientNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EfficientNet
  </strong>
  is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a
  <em>
   compound coefficient
  </em>
  . Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\alpha ^ N$,  width by $\beta ^ N$, and image size by $\gamma ^ N$, where $\alpha, \beta, \gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\phi$ to uniformly scales network width, depth, and resolution in a  principled way.
 </p>
 <p>
  The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
 </p>
 <p>
  The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of
  <a href="https://paperswithcode.com/method/mobilenetv2">
   MobileNetV2
  </a>
  , in addition to squeeze-and-excitation blocks.
 </p>
 <p>
  EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.45.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Xception</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Xception
  </strong>
  is a convolutional neural network architecture that relies solely on
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise separable convolution
  </a>
  layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ResNeXt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   ResNeXt
  </strong>
  repeats a building block that aggregates a set of transformations with the same topology. Compared to a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  , it exposes a new dimension,
  <em>
   cardinality
  </em>
  (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width.
 </p>
 <p>
  Formally, a set of aggregated transformations can be represented as: $\mathcal{F}(x)=\sum_{i=1}^{C}\mathcal{T}_i(x)$, where $\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GoogLeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GoogLeNet
  </strong>
  is a type of convolutional neural network based on the
  <a href="https://paperswithcode.com/method/inception-module">
   Inception
  </a>
  architecture. It utilises Inception modules, which allow the network to choose between multiple convolutional filter sizes in each block. An Inception network stacks these modules on top of each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.28.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSPDarknet53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CSPDarknet53
  </strong>
  is a convolutional neural network and backbone for object detection that uses
  <a href="https://paperswithcode.com/method/darknet-53">
   DarkNet-53
  </a>
  . It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.
 </p>
 <p>
  This CNN is used as the backbone for
  <a href="https://paperswithcode.com/method/yolov4">
   YOLOv4
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_3.55.20_PM_fTGbeXg.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inception-v3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Inception-v3
  </strong>
  is a convolutional neural network architecture from the Inception family that makes several improvements including using
  <a href="https://paperswithcode.com/method/label-smoothing">
   Label Smoothing
  </a>
  , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  for layers in the sidehead).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/inceptionv3onc--oview_vjAbOfw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SqueezeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SqueezeNet
  </strong>
  is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that "squeeze" parameters using 1x1 convolutions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV3
  </strong>
  is a convolutional neural network that is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the
  <a href="https://paperswithcode.com/method/netadapt">
   NetAdapt
  </a>
  algorithm, and then subsequently improved through novel architecture advances. Advances include (1) complementary search techniques, (2) new efficient versions of nonlinearities practical for the mobile setting, (3) new efficient network design.
 </p>
 <p>
  The network design includes the use of a
  <a href="https://paperswithcode.com/method/hard-swish">
   hard swish
  </a>
  activation and squeeze-and-excitation modules in the MBConv blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.03.15_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV1</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNet
  </strong>
  is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_4.26.15_PM_ko4FqXD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HRNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HRNet
  </strong>
  , or
  <strong>
   High-Resolution Net
  </strong>
  , is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and
the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_11.03.39_PM_64dSeMm.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WideResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wide Residual Networks
  </strong>
  are a variant on
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.18.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-19
  </strong>
  is a convolutional neural network that is used as the backbone of
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  .  Similar to the
  <a href="https://paperswithcode.com/method/vgg">
   VGG
  </a>
  models it mostly uses $3 \times 3$ filters and doubles the number of channels after every pooling step. Following the work on Network in Network (NIN) it uses
  <a href="https://paperswithcode.com/method/global-average-pooling">
   global average pooling
  </a>
  to make predictions as well as $1 \times 1$ filters to compress the feature representation between $3 \times 3$ convolutions.
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch Normalization
  </a>
  is used to stabilize training, speed up convergence, and regularize the model batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.38.12_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Vision Transformers</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Vision Transformers
    </strong>
    are
    <a href="https://www.paperswithcode.com/methods/category/transformers">
     Transformer
    </a>
    -like models applied to visual tasks.  They stem from the work of
    <a href="https://www.paperswithcode.com/method/vision-transformer">
     ViT
    </a>
    which directly applied a Transformer architecture on non-overlapping medium-sized image patches for image classification. Below you can find a continually updating list of vision transformers.
   </p>
   <p>
    According to [1], ViT type models can be further categorized into uniform scale ViTs, multi-scale ViT,  hybrid ViTs with convolutions, and self-supervised ViTs. The methods listed below provide a comprehensive overview of ViT models applied to a range of vision tasks.
   </p>
   <p>
    [1]
    <a href="https://arxiv.org/abs/2101.01169">
     Transformers in Vision: A Survey
    </a>
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/VIT.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Vision Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Vision Transformer
  </strong>
  , or
  <strong>
   ViT
  </strong>
  , is a model for image classification that employs a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -like architecture over patches of the image.  An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Swin Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Swin Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  . It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_11.42.58_AM_jkrbxmo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Detr</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Detr
  </strong>
  , or
  <strong>
   Detection Transformer
  </strong>
  , is a set-based object detector using a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  on top of a convolutional backbone. It uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class
and bounding box) or a “no object” class.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-20_at_9.17.39_PM_ZHS2kmV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DINO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DINO
  </strong>
  (self-distillation with no labels) is a self-supervised learning method that directly predicts the output of a teacher network - built with a momentum encoder - using a standard cross-entropy loss.
 </p>
 <p>
  In the example to the right, DINO is illustrated in the case of one single pair of views $\left(x_{1}, x_{2}\right)$ for simplicity.
The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but other parameters.
The output of the teacher network is centered with a mean computed over the batch. Each network outputs a $K$ dimensional feature normalized with a temperature
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  over the feature dimension.
Their similarity is then measured with a cross-entropy loss.
A stop-gradient (sg) operator is applied to the teacher to propagate gradients only through the student.
The teacher parameters are updated with the student parameters' exponential moving average (ema).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.14.59_PM_fumVop5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DeiT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Data-Efficient Image Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  for image classification tasks. The model is trained using a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-12-24_at_1.16.17_PM_4ybxEGe.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>2. Convolutional Neural Networks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Convolutional Neural Networks
    </strong>
    are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/cnn.jpeg" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Networks
  </strong>
  , or
  <strong>
   ResNets
  </strong>
  , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack
  <a href="https://paperswithcode.com/method/residual-block">
   residual blocks
  </a>
  ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=\mathcal{H}(x)-x$. The original mapping is recast into $\mathcal{F}(x)+x$.
 </p>
 <p>
  There is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VGG
  </strong>
  is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.
 </p>
 <p>
  Image:
  <a href="https://www.cs.toronto.edu/frossard/post/vgg16/">
   Davi Frossard
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/vgg_7mT4DML.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DenseNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   DenseNet
  </strong>
  is a type of convolutional neural network that utilises
  <a href="https://paperswithcode.com/method/dense-connections">
   dense connections
  </a>
  between layers, through
  <a href="http://www.paperswithcode.com/method/dense-block">
   Dense Blocks
  </a>
  , where we connect
  <em>
   all layers
  </em>
  (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.35.53_PM_KroVKVL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-16</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV2
  </strong>
  is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer with 32 filters, followed by 19 residual bottleneck layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.37.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AlexNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AlexNet
  </strong>
  is a classic convolutional neural network architecture. It consists of convolutions,
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-53
  </strong>
  is a convolutional neural network that acts as a backbone for the
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  object detection approach. The improvements upon its predecessor
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  include the use of residual connections, as well as more layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.53.56_PM_QQoF5AO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EfficientNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EfficientNet
  </strong>
  is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a
  <em>
   compound coefficient
  </em>
  . Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\alpha ^ N$,  width by $\beta ^ N$, and image size by $\gamma ^ N$, where $\alpha, \beta, \gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\phi$ to uniformly scales network width, depth, and resolution in a  principled way.
 </p>
 <p>
  The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.
 </p>
 <p>
  The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of
  <a href="https://paperswithcode.com/method/mobilenetv2">
   MobileNetV2
  </a>
  , in addition to squeeze-and-excitation blocks.
 </p>
 <p>
  EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.45.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Xception</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Xception
  </strong>
  is a convolutional neural network architecture that relies solely on
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise separable convolution
  </a>
  layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ResNeXt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   ResNeXt
  </strong>
  repeats a building block that aggregates a set of transformations with the same topology. Compared to a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  , it exposes a new dimension,
  <em>
   cardinality
  </em>
  (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width.
 </p>
 <p>
  Formally, a set of aggregated transformations can be represented as: $\mathcal{F}(x)=\sum_{i=1}^{C}\mathcal{T}_i(x)$, where $\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GoogLeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GoogLeNet
  </strong>
  is a type of convolutional neural network based on the
  <a href="https://paperswithcode.com/method/inception-module">
   Inception
  </a>
  architecture. It utilises Inception modules, which allow the network to choose between multiple convolutional filter sizes in each block. An Inception network stacks these modules on top of each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.28.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CSPDarknet53</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CSPDarknet53
  </strong>
  is a convolutional neural network and backbone for object detection that uses
  <a href="https://paperswithcode.com/method/darknet-53">
   DarkNet-53
  </a>
  . It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.
 </p>
 <p>
  This CNN is used as the backbone for
  <a href="https://paperswithcode.com/method/yolov4">
   YOLOv4
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_3.55.20_PM_fTGbeXg.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inception-v3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Inception-v3
  </strong>
  is a convolutional neural network architecture from the Inception family that makes several improvements including using
  <a href="https://paperswithcode.com/method/label-smoothing">
   Label Smoothing
  </a>
  , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  for layers in the sidehead).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/inceptionv3onc--oview_vjAbOfw.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SqueezeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SqueezeNet
  </strong>
  is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that "squeeze" parameters using 1x1 convolutions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VGG-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV3
  </strong>
  is a convolutional neural network that is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the
  <a href="https://paperswithcode.com/method/netadapt">
   NetAdapt
  </a>
  algorithm, and then subsequently improved through novel architecture advances. Advances include (1) complementary search techniques, (2) new efficient versions of nonlinearities practical for the mobile setting, (3) new efficient network design.
 </p>
 <p>
  The network design includes the use of a
  <a href="https://paperswithcode.com/method/hard-swish">
   hard swish
  </a>
  activation and squeeze-and-excitation modules in the MBConv blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.03.15_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV1</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNet
  </strong>
  is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_4.26.15_PM_ko4FqXD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HRNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HRNet
  </strong>
  , or
  <strong>
   High-Resolution Net
  </strong>
  , is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and
the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_11.03.39_PM_64dSeMm.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WideResNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wide Residual Networks
  </strong>
  are a variant on
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.18.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Darknet-19</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Darknet-19
  </strong>
  is a convolutional neural network that is used as the backbone of
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  .  Similar to the
  <a href="https://paperswithcode.com/method/vgg">
   VGG
  </a>
  models it mostly uses $3 \times 3$ filters and doubles the number of channels after every pooling step. Following the work on Network in Network (NIN) it uses
  <a href="https://paperswithcode.com/method/global-average-pooling">
   global average pooling
  </a>
  to make predictions as well as $1 \times 1$ filters to compress the feature representation between $3 \times 3$ convolutions.
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch Normalization
  </a>
  is used to stabilize training, speed up convergence, and regularize the model batch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.38.12_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>3. Generative Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Generative Models
    </strong>
    aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>AutoEncoder</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An
  <strong>
   Autoencoder
  </strong>
  is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).
 </p>
 <p>
  Image:
  <a href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png">
   Michael Massi
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Autoencoder_schema.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   GAN
  </strong>
  , or
  <strong>
   Generative Adversarial Network
  </strong>
  , is a generative model that simultaneously trains
two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the
probability that a sample came from the training data rather than $G$.
 </p>
 <p>
  The training procedure for $G$ is to maximize the probability of $D$ making
a mistake. This framework corresponds to a minimax two-player game. In the
space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$
recovering the training data distribution and $D$ equal to $\frac{1}{2}$
everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,
the entire system can be trained with backpropagation.
 </p>
 <p>
  (Image Source:
  <a href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/gan.jpeg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Variational Autoencoder
  </strong>
  is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_4.47.56_PM_Y06uCVO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CycleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CycleGAN
  </strong>
  , or
  <strong>
   Cycle-Consistent GAN
  </strong>
  , is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \rightarrow Y$ and $F: Y \rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a
  <a href="https://paperswithcode.com/method/cycle-consistency-loss">
   cycle consistency loss
  </a>
  that encourages $F\left(G\left(x\right)\right) \approx x$ and $G\left(F\left(y\right)\right) \approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.
 </p>
 <p>
  For the mapping $G : X \rightarrow Y$ and its discriminator $D_{Y}$ we have the objective:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) =\mathbb{E}_{y \sim p_{data}\left(y\right)}\left[\log D_{Y}\left(y\right)\right] + \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[log(1 − D_{Y}\left(G\left(x\right)\right)\right] $$
 </p>
 <p>
  where $G$ tries to generate images $G\left(x\right)$ that look similar to images from domain $Y$, while $D_{Y}$ tries to discriminate between translated samples $G\left(x\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \rightarrow X$ and its discriminator $D_{X}$.
 </p>
 <p>
  The Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:
 </p>
 <p>
  $$ \mathcal{L}_{cyc}\left(G, F\right) = \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[||F\left(G\left(x\right)\right) - x||_{1}\right] + \mathbb{E}_{y \sim p_{data}\left(y\right)}\left[||G\left(F\left(y\right)\right) - y||_{1}\right] $$
 </p>
 <p>
  The full objective is:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) = \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) + \mathcal{L}_{GAN}\left(F, D_{X}, X, Y\right) + \lambda\mathcal{L}_{cyc}\left(G, F\right) $$
 </p>
 <p>
  Where we aim to solve:
 </p>
 <p>
  $$ G^{*}, F^{*} = \arg \min_{G, F} \max_{D_{X}, D_{Y}} \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) $$
 </p>
 <p>
  For the original architecture the authors use:
 </p>
 <ul>
  <li>
   two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\frac{1}{2}$.
  </li>
  <li>
   <a href="https://paperswithcode.com/method/instance-normalization">
    instance normalization
   </a>
  </li>
  <li>
   PatchGANs for the discriminator
  </li>
  <li>
   Least Square Loss for the
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objectives.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_3.54.24_PM_aoT8JRU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN
  </strong>
  is a type of generative adversarial network. It uses an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature; in particular, the use of
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  . Otherwise it follows Progressive
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  in using a progressively growing training regime. Other quirks include the fact it generates from a fixed value tensor not stochastically generated latent variables as in regular GANs. The stochastically generated latent variables are used as style vectors in the adaptive
  <a href="https://paperswithcode.com/method/instance-normalization">
   instance normalization
  </a>
  at each resolution after being transformed by an 8-layer
  <a href="https://paperswithcode.com/method/feedforward-network">
   feedforward network
  </a>
  . Lastly, it employs a form of regularization called mixing regularization, which mixes two style latent variables during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_9.15.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN2
  </strong>
  is a generative adversarial network that builds on
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  with several improvements. First,
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  is redesigned and replaced with a normalization technique called
  <a href="https://paperswithcode.com/method/weight-demodulation">
   weight demodulation
  </a>
  . Secondly, an improved training scheme upon progressively growing is introduced, which achieves the same goal - training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions - without changing the network topology during training. Additionally, new types of regularization like lazy regularization and
  <a href="https://paperswithcode.com/method/path-length-regularization">
   path length regularization
  </a>
  are proposed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_9.55.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Denoising Autoencoder</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Denoising Autoencoder
  </strong>
  is a modification on the
  <a href="https://paperswithcode.com/method/autoencoder">
   autoencoder
  </a>
  to prevent the network learning the identity function. Specifically, if the autoencoder is too big, then it can just learn the data, so the output equals the input, and does not perform any useful representation learning or dimensionality reduction. Denoising autoencoders solve this problem by corrupting the input data on purpose, adding noise or masking some of the input values.
 </p>
 <p>
  Image Credit:
  <a href="https://www.semanticscholar.org/paper/Static-hand-gesture-recognition-using-stacked-Kumar-Nandi/5191ddf3f0841c89ba9ee592a2f6c33e4a40d4bf">
   Kumar et al
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Denoising-Autoencoder_qm5AOQM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VQ-VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VQ-VAE
  </strong>
  is a type of variational autoencoder that uses vector quantisation to obtain a discrete latent representation. It differs from
  <a href="https://paperswithcode.com/method/vae">
   VAEs
  </a>
  in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, ideas from vector quantisation (VQ) are incorporated. Using the VQ method allows the model to circumvent issues of posterior collapse - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.26.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Self-Attention Generative Adversarial Network
  </strong>
  , or
  <strong>
   SAGAN
  </strong>
  , allows for attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Restricted Boltzmann Machine</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Restricted Boltzmann Machines
  </strong>
  , or
  <strong>
   RBMs
  </strong>
  , are two-layer generative neural networks that learn a probability distribution over the inputs. They are a special class of Boltzmann Machine in that they have a restricted number of connections between visible and hidden units. Every node in the visible layer is connected to every node in the hidden layer, but no nodes in the same group are connected. RBMs are usually trained using the contrastive divergence learning procedure.
 </p>
 <p>
  Image Source:
  <a href="https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-1-introduction-model-formulation-1a4404873b3">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_Z-uEtQkFPk7MtbolOSUvrA_qoiHKUX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EBM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pix2Pix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pix2Pix
  </strong>
  is a conditional image-to-image translation architecture that uses a conditional
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  objective combined with a reconstruction loss. The conditional GAN objective for observed images $x$, output images $y$ and the random noise vector $z$ is:
 </p>
 <p>
  $$ \mathcal{L}_{cGAN}\left(G, D\right) =\mathbb{E}_{x,y}\left[\log D\left(x, y\right)\right]+
\mathbb{E}_{x,z}\left[log(1 − D\left(x, G\left(x, z\right)\right)\right] $$
 </p>
 <p>
  We augment this with a reconstruction term:
 </p>
 <p>
  $$ \mathcal{L}_{L1}\left(G\right) = \mathbb{E}_{x,y,z}\left[||y - G\left(x, z\right)||_{1}\right] $$
 </p>
 <p>
  and we get the final objective as:
 </p>
 <p>
  $$ G^{*} = \arg\min_{G}\max_{D}\mathcal{L}_{cGAN}\left(G, D\right) + \lambda\mathcal{L}_{L1}\left(G\right) $$
 </p>
 <p>
  The architectures employed for the generator and discriminator closely follow
  <a href="https://paperswithcode.com/method/dcgan">
   DCGAN
  </a>
  , with a few modifications:
 </p>
 <ul>
  <li>
   Concatenated skip connections are used to "shuttle" low-level information between the input and output, similar to a
   <a href="https://paperswithcode.com/method/u-net">
    U-Net
   </a>
   .
  </li>
  <li>
   The use of a
   <a href="https://paperswithcode.com/method/patchgan">
    PatchGAN
   </a>
   discriminator that only penalizes structure at the scale of patches.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_12.37.47_PM_dZrgNzj.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BigGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BigGAN
  </strong>
  is a type of generative adversarial network that was designed for scaling generation to high-resolution, high-fidelity images. It includes a number of incremental changes and innovations. The baseline and incremental changes are:
 </p>
 <ul>
  <li>
   Using
   <a href="https://paperswithcode.com/method/sagan">
    SAGAN
   </a>
   as a baseline with spectral norm. for G and D, and using
   <a href="https://paperswithcode.com/method/ttur">
    TTUR
   </a>
   .
  </li>
  <li>
   Using a Hinge Loss
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objective
  </li>
  <li>
   Using class-
   <a href="https://paperswithcode.com/method/conditional-batch-normalization">
    conditional batch normalization
   </a>
   to provide class information to G (but with linear projection not MLP.
  </li>
  <li>
   Using a
   <a href="https://paperswithcode.com/method/projection-discriminator">
    projection discriminator
   </a>
   for D to provide class information to D.
  </li>
  <li>
   Evaluating with EWMA of G's weights, similar to ProGANs.
  </li>
 </ul>
 <p>
  The innovations are:
 </p>
 <ul>
  <li>
   Increasing batch sizes, which has a big effect on the Inception Score of the model.
  </li>
  <li>
   Increasing the width in each layer leads to a further Inception Score improvement.
  </li>
  <li>
   Adding skip connections from the latent variable $z$ to further layers helps performance.
  </li>
  <li>
   A new variant of
   <a href="https://paperswithcode.com/method/orthogonal-regularization">
    Orthogonal Regularization
   </a>
   .
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.41.08_PM_cKfKr80.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wasserstein GAN
  </strong>
  , or
  <strong>
   WGAN
  </strong>
  , is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_2.53.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>cVAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DCGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DCGAN
  </strong>
  , or
  <strong>
   Deep Convolutional GAN
  </strong>
  , is a generative adversarial network architecture. It uses a couple of guidelines, in particular:
 </p>
 <ul>
  <li>
   Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).
  </li>
  <li>
   Using batchnorm in both the generator and the discriminator.
  </li>
  <li>
   Removing fully connected hidden layers for deeper architectures.
  </li>
  <li>
   Using
   <a href="https://paperswithcode.com/method/relu">
    ReLU
   </a>
   activation in generator for all layers except for the output, which uses tanh.
  </li>
  <li>
   Using LeakyReLU activation in the discriminator for all layer.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.27.51_PM_IoGbo1i.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deep Belief Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Deep Belief Network (DBN)
  </strong>
  is a multi-layer generative graphical model. DBNs have bi-directional connections (
  <a href="https://paperswithcode.com/method/restricted-boltzmann-machine">
   RBM
  </a>
  -type connections) on the top layer while the bottom layers only have top-down connections. They are trained using layerwise pre-training. Pre-training occurs by training the network component by component bottom up: treating the first two layers as an RBM and training, then treating the second layer and third layer as another RBM and training for those parameters.
 </p>
 <p>
  Source:
  <a href="https://arxiv.org/pdf/1702.07800.pdf">
   Origins of Deep Learning
  </a>
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Deep_belief_network">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_2.54.43_PM_EwgIrIu.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Generative Adversarial Networks</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Generative Adversarial Networks (GANs)
    </strong>
    are a type of generative model that use two networks, a generator to generate images and a discriminator to discriminate between real and fake, to train a model that approximates the distribution of the data. Below you can find a continuously updating list of GANs.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   GAN
  </strong>
  , or
  <strong>
   Generative Adversarial Network
  </strong>
  , is a generative model that simultaneously trains
two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the
probability that a sample came from the training data rather than $G$.
 </p>
 <p>
  The training procedure for $G$ is to maximize the probability of $D$ making
a mistake. This framework corresponds to a minimax two-player game. In the
space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$
recovering the training data distribution and $D$ equal to $\frac{1}{2}$
everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,
the entire system can be trained with backpropagation.
 </p>
 <p>
  (Image Source:
  <a href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/gan.jpeg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CycleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CycleGAN
  </strong>
  , or
  <strong>
   Cycle-Consistent GAN
  </strong>
  , is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \rightarrow Y$ and $F: Y \rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a
  <a href="https://paperswithcode.com/method/cycle-consistency-loss">
   cycle consistency loss
  </a>
  that encourages $F\left(G\left(x\right)\right) \approx x$ and $G\left(F\left(y\right)\right) \approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.
 </p>
 <p>
  For the mapping $G : X \rightarrow Y$ and its discriminator $D_{Y}$ we have the objective:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) =\mathbb{E}_{y \sim p_{data}\left(y\right)}\left[\log D_{Y}\left(y\right)\right] + \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[log(1 − D_{Y}\left(G\left(x\right)\right)\right] $$
 </p>
 <p>
  where $G$ tries to generate images $G\left(x\right)$ that look similar to images from domain $Y$, while $D_{Y}$ tries to discriminate between translated samples $G\left(x\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \rightarrow X$ and its discriminator $D_{X}$.
 </p>
 <p>
  The Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:
 </p>
 <p>
  $$ \mathcal{L}_{cyc}\left(G, F\right) = \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[||F\left(G\left(x\right)\right) - x||_{1}\right] + \mathbb{E}_{y \sim p_{data}\left(y\right)}\left[||G\left(F\left(y\right)\right) - y||_{1}\right] $$
 </p>
 <p>
  The full objective is:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) = \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) + \mathcal{L}_{GAN}\left(F, D_{X}, X, Y\right) + \lambda\mathcal{L}_{cyc}\left(G, F\right) $$
 </p>
 <p>
  Where we aim to solve:
 </p>
 <p>
  $$ G^{*}, F^{*} = \arg \min_{G, F} \max_{D_{X}, D_{Y}} \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) $$
 </p>
 <p>
  For the original architecture the authors use:
 </p>
 <ul>
  <li>
   two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\frac{1}{2}$.
  </li>
  <li>
   <a href="https://paperswithcode.com/method/instance-normalization">
    instance normalization
   </a>
  </li>
  <li>
   PatchGANs for the discriminator
  </li>
  <li>
   Least Square Loss for the
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objectives.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_3.54.24_PM_aoT8JRU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN
  </strong>
  is a type of generative adversarial network. It uses an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature; in particular, the use of
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  . Otherwise it follows Progressive
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  in using a progressively growing training regime. Other quirks include the fact it generates from a fixed value tensor not stochastically generated latent variables as in regular GANs. The stochastically generated latent variables are used as style vectors in the adaptive
  <a href="https://paperswithcode.com/method/instance-normalization">
   instance normalization
  </a>
  at each resolution after being transformed by an 8-layer
  <a href="https://paperswithcode.com/method/feedforward-network">
   feedforward network
  </a>
  . Lastly, it employs a form of regularization called mixing regularization, which mixes two style latent variables during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_9.15.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN2
  </strong>
  is a generative adversarial network that builds on
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  with several improvements. First,
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  is redesigned and replaced with a normalization technique called
  <a href="https://paperswithcode.com/method/weight-demodulation">
   weight demodulation
  </a>
  . Secondly, an improved training scheme upon progressively growing is introduced, which achieves the same goal - training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions - without changing the network topology during training. Additionally, new types of regularization like lazy regularization and
  <a href="https://paperswithcode.com/method/path-length-regularization">
   path length regularization
  </a>
  are proposed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_9.55.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Self-Attention Generative Adversarial Network
  </strong>
  , or
  <strong>
   SAGAN
  </strong>
  , allows for attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pix2Pix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pix2Pix
  </strong>
  is a conditional image-to-image translation architecture that uses a conditional
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  objective combined with a reconstruction loss. The conditional GAN objective for observed images $x$, output images $y$ and the random noise vector $z$ is:
 </p>
 <p>
  $$ \mathcal{L}_{cGAN}\left(G, D\right) =\mathbb{E}_{x,y}\left[\log D\left(x, y\right)\right]+
\mathbb{E}_{x,z}\left[log(1 − D\left(x, G\left(x, z\right)\right)\right] $$
 </p>
 <p>
  We augment this with a reconstruction term:
 </p>
 <p>
  $$ \mathcal{L}_{L1}\left(G\right) = \mathbb{E}_{x,y,z}\left[||y - G\left(x, z\right)||_{1}\right] $$
 </p>
 <p>
  and we get the final objective as:
 </p>
 <p>
  $$ G^{*} = \arg\min_{G}\max_{D}\mathcal{L}_{cGAN}\left(G, D\right) + \lambda\mathcal{L}_{L1}\left(G\right) $$
 </p>
 <p>
  The architectures employed for the generator and discriminator closely follow
  <a href="https://paperswithcode.com/method/dcgan">
   DCGAN
  </a>
  , with a few modifications:
 </p>
 <ul>
  <li>
   Concatenated skip connections are used to "shuttle" low-level information between the input and output, similar to a
   <a href="https://paperswithcode.com/method/u-net">
    U-Net
   </a>
   .
  </li>
  <li>
   The use of a
   <a href="https://paperswithcode.com/method/patchgan">
    PatchGAN
   </a>
   discriminator that only penalizes structure at the scale of patches.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_12.37.47_PM_dZrgNzj.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BigGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BigGAN
  </strong>
  is a type of generative adversarial network that was designed for scaling generation to high-resolution, high-fidelity images. It includes a number of incremental changes and innovations. The baseline and incremental changes are:
 </p>
 <ul>
  <li>
   Using
   <a href="https://paperswithcode.com/method/sagan">
    SAGAN
   </a>
   as a baseline with spectral norm. for G and D, and using
   <a href="https://paperswithcode.com/method/ttur">
    TTUR
   </a>
   .
  </li>
  <li>
   Using a Hinge Loss
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objective
  </li>
  <li>
   Using class-
   <a href="https://paperswithcode.com/method/conditional-batch-normalization">
    conditional batch normalization
   </a>
   to provide class information to G (but with linear projection not MLP.
  </li>
  <li>
   Using a
   <a href="https://paperswithcode.com/method/projection-discriminator">
    projection discriminator
   </a>
   for D to provide class information to D.
  </li>
  <li>
   Evaluating with EWMA of G's weights, similar to ProGANs.
  </li>
 </ul>
 <p>
  The innovations are:
 </p>
 <ul>
  <li>
   Increasing batch sizes, which has a big effect on the Inception Score of the model.
  </li>
  <li>
   Increasing the width in each layer leads to a further Inception Score improvement.
  </li>
  <li>
   Adding skip connections from the latent variable $z$ to further layers helps performance.
  </li>
  <li>
   A new variant of
   <a href="https://paperswithcode.com/method/orthogonal-regularization">
    Orthogonal Regularization
   </a>
   .
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.41.08_PM_cKfKr80.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wasserstein GAN
  </strong>
  , or
  <strong>
   WGAN
  </strong>
  , is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_2.53.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DCGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DCGAN
  </strong>
  , or
  <strong>
   Deep Convolutional GAN
  </strong>
  , is a generative adversarial network architecture. It uses a couple of guidelines, in particular:
 </p>
 <ul>
  <li>
   Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).
  </li>
  <li>
   Using batchnorm in both the generator and the discriminator.
  </li>
  <li>
   Removing fully connected hidden layers for deeper architectures.
  </li>
  <li>
   Using
   <a href="https://paperswithcode.com/method/relu">
    ReLU
   </a>
   activation in generator for all layers except for the output, which uses tanh.
  </li>
  <li>
   Using LeakyReLU activation in the discriminator for all layer.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.27.51_PM_IoGbo1i.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Generative Training</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Denoising Score Matching</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Training a denoiser on signals gives you a powerful prior over this signal that you can then use to sample examples of this signal.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Informative Sample Mining Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Informative Sample Mining Network
  </strong>
  is a multi-stage sample training scheme for GANs to reduce sample hardness while preserving sample informativeness. Adversarial Importance Weighting is proposed to select informative samples and assign them greater weight. The authors also propose Multi-hop Sample Training to avoid the potential problems in model training caused by sample mining. Based on the principle of divide-and-conquer, the authors produce target images by multiple hops, which means the image translation is decomposed into several separated steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.55.31_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ILVR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Iterative Latent Variable Refinement
  </strong>
  , or
  <strong>
   ILVR
  </strong>
  , is a method to guide the generative process in denoising diffusion probabilistic models (DDPMs) to generate high-quality images based on a given reference image. ILVR conditions the generation process in well-performing unconditional DDPM. Each transition in the generation process is refined utilizing a given reference image. By matching each latent variable, ILVR ensures the given condition in each transition thus enables sampling from a conditional distribution. Thus, ILVR generates high-quality images sharing desired semantics.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.50.18_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Safety-llamas</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Likelihood-Based Generative Models</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Likelihood-based Generative Models
    </strong>
    are a class of generative model that model the distribution of the data $p\left(y\right)$ directly with a likelihood function. The most popular subclass of likelihood-based generative models is variational autoencoders.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Variational Autoencoder
  </strong>
  is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_4.47.56_PM_Y06uCVO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VQ-VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VQ-VAE
  </strong>
  is a type of variational autoencoder that uses vector quantisation to obtain a discrete latent representation. It differs from
  <a href="https://paperswithcode.com/method/vae">
   VAEs
  </a>
  in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, ideas from vector quantisation (VQ) are incorporated. Using the VQ method allows the model to circumvent issues of posterior collapse - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.26.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GLOW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GLOW
  </strong>
  is a type of flow-based generative model that is based on an invertible $1 \times 1$
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . This builds on the flows introduced by
  <a href="https://paperswithcode.com/method/nice">
   NICE
  </a>
  and
  <a href="https://paperswithcode.com/method/realnvp">
   RealNVP
  </a>
  . It consists of a series of steps of flow, combined in a multi-scale architecture; see the Figure to the right. Each step of flow consists of Act Normalization followed by an
  <em>
   invertible $1 \times 1$ convolution
  </em>
  followed by an
  <a href="https://paperswithcode.com/method/affine-coupling">
   affine coupling
  </a>
  layer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_8.43.24_PM_tNckkOB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PixelCNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   PixelCNN
  </strong>
  is a generative model that uses autoregressive connections to model images pixel by pixel, decomposing the joint image distribution as a product of conditionals. PixelCNNs are much faster to train than
  <a href="https://paperswithcode.com/method/pixelrnn">
   PixelRNNs
  </a>
  because convolutions are inherently easier to parallelize; given the vast number of pixels present in large image datasets this is an important advantage.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-16_at_7.27.51_PM_tpsd8Td.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>4. Image Model Blocks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Image Model Blocks
    </strong>
    are building blocks used in image models such as convolutional neural networks. Below you can find a continuously updating list of image model blocks.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Residual Blocks
  </strong>
  are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  architecture.
 </p>
 <p>
  Formally, denoting the desired underlying mapping as $\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\mathcal{F}({x}):=\mathcal{H}({x})-{x}$. The original mapping is recast into $\mathcal{F}({x})+{x}$. The $\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.
 </p>
 <p>
  The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.
 </p>
 <p>
  Note that in practice,
  <a href="https://paperswithcode.com/method/bottleneck-residual-block">
   Bottleneck Residual Blocks
  </a>
  are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/resnet-e1548261477164_2_mD02h5A.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bottleneck Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Bottleneck Residual Block
  </strong>
  is a variant of the
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.12.02_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dense Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Dense Block
  </strong>
  is a module used in convolutional neural networks that connects
  <em>
   all layers
  </em>
  (with matching feature-map sizes) directly with each other. It was originally proposed as part of the
  <a href="https://paperswithcode.com/method/densenet">
   DenseNet
  </a>
  architecture. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. In contrast to
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  , we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the $\ell^{th}$ layer has $\ell$ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all $L-\ell$ subsequent layers. This introduces $\frac{L(L+1)}{2}$  connections in an $L$-layer network, instead of just $L$, as in traditional architectures: "dense connectivity".
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.33.17_PM_Mt0HOZL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Squeeze-and-Excitation Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Squeeze-and-Excitation Block
  </strong>
  is an architectural unit designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature recalibration. The process is:
 </p>
 <ul>
  <li>
   The block has a convolutional block as an input.
  </li>
  <li>
   Each channel is "squeezed" into a single numeric value using
   <a href="https://paperswithcode.com/method/average-pooling">
    average pooling
   </a>
   .
  </li>
  <li>
   A dense layer followed by a
   <a href="https://paperswithcode.com/method/relu">
    ReLU
   </a>
   adds non-linearity and output channel complexity is reduced by a ratio.
  </li>
  <li>
   Another dense layer followed by a sigmoid gives each channel a smooth gating function.
  </li>
  <li>
   Finally, we weight each feature map of the convolutional block based on the side network; the "excitation".
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.55.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inception Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  An
  <strong>
   Inception Module
  </strong>
  is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.22.39_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Spatial Attention Module
  </strong>
  is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the
  <a href="https://paperswithcode.com/method/channel-attention-module">
   channel attention
  </a>
  , the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer to generate a spatial attention map $\textbf{M}_{s}\left(F\right) \in \mathcal{R}^{H×W}$ which encodes where to emphasize or suppress.
 </p>
 <p>
  We aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\mathbf{F}^{s}_{avg} \in \mathbb{R}^{1\times{H}\times{W}}$ and $\mathbf{F}^{s}_{max} \in \mathbb{R}^{1\times{H}\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\text{AvgPool}\left(F\right);\text{MaxPool}\left(F\right)\right]\right)\right) $$
 </p>
 <p>
  $$ \textbf{M}_{s}\left(F\right) = \sigma\left(f^{7x7}\left(\left[\mathbf{F}^{s}_{avg};\mathbf{F}^{s}_{max} \right]\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function and $f^{7×7}$ represents a convolution operation with the filter size of 7 × 7.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Non-Local Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Non-Local Block
  </strong>
  is an image block module used in neural networks that wraps a
  <a href="https://paperswithcode.com/method/non-local-operation">
   non-local operation
  </a>
  . We can define a non-local block as:
 </p>
 <p>
  $$ \mathbb{z}_{i} = W_{z}\mathbb{y_{i}} + \mathbb{x}_{i} $$
 </p>
 <p>
  where $y_{i}$ is the output from the non-local operation and $+ \mathbb{x}_{i}$ is a
  <a href="https://paperswithcode.com/method/residual-connection">
   residual connection
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_5.20.01_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Spatial Transformer
  </strong>
  is an image model block that explicitly allows the spatial manipulation of data within a
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   convolutional neural network
  </a>
  . It gives CNNs the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. Unlike pooling layers, where the receptive fields are fixed and local, the spatial transformer module is a dynamic mechanism that can actively spatially transform an image (or a feature map) by producing an appropriate transformation for each input sample. The transformation is then performed on the entire feature map (non-locally) and can include scaling, cropping, rotations, as well as non-rigid deformations.
 </p>
 <p>
  The architecture is shown in the Figure to the right. The input feature map $U$ is passed to a localisation network which regresses the transformation parameters $\theta$. The regular spatial grid $G$ over $V$ is transformed to the sampling grid $T_{\theta}\left(G\right)$, which is applied to $U$, producing the warped output feature map $V$. The combination of the localisation network and sampling mechanism defines a spatial transformer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-19_at_5.48.34_PM_vFLk7jR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ResNeXt Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   ResNeXt Block
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  used as part of the
  <a href="https://paperswithcode.com/method/resnext">
   ResNeXt
  </a>
  CNN architecture. It uses a "split-transform-merge" strategy (branched paths within a single module) similar to an
  <a href="https://paperswithcode.com/method/inception-module">
   Inception module
  </a>
  , i.e. it aggregates a set of transformations. Compared to a Residual Block, it exposes a new dimension,
  <em>
   cardinality
  </em>
  (size of set of transformations) $C$, as an essential factor in addition to depth and width.
 </p>
 <p>
  Formally, a set of aggregated transformations can be represented as: $\mathcal{F}(x)=\sum_{i=1}^{C}\mathcal{T}_i(x)$, where $\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM_iXtkYE5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Fire Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Fire Module
  </strong>
  is a building block for convolutional neural networks, notably used as part of
  <a href="https://paperswithcode.com/method/squeezenet">
   SqueezeNet
  </a>
  . A Fire module is comprised of: a squeeze
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters.  We expose three tunable dimensions (hyperparameters) in a Fire module: $s_{1x1}$, $e_{1x1}$, and $e_{3x3}$. In a Fire module, $s_{1x1}$ is the number of filters in the squeeze layer (all 1x1), $e_{1x1}$ is the number of 1x1 filters in the expand layer, and $e_{3x3}$ is the number of 3x3 filters in the expand layer. When we use Fire modules we set $s_{1x1}$ to be less than ($e_{1x1}$ + $e_{3x3}$), so the squeeze layer helps to limit the number of input channels to the 3x3 filters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_5.51.48_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Inception-v3 Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Inception-v3 Module
  </strong>
  is an image block used in the
  <a href="https://paperswithcode.com/method/inception-v3">
   Inception-v3
  </a>
  architecture. This architecture is used on the coarsest (8 × 8) grids to promote high dimensional representations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-12_at_9.29.10M_zj6BV6l_fs8hkpx.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PnP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PnP
  </strong>
  , or
  <strong>
   Poll and Pool
  </strong>
  , is sampling module extension for
  <a href="https://paperswithcode.com/method/detr">
   DETR
  </a>
  -type architectures that adaptively allocates its computation spatially to be more efficient. Concretely, the PnP module abstracts the image feature map into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. The
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  models information interaction within the fine-coarse feature space and translates the features into the detection result.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.37.00_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Channel Attention Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Channel Attention Module
  </strong>
  is a module for channel-based attention in convolutional neural networks. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map.
 </p>
 <p>
  We first aggregate spatial information of a feature map by using both average-pooling and max-pooling operations, generating two different spatial context descriptors: $\mathbf{F}^{c}_{avg}$ and $\mathbf{F}^{c}_{max}$, which denote average-pooled features and max-pooled features respectively.
 </p>
 <p>
  Both descriptors are then forwarded to a shared network to produce our channel attention map $\mathbf{M}_{c} \in \mathbb{R}^{C\times{1}\times{1}}$. Here $C$ is the number of channels. The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $\mathbb{R}^{C/r×1×1}$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, we merge the output feature vectors using element-wise summation. In short, the channel attention is computed as:
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\text{MLP}\left(\text{AvgPool}\left(\mathbf{F}\right)\right)+\text{MLP}\left(\text{MaxPool}\left(\mathbf{F}\right)\right)\right) $$
 </p>
 <p>
  $$  \mathbf{M_{c}}\left(\mathbf{F}\right) = \sigma\left(\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{avg}\right)\right) +\mathbf{W_{1}}\left(\mathbf{W_{0}}\left(\mathbf{F}^{c}_{max}\right)\right)\right) $$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function, $\mathbf{W}_{0} \in \mathbb{R}^{C/r\times{C}}$, and $\mathbf{W}_{1} \in \mathbb{R}^{C\times{C/r}}$. Note that the MLP weights, $\mathbf{W}_{0}$ and $\mathbf{W}_{1}$, are shared for both inputs and the
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  activation function is followed by $\mathbf{W}_{0}$.
 </p>
 <p>
  Note that the channel attention module with just
  <a href="https://paperswithcode.com/method/average-pooling">
   average pooling
  </a>
  is the same as the
  <a href="https://paperswithcode.com/method/squeeze-and-excitation-block">
   Squeeze-and-Excitation Module
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.21_PM_YDoPGUi.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Wide Residual Block</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Wide Residual Block
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  that utilises two conv 3x3 layers (with
  <a href="https://paperswithcode.com/method/dropout">
   dropout
  </a>
  ). This is wider than other variants of residual blocks (for instance
  <a href="https://paperswithcode.com/method/bottleneck-residual-block">
   bottleneck residual blocks
  </a>
  ). It was proposed as part of the
  <a href="https://paperswithcode.com/method/wideresnet">
   WideResNet
  </a>
  CNN architecture.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_2.18.54_PM_cxmFnC3.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Axial Attention</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Axial Attention
  </strong>
  is a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. It was first proposed in
  <a href="https://paperswithcode.com/method/ccnet">
   CCNet
  </a>
  [1] named as criss-cross attention, which harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Ho et al [2] extents CCNet to process multi-dimensional data.  The proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. It serves as the basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors, e.g., Axial Transformers. It has been applied in
  <a href="https://paperswithcode.com/method/alphafold">
   AlphaFold
  </a>
  [3] for interpreting protein sequences.
 </p>
 <p>
  [1] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu. CCNet: Criss-Cross Attention for Semantic Segmentation. ICCV, 2019.
 </p>
 <p>
  [2] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans. arXiv:1912.12180
 </p>
 <p>
  [3] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A, Bridgland A. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Jul 15:1-1.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/cca_ANy2LjX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>5. Object Detection Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Object Detection Models
    </strong>
    are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/new_method_Rt2vgvr.jpg" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Faster R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Faster R-CNN
  </strong>
  is an object detection model that improves on
  <a href="https://paperswithcode.com/method/fast-r-cnn">
   Fast R-CNN
  </a>
  by utilising a region proposal network (
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  ) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by
  <a href="https://paperswithcode.com/method/fast-r-cnn">
   Fast R-CNN
  </a>
  for detection. RPN and Fast
  <a href="https://paperswithcode.com/method/r-cnn">
   R-CNN
  </a>
  are merged into a single network by sharing their convolutional features: the RPN component tells the unified network where to look.
 </p>
 <p>
  As a whole, Faster R-CNN consists of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_5.10.31_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Mask R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mask R-CNN
  </strong>
  extends
  <a href="https://paperswithcode.com/method/faster-r-cnn">
   Faster R-CNN
  </a>
  to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster
  <a href="https://paperswithcode.com/method/r-cnn">
   R-CNN
  </a>
  , but constructing the mask branch properly is critical for good results.
 </p>
 <p>
  Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is evident in how
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  , the
  <em>
   de facto
  </em>
  core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, Mask R-CNN utilises a simple, quantization-free layer, called
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , that faithfully preserves exact spatial locations.
 </p>
 <p>
  Secondly, Mask R-CNN
  <em>
   decouples
  </em>
  mask and class prediction: it predicts a binary mask for each class independently, without competition among classes, and relies on the network's RoI classification branch to predict the category. In contrast, an
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  usually perform per-pixel multi-class categorization, which couples segmentation and classification.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.44.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SSD
  </strong>
  is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.
 </p>
 <p>
  The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-27_at_1.59.27_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv3
  </strong>
  is a real-time, single-stage object detection model that builds on
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  with several improvements. Improvements include the use of a new backbone network,
  <a href="https://paperswithcode.com/method/darknet-53">
   Darknet-53
  </a>
  that utilises residual connections, or in the words of the author, "those newfangled residual network stuff", as well as some improvements to the bounding box prediction step, and use of three different scales from which to extract features (similar to an
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  ).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.52.19_PM_awcwYBa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RetinaNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RetinaNet
  </strong>
  is a one-stage object detection model that utilizes a
  <a href="https://paperswithcode.com/method/focal-loss">
   focal loss
  </a>
  function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. RetinaNet is a single, unified network composed of a
  <em>
   backbone
  </em>
  network and two task-specific
  <em>
   subnetworks
  </em>
  . The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone's output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that the authors propose specifically for one-stage, dense detection.
 </p>
 <p>
  We can see the motivation for focal loss by comparing with two-stage object detectors. Here class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g.,
  <a href="https://paperswithcode.com/method/selective-search">
   Selective Search
  </a>
  ,
  <a href="https://paperswithcode.com/method/edgeboxes">
   EdgeBoxes
  </a>
  ,
  <a href="https://paperswithcode.com/method/deepmask">
   DeepMask
  </a>
  ,
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  ) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio, or online hard example mining (
  <a href="https://paperswithcode.com/method/ohem">
   OHEM
  </a>
  ), are performed to maintain a
manageable balance between foreground and background.
 </p>
 <p>
  In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. To tackle this, RetinaNet uses a focal loss function, a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.
 </p>
 <p>
  Formally, the Focal Loss adds a factor $(1 - p_{t})^\gamma$ to the standard cross entropy criterion. Setting $\gamma&gt;0$ reduces the relative loss for well-classified examples ($p_{t}&gt;.5$), putting more focus on hard, misclassified examples. Here there is tunable
  <em>
   focusing
  </em>
  parameter $\gamma \ge 0$.
 </p>
 <p>
  $$ {\text{FL}(p_{t}) = - (1 - p_{t})^\gamma \log\left(p_{t}\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Detr</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Detr
  </strong>
  , or
  <strong>
   Detection Transformer
  </strong>
  , is a set-based object detector using a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  on top of a convolutional backbone. It uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class
and bounding box) or a “no object” class.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-20_at_9.17.39_PM_ZHS2kmV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv4</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv4
  </strong>
  is a one-stage object detection model that improves on
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  with several bags of tricks and modules introduced in the literature. The components section below details the tricks and modules used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ap.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FCOS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FCOS
  </strong>
  is an anchor-box free, proposal free, single-stage object detection model. By eliminating the predefined set of anchor boxes, FCOS avoids computation related to anchor boxes such as calculating overlapping during training. It also avoids all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv2
  </strong>
  , or
  <a href="https://www.youtube.com/watch?v=QsDDXSmGJZA">
   <strong>
    YOLO9000
   </strong>
  </a>
  , is a single-stage real-time object detection model. It improves upon
  <a href="https://paperswithcode.com/method/yolov1">
   YOLOv1
  </a>
  in several ways, including the use of
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  as a backbone,
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  , use of a high-resolution classifier, and the use of anchor boxes to predict bounding boxes, and more.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.33.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Math Formula Detection Models</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ScanSSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ScanSSD
  </strong>
  is a single-shot Detector (
  <a href="https://paperswithcode.com/method/ssd">
   SSD
  </a>
  ) for locating math formulas offset from text and embedded in textlines. It uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.09.40_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>One-Stage Object Detection Models</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     One-Stage Object Detection Models
    </strong>
    refer to a class of object detection models which are one-stage, i.e. models which skip the region proposal stage of two-stage models and run detection directly over a dense sampling of locations. These types of model usually have faster inference (possibly at the cost of performance). Below you can find a continuously updating list of one-stage object detection models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SSD
  </strong>
  is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.
 </p>
 <p>
  The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-27_at_1.59.27_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv3
  </strong>
  is a real-time, single-stage object detection model that builds on
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  with several improvements. Improvements include the use of a new backbone network,
  <a href="https://paperswithcode.com/method/darknet-53">
   Darknet-53
  </a>
  that utilises residual connections, or in the words of the author, "those newfangled residual network stuff", as well as some improvements to the bounding box prediction step, and use of three different scales from which to extract features (similar to an
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  ).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.52.19_PM_awcwYBa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RetinaNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RetinaNet
  </strong>
  is a one-stage object detection model that utilizes a
  <a href="https://paperswithcode.com/method/focal-loss">
   focal loss
  </a>
  function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. RetinaNet is a single, unified network composed of a
  <em>
   backbone
  </em>
  network and two task-specific
  <em>
   subnetworks
  </em>
  . The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone's output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that the authors propose specifically for one-stage, dense detection.
 </p>
 <p>
  We can see the motivation for focal loss by comparing with two-stage object detectors. Here class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g.,
  <a href="https://paperswithcode.com/method/selective-search">
   Selective Search
  </a>
  ,
  <a href="https://paperswithcode.com/method/edgeboxes">
   EdgeBoxes
  </a>
  ,
  <a href="https://paperswithcode.com/method/deepmask">
   DeepMask
  </a>
  ,
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  ) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio, or online hard example mining (
  <a href="https://paperswithcode.com/method/ohem">
   OHEM
  </a>
  ), are performed to maintain a
manageable balance between foreground and background.
 </p>
 <p>
  In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. To tackle this, RetinaNet uses a focal loss function, a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.
 </p>
 <p>
  Formally, the Focal Loss adds a factor $(1 - p_{t})^\gamma$ to the standard cross entropy criterion. Setting $\gamma&gt;0$ reduces the relative loss for well-classified examples ($p_{t}&gt;.5$), putting more focus on hard, misclassified examples. Here there is tunable
  <em>
   focusing
  </em>
  parameter $\gamma \ge 0$.
 </p>
 <p>
  $$ {\text{FL}(p_{t}) = - (1 - p_{t})^\gamma \log\left(p_{t}\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv4</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv4
  </strong>
  is a one-stage object detection model that improves on
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  with several bags of tricks and modules introduced in the literature. The components section below details the tricks and modules used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ap.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FCOS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FCOS
  </strong>
  is an anchor-box free, proposal free, single-stage object detection model. By eliminating the predefined set of anchor boxes, FCOS avoids computation related to anchor boxes such as calculating overlapping during training. It also avoids all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv2
  </strong>
  , or
  <a href="https://www.youtube.com/watch?v=QsDDXSmGJZA">
   <strong>
    YOLO9000
   </strong>
  </a>
  , is a single-stage real-time object detection model. It improves upon
  <a href="https://paperswithcode.com/method/yolov1">
   YOLOv1
  </a>
  in several ways, including the use of
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  as a backbone,
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  , use of a high-resolution classifier, and the use of anchor boxes to predict bounding boxes, and more.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.33.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Oriented Object Detection Models</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DAFNe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DAFNe
  </strong>
  is a dense one-stage anchor-free deep model for oriented object detection. It is a deep neural network that performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, it reduces the prediction complexity by refraining from employing bounding box anchors. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. Moreover, it introduces an orientation-aware generalization of the center-ness function to arbitrary quadrilaterals that takes into account the object's orientation and that, accordingly, accurately down-weights low-quality predictions
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.11.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        <li>
            <details class="category depth1">
            <summary>Webpage Object Detection Pipeline</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    End-to-end training pipeline for Object Detection from Webpages where candidate bounding boxes are obtained from the DOM tree
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CoVA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Context-Aware Visual Attention-based end-to-end pipeline for Webpage Object Detection (
  <em>
   CoVA
  </em>
  ) aims to learn function
  <em>
   f
  </em>
  to predict labels
  <em>
   y = [$y_1, y_2, ..., y_N$]
  </em>
  for a webpage containing
  <em>
   N
  </em>
  elements. The input to CoVA consists of:
1. a screenshot of a webpage,
2. list of bounding boxes
  <em>
   [x, y, w, h]
  </em>
  of the web elements, and
3. neighborhood information for each element obtained from the DOM tree.
 </p>
 <p>
  This information is processed in four stages:
1. the graph representation extraction for the webpage,
2. the Representation Network (
  <em>
   RN
  </em>
  ),
3. the Graph Attention Network (
  <em>
   GAT
  </em>
  ), and
4. a fully connected (
  <em>
   FC
  </em>
  ) layer.
 </p>
 <p>
  The graph representation extraction computes for every web element
  <em>
   i
  </em>
  its set of
  <em>
   K
  </em>
  neighboring web elements
  <em>
   $N_i$
  </em>
  . The
  <em>
   RN
  </em>
  consists of a Convolutional Neural Net (
  <em>
   CNN
  </em>
  ) and a positional encoder aimed to learn a visual representation
  <em>
   $v_i$
  </em>
  for each web element
  <em>
   i ∈ {1, ..., N}
  </em>
  . The
  <em>
   GAT
  </em>
  combines the visual representation
  <em>
   $v_i$
  </em>
  of the web element
  <em>
   i
  </em>
  to be classified and those of its neighbors, i.e.,
  <em>
   $v_k$ ∀k ∈ $N_i$
  </em>
  to compute the contextual representation
  <em>
   $c_i$
  </em>
  for web element
  <em>
   i
  </em>
  . Finally, the visual and contextual representations of the web element are concatenated and passed through the
  <em>
   FC
  </em>
  layer to obtain the classification output.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/199aec2b-13bf-4512-8296-15ab1e782afc.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>6. Image Feature Extractors</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Image Feature Extractors
    </strong>
    are functions or modules that can be used to learn representations from images. The most common type of feature extractor is a convolution where a kernel slides over the image, allowing for parameter sharing and translation invariance. Below you can find a continuously updating list of image feature extractors.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-05-24_at_12.48.24_AM.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   convolution
  </strong>
  is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.
 </p>
 <p>
  Intuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/pdf/1603.07285.pdf">
   https://arxiv.org/pdf/1603.07285.pdf
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.36.17_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   1 x 1 Convolution
  </strong>
  is a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an
  <a href="https://paperswithcode.com/method/feedforward-network">
   MLP
  </a>
  looking at a particular pixel location.
 </p>
 <p>
  Image Credit:
  <a href="http://deeplearning.ai">
   http://deeplearning.ai
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.59.06_AM_LxUvPQA_b3yHrLO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Depthwise Convolution
  </strong>
  is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:
 </p>
 <ol>
  <li>
   Split the input and filter into channels.
  </li>
  <li>
   We convolve each input with the respective filter.
  </li>
  <li>
   We stack the convolved outputs together.
  </li>
 </ol>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_yG6z6ESzsRW-9q5F_neOsg_eaJuoa5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pointwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pointwise Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that uses a 1x1 kernel: a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has. It can be used in conjunction with
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolutions
  </a>
  to produce an efficient class of convolutions known as
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise-separable convolutions
  </a>
  .
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Separable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  While
  <a href="https://paperswithcode.com/method/convolution">
   standard convolution
  </a>
  performs the channelwise and spatial-wise computation in one step,
  <strong>
   Depthwise Separable Convolution
  </strong>
  splits the computation into two steps:
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolution
  </a>
  applies a single convolutional filter per each input channel and
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   pointwise convolution
  </a>
  is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.
 </p>
 <p>
  Credit:
  <a href="https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for">
   Depthwise Convolution Is All You Need for Learning Multiple Visual Domains
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_10.30.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Convolutions
  </strong>
  are a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that “inflate” the kernel by inserting holes between the kernel elements. An additional parameter $l$ (dilation rate) indicates how much the kernel is widened. There are usually $l-1$ spaces inserted between kernel elements.
 </p>
 <p>
  Note that concept has existed in past literature under different names, for instance the
  <em>
   algorithme a trous
  </em>
  ,  an algorithm for wavelet decomposition (Holschneider et al., 1987; Shensa, 1992).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.48.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Grouped Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Grouped Convolution
  </strong>
  uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in
  <a href="https://paperswithcode.com/method/alexnet">
   AlexNet
  </a>
  was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as
  <a href="https://paperswithcode.com/method/resnext">
   ResNeXt
  </a>
  , it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions,
  <em>
   cardinality
  </em>
  (the size of set of transformations), we can increase accuracy by increasing it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_7mD7QoJTtJDDFjuL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   3D Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where the kernel slides in 3 dimensions as opposed to 2 dimensions with 2D convolutions. One example use case is medical imaging where a model is constructed using 3D image slices. Additionally video based data has an additional temporal dimension over images making it suitable for this module.
 </p>
 <p>
  Image: Lung nodule detection based on 3D convolutional neural networks, Fan et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_10.03.11_PM_KEC4Hm0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Non-Local Operation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Non-Local Operation
  </strong>
  is a component for capturing long-range dependencies with deep neural networks. It is a generalization of the classical non-local mean operation in computer vision. Intuitively a non-local operation computes the response at a position as a weighted sum of the features at all positions in the input feature maps. The set of positions can be in space, time, or spacetime, implying that these operations are applicable for image, sequence, and video problems.
 </p>
 <p>
  Following the non-local mean operation, a generic non-local operation for deep neural networks is defined as:
 </p>
 <p>
  $$ \mathbb{y}_{i} = \frac{1}{\mathcal{C}\left(\mathbb{x}\right)}\sum_{\forall{j}}f\left(\mathbb{x}_{i}, \mathbb{x}_{j}\right)g\left(\mathbb{x}_{j}\right) $$
 </p>
 <p>
  Here $i$ is the index of an output position (in space, time, or spacetime) whose response is to be computed and $j$ is the index that enumerates all possible positions. x is the input signal (image, sequence, video; often their features) and $y$ is the output signal of the same size as $x$. A pairwise function $f$ computes a scalar (representing relationship such as affinity) between $i$ and all $j$. The unary function $g$ computes a representation of the input signal at the position $j$. The
response is normalized by a factor $C\left(x\right)$.
 </p>
 <p>
  The non-local behavior is due to the fact that all positions ($\forall{j}$) are considered in the operation. As a comparison, a convolutional operation sums up the weighted input in a local neighborhood (e.g., $i − 1 \leq j \leq i + 1$ in a 1D case with kernel size 3), and a recurrent operation at time $i$ is often based only on the current and the latest time steps (e.g., $j = i$ or $i − 1$).
 </p>
 <p>
  The non-local operation is also different from a fully-connected (fc) layer. The equation above computes responses based on relationships between different locations, whereas fc uses learned weights. In other words, the relationship between $x_{j}$ and $x_{i}$ is not a function of the input data in fc, unlike in nonlocal layers. Furthermore, the formulation in the equation above supports inputs of variable sizes, and maintains the corresponding size in the output. On the contrary, an fc layer requires a fixed-size input/output and loses positional correspondence (e.g., that from $x_{i}$ to $y_{i}$ at the position $i$).
 </p>
 <p>
  A non-local operation is a flexible building block and can be easily used together with convolutional/recurrent layers. It can be added into the earlier part of deep neural networks, unlike fc layers that are often used in the end. This allows us to build a richer hierarchy that combines both non-local and local information.
 </p>
 <p>
  In terms of parameterisation, we usually parameterise $g$ as a linear embedding of the form $g\left(x_{j}\right) = W_{g}\mathbb{x}_{j}$ , where $W_{g}$ is a weight matrix to be learned. This is implemented as, e.g., 1×1
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  in space or 1×1×1 convolution in spacetime. For $f$ we use an affinity function, a list of which can be found
  <a href="https://paperswithcode.com/methods/category/affinity-functions">
   here
  </a>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_4.25.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Switchable Atrous Convolution (SAC)
  </strong>
  softly switches the convolutional computation between different atrous rates and gathers the results using switch functions. The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of SAC. To use SAC in a detector, we convert all the standard 3x3 convolutional layers in the bottom-up backbone to SAC.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_10.24.01_AM_nuyrEGN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deformable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Deformable convolutions
  </strong>
  add 2D offsets to the regular grid sampling locations in the standard
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . It enables free form deformation of the sampling grid. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/newest.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Invertible 1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Invertible 1x1 Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  used in flow-based generative models that reverses the ordering of channels. The weight matrix is initialized as a random rotation matrix. The log-determinant of an invertible 1 × 1 convolution of a $h \times w \times c$ tensor $h$ with $c \times c$ weight matrix $\mathbf{W}$ is straightforward to compute:
 </p>
 <p>
  $$ \log | \text{det}\left(\frac{d\text{conv2D}\left(\mathbf{h};\mathbf{W}\right)}{d\mathbf{h}}\right) | = h \cdot w \cdot \log | \text{det}\left(\mathbf{W}\right) | $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.42.51_PM_MFJT9Z8.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  1D Convolutional Neural Networks are similar to well known and more established 2D Convolutional Neural Networks. 1D Convolutional Neural Networks are used mainly used on text and 1D signals.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Groupwise Point Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Groupwise Point Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where we apply a
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   point convolution
  </a>
  groupwise (using different set of convolution filter groups).
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb_xLJNF0K.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Convolutions</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Convolutions
    </strong>
    are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   convolution
  </strong>
  is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.
 </p>
 <p>
  Intuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/pdf/1603.07285.pdf">
   https://arxiv.org/pdf/1603.07285.pdf
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.36.17_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   1 x 1 Convolution
  </strong>
  is a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an
  <a href="https://paperswithcode.com/method/feedforward-network">
   MLP
  </a>
  looking at a particular pixel location.
 </p>
 <p>
  Image Credit:
  <a href="http://deeplearning.ai">
   http://deeplearning.ai
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.59.06_AM_LxUvPQA_b3yHrLO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Depthwise Convolution
  </strong>
  is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:
 </p>
 <ol>
  <li>
   Split the input and filter into channels.
  </li>
  <li>
   We convolve each input with the respective filter.
  </li>
  <li>
   We stack the convolved outputs together.
  </li>
 </ol>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_yG6z6ESzsRW-9q5F_neOsg_eaJuoa5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pointwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pointwise Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that uses a 1x1 kernel: a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has. It can be used in conjunction with
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolutions
  </a>
  to produce an efficient class of convolutions known as
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise-separable convolutions
  </a>
  .
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Separable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  While
  <a href="https://paperswithcode.com/method/convolution">
   standard convolution
  </a>
  performs the channelwise and spatial-wise computation in one step,
  <strong>
   Depthwise Separable Convolution
  </strong>
  splits the computation into two steps:
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolution
  </a>
  applies a single convolutional filter per each input channel and
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   pointwise convolution
  </a>
  is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.
 </p>
 <p>
  Credit:
  <a href="https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for">
   Depthwise Convolution Is All You Need for Learning Multiple Visual Domains
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_10.30.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Convolutions
  </strong>
  are a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that “inflate” the kernel by inserting holes between the kernel elements. An additional parameter $l$ (dilation rate) indicates how much the kernel is widened. There are usually $l-1$ spaces inserted between kernel elements.
 </p>
 <p>
  Note that concept has existed in past literature under different names, for instance the
  <em>
   algorithme a trous
  </em>
  ,  an algorithm for wavelet decomposition (Holschneider et al., 1987; Shensa, 1992).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.48.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Grouped Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Grouped Convolution
  </strong>
  uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in
  <a href="https://paperswithcode.com/method/alexnet">
   AlexNet
  </a>
  was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as
  <a href="https://paperswithcode.com/method/resnext">
   ResNeXt
  </a>
  , it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions,
  <em>
   cardinality
  </em>
  (the size of set of transformations), we can increase accuracy by increasing it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_7mD7QoJTtJDDFjuL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   3D Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where the kernel slides in 3 dimensions as opposed to 2 dimensions with 2D convolutions. One example use case is medical imaging where a model is constructed using 3D image slices. Additionally video based data has an additional temporal dimension over images making it suitable for this module.
 </p>
 <p>
  Image: Lung nodule detection based on 3D convolutional neural networks, Fan et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_10.03.11_PM_KEC4Hm0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Switchable Atrous Convolution (SAC)
  </strong>
  softly switches the convolutional computation between different atrous rates and gathers the results using switch functions. The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of SAC. To use SAC in a detector, we convert all the standard 3x3 convolutional layers in the bottom-up backbone to SAC.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_10.24.01_AM_nuyrEGN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deformable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Deformable convolutions
  </strong>
  add 2D offsets to the regular grid sampling locations in the standard
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . It enables free form deformation of the sampling grid. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/newest.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Invertible 1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Invertible 1x1 Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  used in flow-based generative models that reverses the ordering of channels. The weight matrix is initialized as a random rotation matrix. The log-determinant of an invertible 1 × 1 convolution of a $h \times w \times c$ tensor $h$ with $c \times c$ weight matrix $\mathbf{W}$ is straightforward to compute:
 </p>
 <p>
  $$ \log | \text{det}\left(\frac{d\text{conv2D}\left(\mathbf{h};\mathbf{W}\right)}{d\mathbf{h}}\right) | = h \cdot w \cdot \log | \text{det}\left(\mathbf{W}\right) | $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.42.51_PM_MFJT9Z8.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  1D Convolutional Neural Networks are similar to well known and more established 2D Convolutional Neural Networks. 1D Convolutional Neural Networks are used mainly used on text and 1D signals.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Groupwise Point Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Groupwise Point Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where we apply a
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   point convolution
  </a>
  groupwise (using different set of convolution filter groups).
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb_xLJNF0K.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>7. Convolutions</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Convolutions
    </strong>
    are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   convolution
  </strong>
  is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.
 </p>
 <p>
  Intuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).
 </p>
 <p>
  Image Source:
  <a href="https://arxiv.org/pdf/1603.07285.pdf">
   https://arxiv.org/pdf/1603.07285.pdf
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.36.17_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   1 x 1 Convolution
  </strong>
  is a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an
  <a href="https://paperswithcode.com/method/feedforward-network">
   MLP
  </a>
  looking at a particular pixel location.
 </p>
 <p>
  Image Credit:
  <a href="http://deeplearning.ai">
   http://deeplearning.ai
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.59.06_AM_LxUvPQA_b3yHrLO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Depthwise Convolution
  </strong>
  is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:
 </p>
 <ol>
  <li>
   Split the input and filter into channels.
  </li>
  <li>
   We convolve each input with the respective filter.
  </li>
  <li>
   We stack the convolved outputs together.
  </li>
 </ol>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_yG6z6ESzsRW-9q5F_neOsg_eaJuoa5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pointwise Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pointwise Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that uses a 1x1 kernel: a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has. It can be used in conjunction with
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolutions
  </a>
  to produce an efficient class of convolutions known as
  <a href="https://paperswithcode.com/method/depthwise-separable-convolution">
   depthwise-separable convolutions
  </a>
  .
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Depthwise Separable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  While
  <a href="https://paperswithcode.com/method/convolution">
   standard convolution
  </a>
  performs the channelwise and spatial-wise computation in one step,
  <strong>
   Depthwise Separable Convolution
  </strong>
  splits the computation into two steps:
  <a href="https://paperswithcode.com/method/depthwise-convolution">
   depthwise convolution
  </a>
  applies a single convolutional filter per each input channel and
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   pointwise convolution
  </a>
  is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.
 </p>
 <p>
  Credit:
  <a href="https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for">
   Depthwise Convolution Is All You Need for Learning Multiple Visual Domains
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_10.30.20_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dilated Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dilated Convolutions
  </strong>
  are a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  that “inflate” the kernel by inserting holes between the kernel elements. An additional parameter $l$ (dilation rate) indicates how much the kernel is widened. There are usually $l-1$ spaces inserted between kernel elements.
 </p>
 <p>
  Note that concept has existed in past literature under different names, for instance the
  <em>
   algorithme a trous
  </em>
  ,  an algorithm for wavelet decomposition (Holschneider et al., 1987; Shensa, 1992).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_12.48.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Grouped Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Grouped Convolution
  </strong>
  uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in
  <a href="https://paperswithcode.com/method/alexnet">
   AlexNet
  </a>
  was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as
  <a href="https://paperswithcode.com/method/resnext">
   ResNeXt
  </a>
  , it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions,
  <em>
   cardinality
  </em>
  (the size of set of transformations), we can increase accuracy by increasing it.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0_7mD7QoJTtJDDFjuL.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   3D Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where the kernel slides in 3 dimensions as opposed to 2 dimensions with 2D convolutions. One example use case is medical imaging where a model is constructed using 3D image slices. Additionally video based data has an additional temporal dimension over images making it suitable for this module.
 </p>
 <p>
  Image: Lung nodule detection based on 3D convolutional neural networks, Fan et al
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_10.03.11_PM_KEC4Hm0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Switchable Atrous Convolution (SAC)
  </strong>
  softly switches the convolutional computation between different atrous rates and gathers the results using switch functions. The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of SAC. To use SAC in a detector, we convert all the standard 3x3 convolutional layers in the bottom-up backbone to SAC.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_10.24.01_AM_nuyrEGN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deformable Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Deformable convolutions
  </strong>
  add 2D offsets to the regular grid sampling locations in the standard
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . It enables free form deformation of the sampling grid. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/newest.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Invertible 1x1 Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Invertible 1x1 Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  used in flow-based generative models that reverses the ordering of channels. The weight matrix is initialized as a random rotation matrix. The log-determinant of an invertible 1 × 1 convolution of a $h \times w \times c$ tensor $h$ with $c \times c$ weight matrix $\mathbf{W}$ is straightforward to compute:
 </p>
 <p>
  $$ \log | \text{det}\left(\frac{d\text{conv2D}\left(\mathbf{h};\mathbf{W}\right)}{d\mathbf{h}}\right) | = h \cdot w \cdot \log | \text{det}\left(\mathbf{W}\right) | $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.42.51_PM_MFJT9Z8.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>1D CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  1D Convolutional Neural Networks are similar to well known and more established 2D Convolutional Neural Networks. 1D Convolutional Neural Networks are used mainly used on text and 1D signals.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Groupwise Point Convolution</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Groupwise Point Convolution
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  where we apply a
  <a href="https://paperswithcode.com/method/pointwise-convolution">
   point convolution
  </a>
  groupwise (using different set of convolution filter groups).
 </p>
 <p>
  Image Credit:
  <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">
   Chi-Feng Wang
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_37sVdBZZ9VK50pcAklh8AQ_KE6C7Yb_xLJNF0K.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>8. Generative Adversarial Networks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Generative Adversarial Networks (GANs)
    </strong>
    are a type of generative model that use two networks, a generator to generate images and a discriminator to discriminate between real and fake, to train a model that approximates the distribution of the data. Below you can find a continuously updating list of GANs.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   GAN
  </strong>
  , or
  <strong>
   Generative Adversarial Network
  </strong>
  , is a generative model that simultaneously trains
two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the
probability that a sample came from the training data rather than $G$.
 </p>
 <p>
  The training procedure for $G$ is to maximize the probability of $D$ making
a mistake. This framework corresponds to a minimax two-player game. In the
space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$
recovering the training data distribution and $D$ equal to $\frac{1}{2}$
everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,
the entire system can be trained with backpropagation.
 </p>
 <p>
  (Image Source:
  <a href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html">
   here
  </a>
  )
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/gan.jpeg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CycleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CycleGAN
  </strong>
  , or
  <strong>
   Cycle-Consistent GAN
  </strong>
  , is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \rightarrow Y$ and $F: Y \rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a
  <a href="https://paperswithcode.com/method/cycle-consistency-loss">
   cycle consistency loss
  </a>
  that encourages $F\left(G\left(x\right)\right) \approx x$ and $G\left(F\left(y\right)\right) \approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.
 </p>
 <p>
  For the mapping $G : X \rightarrow Y$ and its discriminator $D_{Y}$ we have the objective:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) =\mathbb{E}_{y \sim p_{data}\left(y\right)}\left[\log D_{Y}\left(y\right)\right] + \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[log(1 − D_{Y}\left(G\left(x\right)\right)\right] $$
 </p>
 <p>
  where $G$ tries to generate images $G\left(x\right)$ that look similar to images from domain $Y$, while $D_{Y}$ tries to discriminate between translated samples $G\left(x\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \rightarrow X$ and its discriminator $D_{X}$.
 </p>
 <p>
  The Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:
 </p>
 <p>
  $$ \mathcal{L}_{cyc}\left(G, F\right) = \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[||F\left(G\left(x\right)\right) - x||_{1}\right] + \mathbb{E}_{y \sim p_{data}\left(y\right)}\left[||G\left(F\left(y\right)\right) - y||_{1}\right] $$
 </p>
 <p>
  The full objective is:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) = \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) + \mathcal{L}_{GAN}\left(F, D_{X}, X, Y\right) + \lambda\mathcal{L}_{cyc}\left(G, F\right) $$
 </p>
 <p>
  Where we aim to solve:
 </p>
 <p>
  $$ G^{*}, F^{*} = \arg \min_{G, F} \max_{D_{X}, D_{Y}} \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) $$
 </p>
 <p>
  For the original architecture the authors use:
 </p>
 <ul>
  <li>
   two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\frac{1}{2}$.
  </li>
  <li>
   <a href="https://paperswithcode.com/method/instance-normalization">
    instance normalization
   </a>
  </li>
  <li>
   PatchGANs for the discriminator
  </li>
  <li>
   Least Square Loss for the
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objectives.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_3.54.24_PM_aoT8JRU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN
  </strong>
  is a type of generative adversarial network. It uses an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature; in particular, the use of
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  . Otherwise it follows Progressive
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  in using a progressively growing training regime. Other quirks include the fact it generates from a fixed value tensor not stochastically generated latent variables as in regular GANs. The stochastically generated latent variables are used as style vectors in the adaptive
  <a href="https://paperswithcode.com/method/instance-normalization">
   instance normalization
  </a>
  at each resolution after being transformed by an 8-layer
  <a href="https://paperswithcode.com/method/feedforward-network">
   feedforward network
  </a>
  . Lastly, it employs a form of regularization called mixing regularization, which mixes two style latent variables during training.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_9.15.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>StyleGAN2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   StyleGAN2
  </strong>
  is a generative adversarial network that builds on
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  with several improvements. First,
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   adaptive instance normalization
  </a>
  is redesigned and replaced with a normalization technique called
  <a href="https://paperswithcode.com/method/weight-demodulation">
   weight demodulation
  </a>
  . Secondly, an improved training scheme upon progressively growing is introduced, which achieves the same goal - training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions - without changing the network topology during training. Additionally, new types of regularization like lazy regularization and
  <a href="https://paperswithcode.com/method/path-length-regularization">
   path length regularization
  </a>
  are proposed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-29_at_9.55.21_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SAGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Self-Attention Generative Adversarial Network
  </strong>
  , or
  <strong>
   SAGAN
  </strong>
  , allows for attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_1.36.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pix2Pix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pix2Pix
  </strong>
  is a conditional image-to-image translation architecture that uses a conditional
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  objective combined with a reconstruction loss. The conditional GAN objective for observed images $x$, output images $y$ and the random noise vector $z$ is:
 </p>
 <p>
  $$ \mathcal{L}_{cGAN}\left(G, D\right) =\mathbb{E}_{x,y}\left[\log D\left(x, y\right)\right]+
\mathbb{E}_{x,z}\left[log(1 − D\left(x, G\left(x, z\right)\right)\right] $$
 </p>
 <p>
  We augment this with a reconstruction term:
 </p>
 <p>
  $$ \mathcal{L}_{L1}\left(G\right) = \mathbb{E}_{x,y,z}\left[||y - G\left(x, z\right)||_{1}\right] $$
 </p>
 <p>
  and we get the final objective as:
 </p>
 <p>
  $$ G^{*} = \arg\min_{G}\max_{D}\mathcal{L}_{cGAN}\left(G, D\right) + \lambda\mathcal{L}_{L1}\left(G\right) $$
 </p>
 <p>
  The architectures employed for the generator and discriminator closely follow
  <a href="https://paperswithcode.com/method/dcgan">
   DCGAN
  </a>
  , with a few modifications:
 </p>
 <ul>
  <li>
   Concatenated skip connections are used to "shuttle" low-level information between the input and output, similar to a
   <a href="https://paperswithcode.com/method/u-net">
    U-Net
   </a>
   .
  </li>
  <li>
   The use of a
   <a href="https://paperswithcode.com/method/patchgan">
    PatchGAN
   </a>
   discriminator that only penalizes structure at the scale of patches.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_12.37.47_PM_dZrgNzj.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BigGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BigGAN
  </strong>
  is a type of generative adversarial network that was designed for scaling generation to high-resolution, high-fidelity images. It includes a number of incremental changes and innovations. The baseline and incremental changes are:
 </p>
 <ul>
  <li>
   Using
   <a href="https://paperswithcode.com/method/sagan">
    SAGAN
   </a>
   as a baseline with spectral norm. for G and D, and using
   <a href="https://paperswithcode.com/method/ttur">
    TTUR
   </a>
   .
  </li>
  <li>
   Using a Hinge Loss
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objective
  </li>
  <li>
   Using class-
   <a href="https://paperswithcode.com/method/conditional-batch-normalization">
    conditional batch normalization
   </a>
   to provide class information to G (but with linear projection not MLP.
  </li>
  <li>
   Using a
   <a href="https://paperswithcode.com/method/projection-discriminator">
    projection discriminator
   </a>
   for D to provide class information to D.
  </li>
  <li>
   Evaluating with EWMA of G's weights, similar to ProGANs.
  </li>
 </ul>
 <p>
  The innovations are:
 </p>
 <ul>
  <li>
   Increasing batch sizes, which has a big effect on the Inception Score of the model.
  </li>
  <li>
   Increasing the width in each layer leads to a further Inception Score improvement.
  </li>
  <li>
   Adding skip connections from the latent variable $z$ to further layers helps performance.
  </li>
  <li>
   A new variant of
   <a href="https://paperswithcode.com/method/orthogonal-regularization">
    Orthogonal Regularization
   </a>
   .
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.41.08_PM_cKfKr80.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Wasserstein GAN
  </strong>
  , or
  <strong>
   WGAN
  </strong>
  , is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_2.53.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DCGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DCGAN
  </strong>
  , or
  <strong>
   Deep Convolutional GAN
  </strong>
  , is a generative adversarial network architecture. It uses a couple of guidelines, in particular:
 </p>
 <ul>
  <li>
   Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).
  </li>
  <li>
   Using batchnorm in both the generator and the discriminator.
  </li>
  <li>
   Removing fully connected hidden layers for deeper architectures.
  </li>
  <li>
   Using
   <a href="https://paperswithcode.com/method/relu">
    ReLU
   </a>
   activation in generator for all layers except for the output, which uses tanh.
  </li>
  <li>
   Using LeakyReLU activation in the discriminator for all layer.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.27.51_PM_IoGbo1i.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>9. Image Data Augmentation</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Image Data Augmentation
    </strong>
    refers to a class of methods that augment an image dataset to increase the effective size of the training set, or as a form of regularization to help the network learn more effective representations.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/data_augmentation.jpg" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Mixup</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mixup
  </strong>
  is a data augmentation technique that generates a weighted combination of random image pairs from the training data. Given two images and their ground truth labels: $\left(x_{i}, y_{i}\right), \left(x_{j}, y_{j}\right)$, a synthetic training example $\left(\hat{x}, \hat{y}\right)$ is generated as:
 </p>
 <p>
  $$ \hat{x} = \lambda{x_{i}} + \left(1 − \lambda\right){x_{j}} $$
$$ \hat{y} = \lambda{y_{i}} + \left(1 − \lambda\right){y_{j}} $$
 </p>
 <p>
  where $\lambda \sim \text{Beta}\left(\alpha = 0.2\right)$ is independently sampled for each augmented example.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_cifar10_test_error.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GPS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Greedy Policy Search
  </strong>
  (GPS) is a simple algorithm that learns a policy for test-time data augmentation based on the predictive performance on a validation set. GPS starts with an empty policy and builds it in an iterative fashion. Each step selects a sub-policy that provides the largest improvement in calibrated log-likelihood of ensemble predictions and adds it to the current policy.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.43.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Resized Crop</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RandomResizedCrop
  </strong>
  is a type of image data augmentation where a crop of random size of the original size and a random aspect ratio of the original aspect ratio is made. This crop is finally resized to given size.
 </p>
 <p>
  Image Credit:
  <a href="https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html">
   Apache MXNet
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/output_12_0_PevPHfi.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ColorJitter</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ColorJitter
  </strong>
  is a type of image data augmentation where we randomly change the brightness, contrast and saturation of an image.
 </p>
 <p>
  Image Credit:
  <a href="https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html">
   Apache MXNet
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/output_35_0_DxAQHli.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Gaussian Blur</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Random Gaussian Blur
  </strong>
  is an image data augmentation technique where we randomly blur the image using a Gaussian distribution.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Gaussian_blur">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_2.10.04_PM_k4r9kS5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CutMix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CutMix
  </strong>
  is an image data augmentation strategy. Instead of simply removing pixels as in
  <a href="https://paperswithcode.com/method/cutout">
   Cutout
  </a>
  , we replace the removed regions with a patch from another image. The ground truth labels are also mixed proportionally to the number of pixels of combined images. The added patches further enhance localization ability by requiring the model to identify the object from a partial view.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_1.40.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Random Horizontal Flip</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RandomHorizontalFlip
  </strong>
  is a type of image data augmentation which horizontally flips a given image with a given probability.
 </p>
 <p>
  Image Credit:
  <a href="https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html">
   Apache MXNet
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/output_15_0_WrOyS6Q.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cutout</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cutout
  </strong>
  is an image augmentation and regularization technique that randomly masks out square regions of input during training. and can be used to improve the robustness and overall performance of convolutional neural networks. The main motivation for cutout comes from the problem of object occlusion, which is commonly encountered in many computer vision tasks, such as object recognition,
tracking, or human pose estimation. By generating new images which simulate occluded examples, we not only better prepare the model for encounters with occlusions in the real world, but the model also learns to take more of the image context into consideration when making decisions
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_1.33.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RandAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RandAugment
  </strong>
  is an automated data augmentation method. The search space for data augmentation has 2 interpretable hyperparameter $N$ and $M$.  $N$ is the number of augmentation transformations to apply sequentially, and $M$ is the magnitude for all the transformations. To reduce the parameter space but still maintain image diversity, learned policies and probabilities for applying each transformation are replaced with a parameter-free procedure of always selecting a transformation with uniform probability $\frac{1}{K}$. Here $K$ is the number of transformation options. So given $N$ transformations for a training image, RandAugment may thus express $KN$ potential policies.
 </p>
 <p>
  Transformations applied include identity transformation, autoContrast, equalize, rotation, solarixation, colorjittering, posterizing, changing contrast, changing brightness, changing sharpness, shear-x, shear-y, translate-x, translate-y.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_11.09.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AutoAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AutoAugment
  </strong>
  is an automated approach to find data augmentation policies from data. It formulates the problem of finding the best augmentation policy as a discrete search problem. It consists of two components: a search algorithm and a search space.
 </p>
 <p>
  At a high level, the search algorithm (implemented as a controller RNN) samples a data augmentation policy $S$, which has information about what image processing operation to use, the probability of using the operation in each batch, and the magnitude of the operation. The policy $S$ is used to train a neural network with a fixed architecture, whose validation accuracy $R$ is sent back to update the controller. Since $R$ is not differentiable, the controller will be updated by policy gradient methods.
 </p>
 <p>
  The operations used are from PIL, a popular Python image library: all functions in PIL that accept an image as input and output an image. It additionally uses two other augmentation techniques:
  <a href="https://paperswithcode.com/method/cutout">
   Cutout
  </a>
  and SamplePairing. The operations searched over are ShearX/Y, TranslateX/Y, Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, Cutout and Sample Pairing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_1.39.41_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Adversarial Image Data Augmentation</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DiffAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Differentiable Augmentation (DiffAugment)
  </strong>
  is a set of differentiable image transformations used to augment data during
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  training. The transformations are applied to the real and generated images. It enables the gradients to be propagated through the augmentation back to the generator, regularizes
the discriminator without manipulating the target distribution, and maintains the balance of training
dynamics. Three choices of transformation are preferred by the authors in their experiments: Translation,
  <a href="https://paperswithcode.com/method/cutout">
   CutOut
  </a>
  , and Color.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/diffaugment_UH9jHYa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adversarial Color Enhancement</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adversarial Color Enhancement
  </strong>
  is an approach to generating unrestricted adversarial images by optimizing a color filter via gradient descent.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.48.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MaxUp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MaxUp
  </strong>
  is an adversarial data augmentation technique for improving the generalization performance of machine learning models. The idea is to generate a set of augmented data with some random perturbations or transforms, and minimize the maximum, or worst case loss over the augmented data.  By doing so, we implicitly introduce a smoothness or robustness regularization against the random perturbations, and hence improve the generation performance.  For example, in the case of Gaussian perturbation, MaxUp is asymptotically equivalent to using the gradient norm of the loss as a penalty to encourage smoothness.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_1.18.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>10. Vision Transformers</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Vision Transformers
    </strong>
    are
    <a href="https://www.paperswithcode.com/methods/category/transformers">
     Transformer
    </a>
    -like models applied to visual tasks.  They stem from the work of
    <a href="https://www.paperswithcode.com/method/vision-transformer">
     ViT
    </a>
    which directly applied a Transformer architecture on non-overlapping medium-sized image patches for image classification. Below you can find a continually updating list of vision transformers.
   </p>
   <p>
    According to [1], ViT type models can be further categorized into uniform scale ViTs, multi-scale ViT,  hybrid ViTs with convolutions, and self-supervised ViTs. The methods listed below provide a comprehensive overview of ViT models applied to a range of vision tasks.
   </p>
   <p>
    [1]
    <a href="https://arxiv.org/abs/2101.01169">
     Transformers in Vision: A Survey
    </a>
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/VIT.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Vision Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Vision Transformer
  </strong>
  , or
  <strong>
   ViT
  </strong>
  , is a model for image classification that employs a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -like architecture over patches of the image.  An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Swin Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The
  <strong>
   Swin Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  . It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_11.42.58_AM_jkrbxmo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Detr</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Detr
  </strong>
  , or
  <strong>
   Detection Transformer
  </strong>
  , is a set-based object detector using a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  on top of a convolutional backbone. It uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class
and bounding box) or a “no object” class.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-20_at_9.17.39_PM_ZHS2kmV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DINO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DINO
  </strong>
  (self-distillation with no labels) is a self-supervised learning method that directly predicts the output of a teacher network - built with a momentum encoder - using a standard cross-entropy loss.
 </p>
 <p>
  In the example to the right, DINO is illustrated in the case of one single pair of views $\left(x_{1}, x_{2}\right)$ for simplicity.
The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but other parameters.
The output of the teacher network is centered with a mean computed over the batch. Each network outputs a $K$ dimensional feature normalized with a temperature
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  over the feature dimension.
Their similarity is then measured with a cross-entropy loss.
A stop-gradient (sg) operator is applied to the teacher to propagate gradients only through the student.
The teacher parameters are updated with the student parameters' exponential moving average (ema).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.14.59_PM_fumVop5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DeiT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Data-Efficient Image Transformer
  </strong>
  is a type of
  <a href="https://paperswithcode.com/method/vision-transformer">
   Vision Transformer
  </a>
  for image classification tasks. The model is trained using a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-12-24_at_1.16.17_PM_4ybxEGe.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>11. Semantic Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Semantic Segmentation Models
    </strong>
    are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>U-Net</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   U-Net
  </strong>
  is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (
  <a href="https://paperswithcode.com/method/relu">
   ReLU
  </a>
  ) and a 2x2
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a
  <a href="https://paperswithcode.com/method/1x1-convolution">
   1x1 convolution
  </a>
  is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.
 </p>
 <p>
  <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz">
   Original MATLAB Code
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FCN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fully Convolutional Networks
  </strong>
  , or
  <strong>
   FCNs
  </strong>
  , are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  , pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train). It also means an FCN can work for variable image sizes given all connections are local.
 </p>
 <p>
  The network consists of a downsampling path, used to extract and interpret the context, and an upsampling path, which allows for localization.
 </p>
 <p>
  FCNs also employ skip connections to recover the fine-grained spatial information lost in the downsampling path.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_alex-model.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SegNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SegNet
  </strong>
  is a semantic segmentation model. This core trainable segmentation architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the
VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature maps. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to
perform non-linear upsampling.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/segnet_Vorazx7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DeepLab</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeepLab
  </strong>
  is a semantic segmentation architecture. First, the input image goes through the network with the use of dilated convolutions. Then the output from the network is bilinearly interpolated and goes through the fully connected
  <a href="https://paperswithcode.com/method/crf">
   CRF
  </a>
  to fine tune the result we obtain the final predictions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-27_at_4.44.28_PM_EX1YqfX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Interactive Semantic Segmentation Models</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>EdgeFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EdgeFlow
  </strong>
  is an interactive segmentation architecture that fully utilizes interactive information of user clicks with edge-guided flow. Edge guidance is the idea that interactive segmentation improves segmentation masks progressively with user clicks. Based on user clicks, an edge mask scheme is used, which takes the object edges estimated from the previous iteration as prior information, instead of direct mask estimation (if the previous mask is used as input, poor segmentation results could result).
 </p>
 <p>
  The architecture consists of a coarse-to-fine network including CoarseNet and FineNet. For CoarseNet,
  <a href="https://paperswithcode.com/method/hrnet">
   HRNet
  </a>
  -18+OCR is utilized as the base segmentation model and the edge-guided flow is appended to deal with interactive information. For FineNet, three
  <a href="https://paperswithcode.com/method/dilated-convolution">
   atrous convolution
  </a>
  blocks are utilized to refine the coarse masks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/97e2dc43-ab5a-4cd3-82d8-5f7706ad3fe4.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>12. Feature Extractors</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Feature Extractors
    </strong>
    for object detection are modules used to construct features that can be used for detecting objects. They address issues such as the need to detect multiple-sized objects in an image (and the need to have representations that are suitable for the different scales).
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Feature Pyramid Network
  </strong>
  , or
  <strong>
   FPN
  </strong>
  , is a feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection.
 </p>
 <p>
  The construction of the pyramid involves a bottom-up pathway and a top-down pathway.
 </p>
 <p>
  The bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature
pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  we use the feature activations output by each stage’s last
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  .
 </p>
 <p>
  The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Spatio-temporal features extraction that measure the stabilty. The proposed method is based on a compression algorithm named Run Length Encoding. The workflow of the method is presented bellow.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PAFPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PAFPN
  </strong>
  is a feature pyramid module used in Path Aggregation networks (
  <a href="https://paperswithcode.com/method/panet">
   PANet
  </a>
  ) that combines FPNs with
  <a href="https://paperswithcode.com/method/bottom-up-path-augmentation">
   bottom-up path augmentation
  </a>
  , which shortens the information path between lower layers and topmost feature.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.45.11_PM_rBS7GBR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bottom-up Path Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Bottom-up Path Augmentation
  </strong>
  is a feature extraction technique that seeks to shorten the information path and enhance a feature pyramid with accurate localization signals existing in low-levels. This is based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances.
 </p>
 <p>
  Each building block takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ through lateral connection and generates the new feature map $N_{i+1}$ Each feature map $N_{i}$ first goes through a $3 \times 3$ convolutional layer with stride $2$ to reduce the spatial size. Then each element of feature map $P_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 \times 3$ convolutional layer to generate $N_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P_{5}$. In these building blocks, we consistently use channel 256 of feature maps. The feature grid for each proposal is then pooled from new feature maps, i.e., {$N_{2}$, $N_{3}$, $N_{4}$, $N_{5}$}.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.46.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DLA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DLA
  </strong>
  , or
  <strong>
   Deep Layer Aggregation
  </strong>
  ,  iteratively and hierarchically merges the feature hierarchy across layers in neural networks to make networks with better accuracy and fewer parameters.
 </p>
 <p>
  In iterative deep aggregation (IDA), aggregation begins at the shallowest, smallest scale and then iteratively merges deeper,
larger scales. In this way shallow features are refined as
they are propagated through different stages of aggregation.
 </p>
 <p>
  In hierarchical deep aggregation (HDA), blocks and stages
in a tree are merged to preserve and combine feature channels. With
HDA shallower and deeper layers are combined to learn
richer combinations that span more of the feature hierarchy.
While IDA effectively combines stages, it is insufficient
for fusing the many blocks of a network, as it is still only
sequential.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_3.48.01_PM_ZEEYRZq.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Feature Pyramid Blocks</summary>
            <ul>
                <li class="col-md-12">
                    <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Feature Pyramid Blocks
    </strong>
    are a basic component in recognition systems for detecting objects at different scales.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Feature Pyramid Network
  </strong>
  , or
  <strong>
   FPN
  </strong>
  , is a feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection.
 </p>
 <p>
  The construction of the pyramid involves a bottom-up pathway and a top-down pathway.
 </p>
 <p>
  The bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature
pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  we use the feature activations output by each stage’s last
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  .
 </p>
 <p>
  The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PAFPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PAFPN
  </strong>
  is a feature pyramid module used in Path Aggregation networks (
  <a href="https://paperswithcode.com/method/panet">
   PANet
  </a>
  ) that combines FPNs with
  <a href="https://paperswithcode.com/method/bottom-up-path-augmentation">
   bottom-up path augmentation
  </a>
  , which shortens the information path between lower layers and topmost feature.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.45.11_PM_rBS7GBR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bottom-up Path Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Bottom-up Path Augmentation
  </strong>
  is a feature extraction technique that seeks to shorten the information path and enhance a feature pyramid with accurate localization signals existing in low-levels. This is based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances.
 </p>
 <p>
  Each building block takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ through lateral connection and generates the new feature map $N_{i+1}$ Each feature map $N_{i}$ first goes through a $3 \times 3$ convolutional layer with stride $2$ to reduce the spatial size. Then each element of feature map $P_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 \times 3$ convolutional layer to generate $N_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P_{5}$. In these building blocks, we consistently use channel 256 of feature maps. The feature grid for each proposal is then pooled from new feature maps, i.e., {$N_{2}$, $N_{3}$, $N_{4}$, $N_{5}$}.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.46.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BiFPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   BiFPN
  </strong>
  , or
  <strong>
   Weighted Bi-directional Feature Pyramid Network
  </strong>
  , is a type of feature pyramid network which allows easy and fast multi-scale feature fusion. It incorporates the multi-level feature fusion idea from
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  ,
  <a href="https://paperswithcode.com/method/panet">
   PANet
  </a>
  and
  <a href="https://paperswithcode.com/method/nas-fpn">
   NAS-FPN
  </a>
  that enables information to flow in both the top-down and bottom-up directions, while using regular and efficient connections. It also utilizes a fast normalized fusion technique. Traditional approaches usually treat all features input to the FPN equally, even those with different resolutions. However, input features at different resolutions often have unequal contributions to the output features. Thus, the BiFPN adds an additional weight for each input feature allowing the network to learn the importance of each. All regular convolutions are also replaced with less expensive depthwise separable convolutions.
 </p>
 <p>
  Comparing with PANet, PANet added an extra bottom-up path for information flow at the expense of more computational cost. Whereas BiFPN optimizes these cross-scale connections by removing nodes with a single input edge, adding an extra edge from the original input to output node if they are on the same level, and treating each bidirectional path as one feature network layer (repeating it several times for more high-level future fusion).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_3.01.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>13. Vision and Language Pre-Trained Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Involves models that adapt pre-training to the field of Vision-and-Language (V-L) learning and improve the performance on downstream tasks like visual question answering and visual captioning.
   </p>
   <p>
    According to
    <a href="https://arxiv.org/pdf/2202.10936.pdf">
     Du et al. (2022)
    </a>
    , information coming from the different modalities can be encoded in three ways: fusion encoder, dual encoder, and a combination of both.
   </p>
   <p>
    References:
   </p>
   <ul>
    <li>
     <a href="https://arxiv.org/pdf/2202.10936.pdf">
      A Survey of Vision-Language Pre-Trained Models
     </a>
    </li>
    <li>
     <a href="https://theaisummer.com/vision-language-models/">
      Vision Language models: towards multi-modal deep learning
     </a>
    </li>
   </ul>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
  <div class="mb-3">
   <a id="pop">
    <img id="imageresource" src="https://production-media.paperswithcode.com/method_collections/f3eeb095-fbea-4f60-88ac-0d8190d3dcb7.png" width="100%"/>
   </a>
  </div>
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ALIGN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In the ALIGN method, visual and language representations are jointly trained from noisy image alt-text data. The image and text encoders are learned via contrastive loss (formulated as normalized softmax) that pushes the embeddings of the matched image-text pair together and pushing those of non-matched image-text pair apart. The model learns to align visual and language representations of the image and text pairs using the contrastive loss. The representations can be used for vision-only or vision-language task transfer. Without any fine-tuning, ALIGN powers zero-shot visual classification and cross-modal search including image-to-text search, text-to image search and even search with joint image+text queries.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/62d2465b-1ef3-41dd-b221-ab69de5b2040.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CLIP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contrastive Language-Image Pre-training
  </strong>
  (
  <strong>
   CLIP
  </strong>
  ), consisting of a simplified version of ConVIRT trained from scratch, is an efficient method of image representation learning from natural language supervision. , CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.
 </p>
 <p>
  For pre-training, CLIP is trained to predict which of the $N X N$ possible (image, text) pairings across a batch actually occurred. CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 - N$ incorrect pairings. A symmetric cross entropy loss is optimized over these similarity scores.
 </p>
 <p>
  Image credit:
  <a href="https://arxiv.org/pdf/2103.00020.pdf">
   Learning Transferable Visual Models From Natural Language Supervision
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3d5d1009-6e3d-4570-8fd9-ee8f588003e7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BLIP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/bf9bd9a4-da80-4059-bdc1-3f49549d4044.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LXMERT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  LXMERT is a model for learning vision-and-language cross-modality representations. It consists of a Transformer model that consists three encoders: object relationship encoder, a language encoder, and a cross-modality encoder. The model takes two inputs: image with its related sentence. The images are represented as a sequence of objects, whereas each sentence is represented as sequence of words. By combining the self-attention and cross-attention layers the model is able to generated language representation, image representations, and cross-modality representations from the input. The model is pre-trained with image-sentence pairs via five pre-training tasks: masked language modeling, masked object prediction, cross-modality matching, and image questions answering. These tasks help the model to learn both intra-modality and cross-modality relationships.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6f81bf57-d48e-40ce-9d9f-258f552719f0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>14. One-Stage Object Detection Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     One-Stage Object Detection Models
    </strong>
    refer to a class of object detection models which are one-stage, i.e. models which skip the region proposal stage of two-stage models and run detection directly over a dense sampling of locations. These types of model usually have faster inference (possibly at the cost of performance). Below you can find a continuously updating list of one-stage object detection models.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SSD
  </strong>
  is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.
 </p>
 <p>
  The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-27_at_1.59.27_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv3
  </strong>
  is a real-time, single-stage object detection model that builds on
  <a href="https://paperswithcode.com/method/yolov2">
   YOLOv2
  </a>
  with several improvements. Improvements include the use of a new backbone network,
  <a href="https://paperswithcode.com/method/darknet-53">
   Darknet-53
  </a>
  that utilises residual connections, or in the words of the author, "those newfangled residual network stuff", as well as some improvements to the bounding box prediction step, and use of three different scales from which to extract features (similar to an
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  ).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.52.19_PM_awcwYBa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RetinaNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RetinaNet
  </strong>
  is a one-stage object detection model that utilizes a
  <a href="https://paperswithcode.com/method/focal-loss">
   focal loss
  </a>
  function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. RetinaNet is a single, unified network composed of a
  <em>
   backbone
  </em>
  network and two task-specific
  <em>
   subnetworks
  </em>
  . The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone's output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that the authors propose specifically for one-stage, dense detection.
 </p>
 <p>
  We can see the motivation for focal loss by comparing with two-stage object detectors. Here class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g.,
  <a href="https://paperswithcode.com/method/selective-search">
   Selective Search
  </a>
  ,
  <a href="https://paperswithcode.com/method/edgeboxes">
   EdgeBoxes
  </a>
  ,
  <a href="https://paperswithcode.com/method/deepmask">
   DeepMask
  </a>
  ,
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  ) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio, or online hard example mining (
  <a href="https://paperswithcode.com/method/ohem">
   OHEM
  </a>
  ), are performed to maintain a
manageable balance between foreground and background.
 </p>
 <p>
  In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. To tackle this, RetinaNet uses a focal loss function, a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.
 </p>
 <p>
  Formally, the Focal Loss adds a factor $(1 - p_{t})^\gamma$ to the standard cross entropy criterion. Setting $\gamma&gt;0$ reduces the relative loss for well-classified examples ($p_{t}&gt;.5$), putting more focus on hard, misclassified examples. Here there is tunable
  <em>
   focusing
  </em>
  parameter $\gamma \ge 0$.
 </p>
 <p>
  $$ {\text{FL}(p_{t}) = - (1 - p_{t})^\gamma \log\left(p_{t}\right)} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv4</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv4
  </strong>
  is a one-stage object detection model that improves on
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  with several bags of tricks and modules introduced in the literature. The components section below details the tricks and modules used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_ap.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FCOS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FCOS
  </strong>
  is an anchor-box free, proposal free, single-stage object detection model. By eliminating the predefined set of anchor boxes, FCOS avoids computation related to anchor boxes such as calculating overlapping during training. It also avoids all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>YOLOv2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOv2
  </strong>
  , or
  <a href="https://www.youtube.com/watch?v=QsDDXSmGJZA">
   <strong>
    YOLO9000
   </strong>
  </a>
  , is a single-stage real-time object detection model. It improves upon
  <a href="https://paperswithcode.com/method/yolov1">
   YOLOv1
  </a>
  in several ways, including the use of
  <a href="https://paperswithcode.com/method/darknet-19">
   Darknet-19
  </a>
  as a backbone,
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  , use of a high-resolution classifier, and the use of anchor boxes to predict bounding boxes, and more.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_12.33.47_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>15. Light-weight neural networks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Lightweight neural networks or mobile-friendly neural networks are deep neural networks which are designed for mobile devices or other resource-limited hardwares
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MobileNetV2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV2
  </strong>
  is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer with 32 filters, followed by 19 residual bottleneck layers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_10.37.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SqueezeNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SqueezeNet
  </strong>
  is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that "squeeze" parameters using 1x1 convolutions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV3</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNetV3
  </strong>
  is a convolutional neural network that is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the
  <a href="https://paperswithcode.com/method/netadapt">
   NetAdapt
  </a>
  algorithm, and then subsequently improved through novel architecture advances. Advances include (1) complementary search techniques, (2) new efficient versions of nonlinearities practical for the mobile setting, (3) new efficient network design.
 </p>
 <p>
  The network design includes the use of a
  <a href="https://paperswithcode.com/method/hard-swish">
   hard swish
  </a>
  activation and squeeze-and-excitation modules in the MBConv blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.03.15_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MobileNetV1</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MobileNet
  </strong>
  is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_4.26.15_PM_ko4FqXD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>16. Pooling Operations</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Pooling Operations
    </strong>
    are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales).
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Max Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Max Pooling
  </strong>
  is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.
 </p>
 <p>
  Image Source:
  <a href="https://computersciencewiki.org/index.php/File:MaxpoolSample2.png">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/MaxpoolSample2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Average Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Average Pooling
  </strong>
  is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than
  <a href="https://paperswithcode.com/method/max-pooling">
   Max Pooling
  </a>
  , whereas max pooling extracts more pronounced features like edges.
 </p>
 <p>
  Image Source:
  <a href="https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451">
   here
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_1.51.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Global Average Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Global Average Pooling
  </strong>
  is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the
  <a href="https://paperswithcode.com/method/softmax">
   softmax
  </a>
  layer.
 </p>
 <p>
  One advantage of global
  <a href="https://paperswithcode.com/method/average-pooling">
   average pooling
  </a>
  over the fully connected layers is that it is more native to the
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_12.15.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Pyramid Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Spatial Pyramid Pooling (SPP)
  </strong>
  is a pooling layer that removes the fixed-size constraint of the network, i.e. a CNN does not require a fixed-size input image. Specifically, we add an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed-length outputs, which are then fed into the fully-connected layers (or other classifiers). In other words, we perform some information aggregation at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_3.05.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>17. Instance Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Instance Segmentation
    </strong>
    models are models that perform the task of
    <a href="https://paperswithcode.com/task/instance-segmentation">
     Instance Segmentation
    </a>
    .
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Mask R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mask R-CNN
  </strong>
  extends
  <a href="https://paperswithcode.com/method/faster-r-cnn">
   Faster R-CNN
  </a>
  to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster
  <a href="https://paperswithcode.com/method/r-cnn">
   R-CNN
  </a>
  , but constructing the mask branch properly is critical for good results.
 </p>
 <p>
  Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is evident in how
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  , the
  <em>
   de facto
  </em>
  core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, Mask R-CNN utilises a simple, quantization-free layer, called
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , that faithfully preserves exact spatial locations.
 </p>
 <p>
  Secondly, Mask R-CNN
  <em>
   decouples
  </em>
  mask and class prediction: it predicts a binary mask for each class independently, without competition among classes, and relies on the network's RoI classification branch to predict the category. In contrast, an
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  usually perform per-pixel multi-class categorization, which couples segmentation and classification.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.44.34_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HTC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hybrid Task Cascade
  </strong>
  , or
  <strong>
   HTC
  </strong>
  , is a framework for cascading in instance segmentation. It differs from
  <a href="https://paperswithcode.com/method/cascade-mask-r-cnn">
   Cascade Mask R-CNN
  </a>
  in two important aspects:  (1) instead of performing cascaded refinement on the two tasks of detection and segmentation separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_4.13.03_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Cascade Mask R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Cascade Mask R-CNN
  </strong>
  extends
  <a href="https://paperswithcode.com/method/cascade-r-cnn">
   Cascade R-CNN
  </a>
  to instance segmentation, by adding a
mask head to the cascade.
 </p>
 <p>
  In the
  <a href="https://paperswithcode.com/method/mask-r-cnn">
   Mask R-CNN
  </a>
  , the segmentation branch is inserted in parallel to the detection branch. However, the Cascade
  <a href="https://paperswithcode.com/method/r-cnn">
   R-CNN
  </a>
  has multiple detection branches. This raises the questions of 1) where to add the segmentation branch and 2) how many segmentation branches to add. The authors consider three strategies for mask prediction in the Cascade R-CNN. The first two strategies address the first question, adding a single mask prediction head at either the first or last stage of the Cascade R-CNN. Since the instances used to train the segmentation branch are the positives of the detection branch, their number varies in these two strategies. Placing the segmentation head later on the cascade leads to more examples. However, because segmentation is a pixel-wise operation, a large number of highly overlapping instances is not necessarily as helpful as for object detection, which is a patch-based operation. The third strategy addresses the second question, adding a segmentation branch to each
cascade stage. This maximizes the diversity of samples used to learn the mask prediction task.
 </p>
 <p>
  At inference time, all three strategies predict the segmentation masks on the patches produced by the final object detection stage, irrespective of the cascade stage on which the segmentation mask is implemented and how many segmentation branches there are.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_11.51.41_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PANet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Path Aggregation Network
  </strong>
  , or
  <strong>
   PANet
  </strong>
  , aims to boost information flow in a proposal-based instance segmentation framework. Specifically, the feature hierarchy is enhanced with accurate localization signals in lower layers by
  <a href="https://paperswithcode.com/method/bottom-up-path-augmentation">
   bottom-up path augmentation
  </a>
  , which shortens the information path between lower layers and topmost feature. Additionally,
  <a href="https://paperswithcode.com/method/adaptive-feature-pooling">
   adaptive feature pooling
  </a>
  is employed, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.45.11_PM_5uZqVAp.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>18. 3D Reconstruction</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>NeRF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NeRF represents a scene with learned, continuous volumetric radiance field $F_\theta$ defined over a bounded 3D volume. In a NeRF, $F_\theta$ is a multilayer perceptron (MLP) that takes as input a 3D position $x = (x, y, z)$ and unit-norm viewing direction $d = (dx, dy, dz)$, and produces as output a density $\sigma$ and color $c = (r, g, b)$. The weights of the multilayer perceptron that parameterize $F_\theta$ are optimized so as to encode the radiance field of the scene. Volume rendering is used to compute the color of a single pixel.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/5e700ef6067b43821ed52768_pipeline_website-01_J5dBbiA.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3d gaussian splatting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  It represents the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ARCH</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Animatable Reconstruction of Clothed Humans
  </strong>
  is an end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.38.26_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NICE-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NICE-SLAM, a dense RGB-D SLAM system that combines neural implicit decoders with hierarchical grid-based representations, which can be applied to large-scale scenes.
 </p>
 <p>
  Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/de7409fc-3ba3-4fe5-8c4f-f597722814e1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>19. 3D Representations</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>NeRF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NeRF represents a scene with learned, continuous volumetric radiance field $F_\theta$ defined over a bounded 3D volume. In a NeRF, $F_\theta$ is a multilayer perceptron (MLP) that takes as input a 3D position $x = (x, y, z)$ and unit-norm viewing direction $d = (dx, dy, dz)$, and produces as output a density $\sigma$ and color $c = (r, g, b)$. The weights of the multilayer perceptron that parameterize $F_\theta$ are optimized so as to encode the radiance field of the scene. Volume rendering is used to compute the color of a single pixel.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/5e700ef6067b43821ed52768_pipeline_website-01_J5dBbiA.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PointNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PointNet
  </strong>
  provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. It directly takes point clouds as input and outputs either class labels for the entire input or per point segment/part labels for each point of the input.
 </p>
 <p>
  Source:
  <a href="https://arxiv.org/pdf/1612.00593v2.pdf">
   Qi et al.
  </a>
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/1612.00593v2.pdf">
   Qi et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-11_at_14.09.05_jNbJxcd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3DIS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   trainable 3D interaction space
  </strong>
  aims to captures the associations between the triplet components and helps model the recognition of multiple triplets in the same frame.
 </p>
 <p>
  Source:
  <a href="https://arxiv.org/pdf/2007.05405v1.pdf">
   Nwoye et al.
  </a>
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/2007.05405v1.pdf">
   Nwoye et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-11_at_14.19.50_2esAImJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3D Dynamic Scene Graph</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   3D Dynamic Scene Graph
  </strong>
  , or
  <strong>
   DSG
  </strong>
  , is a representation that captures metric and semantic aspects of a dynamic environment. A DSG is a layered graph where nodes represent spatial concepts at different levels of abstraction, and edges represent spatio-temporal relations among nodes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_10.57.36_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>20. Image Representations</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CLIP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contrastive Language-Image Pre-training
  </strong>
  (
  <strong>
   CLIP
  </strong>
  ), consisting of a simplified version of ConVIRT trained from scratch, is an efficient method of image representation learning from natural language supervision. , CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.
 </p>
 <p>
  For pre-training, CLIP is trained to predict which of the $N X N$ possible (image, text) pairings across a batch actually occurred. CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 - N$ incorrect pairings. A symmetric cross entropy loss is optimized over these similarity scores.
 </p>
 <p>
  Image credit:
  <a href="https://arxiv.org/pdf/2103.00020.pdf">
   Learning Transferable Visual Models From Natural Language Supervision
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3d5d1009-6e3d-4570-8fd9-ee8f588003e7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Laplacian Pyramid</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Laplacian Pyramid
  </strong>
  is a linear invertible image representation consisting of a set of band-pass
images spaced an octave apart, plus a low-frequency residual. Formally, let $d\left(.\right)$ be a downsampling operation that blurs and decimates a $j \times j$ image $I$ so that $d\left(I\right)$ is a new image of size $\frac{j}{2} \times \frac{j}{2}$. Also, let $u\left(.\right)$ be an upsampling operator which smooths and expands $I$ to be twice the size, so $u\left(I\right)$ is a new image of size $2j \times 2j$. We first build a Gaussian pyramid $G\left(I\right) = \left[I_{0}, I_{1}, \dots, I_{K}\right]$, where
$I_{0} = I$ and $I_{k}$ is $k$ repeated application of $d\left(.\right)$ to $I$. $K$ is the number of levels in the pyramid selected so that the final level has a minimal spatial extent ($\leq 8 \times 8$ pixels).
 </p>
 <p>
  The coefficients $h_{k}$ at each level $k$ of the Laplacian pyramid $L\left(I\right)$ are constructed by taking the difference between adjacent levels in the Gaussian pyramid, upsampling the smaller one with $u\left(.\right)$ so that the sizes are compatible:
 </p>
 <p>
  $$ h_{k} = \mathcal{L}_{k}\left(I\right) = G_{k}\left(I\right) − u\left(G_{k+1}\left(I\right)\right) = I_{k} − u\left(I_{k+1}\right) $$
 </p>
 <p>
  Intuitively, each level captures the image structure present at a particular scale. The final level of the
Laplacian pyramid $h_{K}$ is not a difference image, but a low-frequency residual equal to the final
Gaussian pyramid level, i.e. $h_{K} = I_{K}$. Reconstruction from a Laplacian pyramid coefficients
$\left[h_{1}, \dots, h_{K}\right]$ is performed using the backward recurrence:
 </p>
 <p>
  $$ I_{k} = u\left(I_{k+1}\right) + h_{k} $$
 </p>
 <p>
  which is started with $I_{K} = h_{K}$ and the reconstructed image being $I = I_{o}$. In other words, starting at the coarsest level, we repeatedly upsample and add the difference image h at the next finer level until we return to the full-resolution image.
Source:
  <a href="https://paperswithcode.com/method/lapgan">
   LAPGAN
  </a>
 </p>
 <p>
  Image :
  <a href="https://www.researchgate.net/figure/Relationship-between-Gaussian-and-Laplacian-Pyramids_fig2_275038450">
   Design of FIR Filters for Fast Multiscale Directional Filter Banks
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/2a9881dd-d953-4d3e-a417-09e1292938c9.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Low-resolution input</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>High-resolution input</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>21. Semantic Segmentation Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ASPP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Atrous Spatial Pyramid Pooling (ASPP)
  </strong>
  is a semantic segmentation module for resampling a given feature layer at multiple rates prior to
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . This amounts to probing the original image with multiple filters that have complementary effective fields of view, thus capturing objects as well as useful image context at multiple scales. Rather than actually resampling features, the mapping is implemented using multiple parallel atrous convolutional layers with different sampling rates.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_2.59.27_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Pyramid Pooling Module</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Pyramid Pooling Module
  </strong>
  is a module for semantic segmentation which acts as an effective global contextual prior. The motivation is that the problem of using a convolutional network like a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  is that, while the receptive field is already larger than the input image, the empirical receptive field is much smaller than the theoretical one especially on high-level layers. This makes many networks not sufficiently incorporate the momentous global scenery prior.
 </p>
 <p>
  The PPM is an effective global prior representation that addresses this problem. It contains information with different scales and varying among different sub-regions. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior. Then we concatenate the prior with the original feature map in the final part.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_pspnet-eps-converted-to_N6tDBnG.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NEAT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NEAT
  </strong>
  , or
  <strong>
   Neural Attention Fields
  </strong>
  , is a feature representation for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows the model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.26.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PointRend</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PointRend
  </strong>
  is a module for image segmentation tasks, such as instance and semantic segmentation, that attempts to treat segmentation as image rending problem to efficiently "render" high-quality label maps. It uses a subdivision strategy to adaptively select a non-uniform set of points at which to compute labels. PointRend can be incorporated into popular meta-architectures for both instance segmentation (e.g.
  <a href="https://paperswithcode.com/method/mask-r-cnn">
   Mask R-CNN
  </a>
  ) and semantic segmentation (e.g.
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  ). Its subdivision strategy efficiently computes high-resolution segmentation maps using an order of magnitude fewer floating-point operations than direct, dense computation.
 </p>
 <p>
  PointRend is a general module that admits many possible implementations. Viewed abstractly, a PointRend module accepts one or more typical CNN feature maps $f\left(x_{i}, y_{i}\right)$ that are defined over regular grids, and outputs high-resolution predictions $p\left(x^{'}_{i}, y^{'}_{i}\right)$ over a finer grid. Instead of making excessive predictions over all points on the output grid, PointRend makes predictions only on carefully selected points. To make these predictions, it extracts a point-wise feature representation for the selected points by interpolating $f$, and uses a small point head subnetwork to predict output labels from the point-wise features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/POINTREND_JBHqa6Q.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>22. RoI Feature Extractors</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     RoI Feature Extractors
    </strong>
    are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>RoIPool</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Region of Interest Pooling
  </strong>
  , or
  <strong>
   RoIPool
  </strong>
  , is an operation for extracting a small feature map (e.g., $7×7$) from each RoI in detection and segmentation based tasks. Features are extracted from each candidate box, and thereafter in models like
  <a href="https://paperswithcode.com/method/fast-r-cnn">
   Fast R-CNN
  </a>
  , are then classified and bounding box regression performed.
 </p>
 <p>
  The actual scaling to, e.g., $7×7$, occurs by dividing the region proposal into equally sized sections, finding the largest value in each section, and then copying these max values to the output buffer. In essence,
  <strong>
   RoIPool
  </strong>
  is
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  on a discrete grid based on a box.
 </p>
 <p>
  Image Source:
  <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">
   Joyce Xu
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1_YZMAa60ycjCzLn5T_HkXgQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RoIAlign</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Region of Interest Align
  </strong>
  , or
  <strong>
   RoIAlign
  </strong>
  , is an operation for extracting a small feature map from each RoI in detection and segmentation based tasks. It removes the harsh quantization of
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoI Pool
  </a>
  , properly
  <em>
   aligning
  </em>
  the extracted features with the input. To avoid any quantization of the RoI boundaries or bins (using $x/16$ instead of $[x/16]$), RoIAlign uses bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and the result is then aggregated (using max or average).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_roialign_t0Wv9vI.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Position-Sensitive RoI Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Position-Sensitive RoI Pooling layer
  </strong>
  aggregates the outputs of the last convolutional layer and generates scores for each RoI. Unlike
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoI Pooling
  </a>
  , PS RoI Pooling conducts selective pooling, and each of the $k$ × $k$ bin aggregates responses from only one score map out of the bank of $k$ × $k$ score maps. With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_method.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Voxel RoI Pooling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Voxel RoI Pooling
  </strong>
  is a RoI feature extractor extracts RoI features directly from voxel features for further refinement. It starts by dividing a region proposal into $G \times G \times G$ regular sub-voxels. The center point is taken as the grid point of the corresponding sub-voxel. Since $3 D$ feature volumes are extremely sparse (non-empty voxels account for $&lt;3 \%$ spaces), we cannot directly utilize max pooling over features of each sub-voxel. Instead, features are integrated from neighboring voxels into the grid points for feature extraction. Specifically, given a grid point $g_{i}$, we first exploit voxel query to group a set of neighboring voxels $\Gamma_{i}=\left(\mathbf{v}_{i}^{1}, \mathbf{v}_{i}^{2}, \cdots, \mathbf{v}_{i}^{K}\right) .$ Then, we aggregate the neighboring voxel features with a
  <a href="https://paperswithcode.com/method/pointnet">
   PointNet
  </a>
  module $\mathrm{a}$ as:
 </p>
 <p>
  $$
\mathbf{\eta}_{i}=\max _{k=1,2, \cdots, K}\left(\Psi\left(\left[\mathbf{v}_{i}^{k}-\mathbf{g}_{i} ; \mathbf{\phi}_{i}^{k}\right]\right)\right)
$$
 </p>
 <p>
  where $\mathbf{v}_{i}-\mathbf{g}_{i}$ represents the relative coordinates, $\mathbf{\phi}_{i}^{k}$ is the voxel feature of $\mathbf{v}_{i}^{k}$, and $\Psi(\cdot)$ indicates an MLP. The
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  operation $\max (\cdot)$ is performed along the channels to obtain the aggregated feature vector $\eta_{i} .$ Particularly, Voxel RoI pooling is exploited to extract voxel features from the 3D feature volumes out of the last two stages in the $3 \mathrm{D}$ backbone network. And for each stage, two Manhattan distance thresholds are set to group voxels with multiple scales. Then, we concatenate the aggregated features pooled from different stages and scales to obtain the RoI features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/27eefe7c-0cc0-4dca-96c3-b82eafc1120f.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>23. Feature Pyramid Blocks</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Feature Pyramid Blocks
    </strong>
    are a basic component in recognition systems for detecting objects at different scales.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Feature Pyramid Network
  </strong>
  , or
  <strong>
   FPN
  </strong>
  , is a feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection.
 </p>
 <p>
  The construction of the pyramid involves a bottom-up pathway and a top-down pathway.
 </p>
 <p>
  The bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature
pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For
  <a href="https://paperswithcode.com/method/resnet">
   ResNets
  </a>
  we use the feature activations output by each stage’s last
  <a href="https://paperswithcode.com/method/residual-block">
   residual block
  </a>
  .
 </p>
 <p>
  The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_teaser_TMZlD2J.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PAFPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PAFPN
  </strong>
  is a feature pyramid module used in Path Aggregation networks (
  <a href="https://paperswithcode.com/method/panet">
   PANet
  </a>
  ) that combines FPNs with
  <a href="https://paperswithcode.com/method/bottom-up-path-augmentation">
   bottom-up path augmentation
  </a>
  , which shortens the information path between lower layers and topmost feature.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.45.11_PM_rBS7GBR.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bottom-up Path Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Bottom-up Path Augmentation
  </strong>
  is a feature extraction technique that seeks to shorten the information path and enhance a feature pyramid with accurate localization signals existing in low-levels. This is based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances.
 </p>
 <p>
  Each building block takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ through lateral connection and generates the new feature map $N_{i+1}$ Each feature map $N_{i}$ first goes through a $3 \times 3$ convolutional layer with stride $2$ to reduce the spatial size. Then each element of feature map $P_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 \times 3$ convolutional layer to generate $N_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P_{5}$. In these building blocks, we consistently use channel 256 of feature maps. The feature grid for each proposal is then pooled from new feature maps, i.e., {$N_{2}$, $N_{3}$, $N_{4}$, $N_{5}$}.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_1.46.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BiFPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   BiFPN
  </strong>
  , or
  <strong>
   Weighted Bi-directional Feature Pyramid Network
  </strong>
  , is a type of feature pyramid network which allows easy and fast multi-scale feature fusion. It incorporates the multi-level feature fusion idea from
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  ,
  <a href="https://paperswithcode.com/method/panet">
   PANet
  </a>
  and
  <a href="https://paperswithcode.com/method/nas-fpn">
   NAS-FPN
  </a>
  that enables information to flow in both the top-down and bottom-up directions, while using regular and efficient connections. It also utilizes a fast normalized fusion technique. Traditional approaches usually treat all features input to the FPN equally, even those with different resolutions. However, input features at different resolutions often have unequal contributions to the output features. Thus, the BiFPN adds an additional weight for each input feature allowing the network to learn the importance of each. All regular convolutions are also replaced with less expensive depthwise separable convolutions.
 </p>
 <p>
  Comparing with PANet, PANet added an extra bottom-up path for information flow at the expense of more computational cost. Whereas BiFPN optimizes these cross-scale connections by removing nodes with a single input edge, adding an extra edge from the original input to output node if they are on the same level, and treating each bidirectional path as one feature network layer (repeating it several times for more high-level future fusion).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_3.01.23_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>24. 3D Object Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CenterPoint</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CenterPoint
  </strong>
  is a two-stage 3D detector that finds centers of objects and their properties using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation and velocity. In a second-stage, it refines these estimates using additional point features on the object. CenterPoint uses a standard Lidar-based backbone network, i.e., VoxelNet or PointPillars, to build a representation of the input point-cloud. CenterPoint predicts the relative offset (velocity) of objects between consecutive frames, which are then linked up greedily -- so in Centerpoint, 3D object tracking simplifies to greedy closest-point matching.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_9.33.05_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Voxel R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Voxel R-CNN
  </strong>
  is a voxel-based two stage framework for 3D object detection. It consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. Voxel RoI Pooling is devised to extract RoI features directly from raw features for further refinement.
 </p>
 <p>
  End-to-end, the point clouds are first divided into regular voxels and fed into the 3D backbone network for feature extraction. Then, the 3D feature volumes are converted into BEV representation, on which the 2D backbone and
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  are applied for region proposal generation. Subsequently,
  <a href="https://paperswithcode.com/method/voxel-roi-pooling">
   Voxel RoI Pooling
  </a>
  directly extracts RoI features from the 3D feature volumes. Finally the RoI features are exploited in the detect head for further box refinement.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a19bcf51-5f0f-4331-98ec-1d9dd46d9ac2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DSGN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Deep Stereo Geometry Network
  </strong>
  is a 3D object detection pipeline that relies on space transformation from 2D features to an effective 3D structure, called 3D geometric volume (3DGV). The whole neural network consists of four components. (a) A 2D image
feature extractor for capture of both pixel- and high-level feature. (b) Constructing the plane-sweep volume and 3D geometric volume. (c) Depth Estimation on the plane-sweep volume. (d) 3D object detection on 3D geometric volume.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.02.29_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>3DSSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   3DSSD
  </strong>
  is a point-based 3D single stage object detection detector. In this paradigm, all upsampling layers and refinement stage, which are indispensable in all existing point-based methods, are abandoned to reduce the large computation cost. The authors propose a fusion sampling strategy in the downsampling process to make detection on less representative points feasible. A delicate box prediction network including a candidate generation layer, an anchor-free regression head with a 3D center-ness assignment strategy is designed to meet the needs of accuracy and speed.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.58.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>25. Multi-Modal Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>GLIDE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  GLIDE is a generative model based on text-guided diffusion models for more photorealistic image generation. Guided diffusion is applied to text-conditional image synthesis and the model is able to handle free-form prompts. The diffusion model uses a text encoder to condition on natural language descriptions. The model is provided with editing capabilities in addition to zero-shot generation, allowing for iterative improvement of model samples to match more complex prompts. The model is fine-tuned to perform image inpainting.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/2b1b3128-5071-4b14-bd1b-ecb5f1effc3b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>UNIMO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   UNIMO
  </strong>
  is a multi-modal pre-training architecture that can effectively adapt to both single modal and multimodal understanding and generation tasks. UNIMO learns visual representations and textual representations simultaneously, and unifies them into the same semantic space via
  <a href="https://paperswithcode.com/method/cmcl">
   cross-modal contrastive learning
  </a>
  (CMCL) based on a large-scale corpus of image collections, text corpus and image-text pairs. The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic
space based on image-text pairs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/9b2ef281-1337-4cc3-91af-6f4918efa569.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EmbraceNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Vokenization</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Vokenization
  </strong>
  is an approach for extrapolating multimodal alignments to language-only data by contextually mapping language tokens to their related images ("vokens") by retrieval. Instead of directly supervising the language model with visually grounded language datasets (e.g., MS COCO) these relative small datasets are used to train the vokenization processor (i.e. the vokenizer). Vokens are generated for large language corpora (e.g., English Wikipedia), and the visually-supervised language model takes the
input supervision from these large datasets, thus bridging the gap between different data sources.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.26.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>26. Likelihood-Based Generative Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    <strong>
     Likelihood-based Generative Models
    </strong>
    are a class of generative model that model the distribution of the data $p\left(y\right)$ directly with a likelihood function. The most popular subclass of likelihood-based generative models is variational autoencoders.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Variational Autoencoder
  </strong>
  is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_4.47.56_PM_Y06uCVO.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VQ-VAE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VQ-VAE
  </strong>
  is a type of variational autoencoder that uses vector quantisation to obtain a discrete latent representation. It differs from
  <a href="https://paperswithcode.com/method/vae">
   VAEs
  </a>
  in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, ideas from vector quantisation (VQ) are incorporated. Using the VQ method allows the model to circumvent issues of posterior collapse - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.26.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GLOW</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GLOW
  </strong>
  is a type of flow-based generative model that is based on an invertible $1 \times 1$
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . This builds on the flows introduced by
  <a href="https://paperswithcode.com/method/nice">
   NICE
  </a>
  and
  <a href="https://paperswithcode.com/method/realnvp">
   RealNVP
  </a>
  . It consists of a series of steps of flow, combined in a multi-scale architecture; see the Figure to the right. Each step of flow consists of Act Normalization followed by an
  <em>
   invertible $1 \times 1$ convolution
  </em>
  followed by an
  <a href="https://paperswithcode.com/method/affine-coupling">
   affine coupling
  </a>
  layer.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_8.43.24_PM_tNckkOB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PixelCNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   PixelCNN
  </strong>
  is a generative model that uses autoregressive connections to model images pixel by pixel, decomposing the joint image distribution as a product of conditionals. PixelCNNs are much faster to train than
  <a href="https://paperswithcode.com/method/pixelrnn">
   PixelRNNs
  </a>
  because convolutions are inherently easier to parallelize; given the vast number of pixels present in large image datasets this is an important advantage.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-16_at_7.27.51_PM_tpsd8Td.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>27. Generative Video Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>TimeSformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TimeSformer
  </strong>
  is a
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  -free approach to video classification built exclusively on self-attention over space and time. It adapts the standard
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Specifically, the method adapts the image model
  <a href="https//www.paperswithcode.com/method/vision-transformer">
   [Vision Transformer](https://paperswithcode.com/method/vision-transformer)
  </a>
  (ViT) to video by extending the self-attention mechanism from the image space to the space-time 3D volume. As in ViT, each patch is linearly mapped into an embedding and augmented with positional information. This makes it possible to interpret the resulting sequence of vector
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_1.26.56_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TGAN
  </strong>
  is a type of generative adversarial network that is capable of learning representation from an unlabeled video dataset and producing a new video. The generator consists of two sub networks
called a temporal generator and an image generator. Specifically, the temporal generator first yields a set of latent variables, each of which corresponds to a latent variable for the image generator. Then, the image generator transforms these latent variables into a video which has the same number of frames as the variables. The model comprised of the temporal and image generators can not only enable to efficiently capture the time series, but also be easily extended to frame interpolation. The authors opt for a
  <a href="https://paperswithcode.com/method/wgan">
   WGAN
  </a>
  as the basic
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  structure and objective, but use
  <a href="https://paperswithcode.com/method/singular-value-clipping">
   singular value clipping
  </a>
  to enforce the Lipschitz constraint.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_8.41.09_PM_t1WUmee.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CVRL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contrastive Video Representation Learning
  </strong>
  , or
  <strong>
   CVRL
  </strong>
  , is a self-supervised contrastive learning framework for learning spatiotemporal visual representations from unlabeled videos. Representations are learned using a contrastive loss, where two clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. Data augmentations are designed involving spatial and temporal cues. Concretely, a
  <a href="https://paperswithcode.com/method/temporally-consistent-spatial-augmentation#">
   temporally consistent spatial augmentation
  </a>
  method is used to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. A sampling-based temporal augmentation method is also used to avoid overly enforcing invariance on clips that are distant in time.
 </p>
 <p>
  End-to-end, from a raw video, we first sample a temporal interval from a monotonically decreasing distribution. The temporal interval represents the number of frames between the start points of two clips, and we sample two clips from a video according to this interval. Afterwards we apply a
  <a href="https://paperswithcode.com/method/temporally-consistent-spatial-augmentation">
   temporally consistent spatial augmentation
  </a>
  to each of the clips and feed them into a 3D backbone with an MLP head. The contrastive loss is used to train the network to attract the clips from the same video and repel the clips from different videos in the embedding space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_10.45.55_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Dreamix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>28. Image Generation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Diffusion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Diffusion models generate samples by gradually
removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound (https://arxiv.org/abs/2006.11239).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GLIDE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  GLIDE is a generative model based on text-guided diffusion models for more photorealistic image generation. Guided diffusion is applied to text-conditional image synthesis and the model is able to handle free-form prompts. The diffusion model uses a text encoder to condition on natural language descriptions. The model is provided with editing capabilities in addition to zero-shot generation, allowing for iterative improvement of model samples to match more complex prompts. The model is fine-tuned to perform image inpainting.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/2b1b3128-5071-4b14-bd1b-ecb5f1effc3b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>classifier-guidance</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Blended Diffusion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Blended Diffusion enables a zero-shot local text-guided image editing of natural images.
Given an input image $x$, an input mask $m$ and a target guiding text $t$ - the method enables to change the masked area within the image corresponding the the guiding text s.t. the unmasked area is left unchanged.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/172c31ff-899f-415f-9440-770ad23e1ebb.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>29. Backbone Architectures</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ConvNeXt</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Deep Sets</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Spatial Broadcast Decoder</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Spatial Broadcast Decoder is an architecture that aims to improve disentangling, reconstruction accuracy, and generalization to held-out regions in data space. It provides a particularly dramatic
benefit when applied to datasets with small objects.
 </p>
 <p>
  Source:
  <a href="https://arxiv.org/pdf/1901.07017v2.pdf">
   Watters et al.
  </a>
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/1901.07017v2.pdf">
   Watters et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-11_at_14.24.09_OZMIgWV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TNT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation. In each TNT block, an outer transformer block is utilized to process patch embeddings, and an inner transformer block extracts local features from pixel embeddings. The pixel-level feature is projected to the space of patch embedding by a linear transformation layer and then added into the patch. By stacking the TNT blocks, we build the TNT model for image recognition.
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/2103.00112v1.pdf">
   Han et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-03-03_at_13.43.50_3W1ILqB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>30. Point Cloud Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>YOHO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   You Only Hypothesize Once
  </strong>
  is a local descriptor-based framework for the registration of two unaligned point clouds. The proposed descriptor achieves the rotation invariance by recent technologies of group equivariant feature learning, which brings more robustness to point density and noise. The descriptor in YOHO also has a rotation-equivariant part, which enables the estimation the registration from just one correspondence hypothesis.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.54.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Voxel R-CNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Voxel R-CNN
  </strong>
  is a voxel-based two stage framework for 3D object detection. It consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. Voxel RoI Pooling is devised to extract RoI features directly from raw features for further refinement.
 </p>
 <p>
  End-to-end, the point clouds are first divided into regular voxels and fed into the 3D backbone network for feature extraction. Then, the 3D feature volumes are converted into BEV representation, on which the 2D backbone and
  <a href="https://paperswithcode.com/method/rpn">
   RPN
  </a>
  are applied for region proposal generation. Subsequently,
  <a href="https://paperswithcode.com/method/voxel-roi-pooling">
   Voxel RoI Pooling
  </a>
  directly extracts RoI features from the 3D feature volumes. Finally the RoI features are exploited in the detect head for further box refinement.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a19bcf51-5f0f-4331-98ec-1d9dd46d9ac2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RPM-Net</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RPM-Net
  </strong>
  is an end-to-end differentiable deep network for robust point matching uses learned features. It preserves robustness of RPM against noisy/outlier points while desensitizing initialization with point correspondences from learned feature distances instead of spatial distances. The network uses the differentiable Sinkhorn layer and annealing to get soft assignments of point correspondences from hybrid features learned from both spatial coordinates and local geometry. To further improve registration performance, the authors introduce a secondary network to predict optimal annealing parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_10.21.22_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PREDATOR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PREDATOR
  </strong>
  is a model for pairwise point-cloud registration with deep attention to the overlap region. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.14.26_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>31. Image Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DEXTR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DEXTR
  </strong>
  , or
  <strong>
   Deep Extreme Cut
  </strong>
  , obtains an object segmentation from its four extreme points: the left-most, right-most, top, and bottom pixels. The annotated extreme points are given as a guiding signal to the input of the network. To this end, we create a
  <a href="https://paperswithcode.com/method/heatmap">
   heatmap
  </a>
  with activations in the regions of extreme points. We center a 2D Gaussian around each of the points, in order to create a single heatmap. The heatmap is concatenated with the RGB channels of the input image, to form a 4-channel input for the CNN. In order to focus on the object of interest, the input is cropped by the bounding box, formed from the extreme point annotations. To include context on the resulting
crop, we relax the tight bounding box by several pixels. After the pre-processing step that comes exclusively from the extreme clicks, the input consists of an RGB crop including an object, plus its extreme points.
 </p>
 <p>
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  -101 is chosen as backbone of the architecture. We remove the fully connected layers as well as the
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  layers in the last two stages to preserve acceptable output resolution for dense prediction, and we introduce atrous convolutions in the last two stages to maintain the same receptive field. After the last ResNet-101 stage, we introduce a pyramid scene parsing module to aggregate global context to the final feature map. The output of the CNN is a probability map representing whether a pixel belongs to the object that we want to segment or not. The CNN is trained to minimize the standard cross entropy loss, which takes into account that different classes occur with different frequency in a dataset.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-24_at_1.51.37_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HANet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Height-driven Attention Network
  </strong>
  , or
  <strong>
   HANet
  </strong>
  , is a general add-on module for improving semantic segmentation for urban-scene images. It emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urban scene dataset effectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.40.36_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SegSort</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>32. Proposal Filtering</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Non Maximum Suppression</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Non Maximum Suppression
  </strong>
  is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the prediction, and discard any remaining box where a $\text{IoU} \geq 0.5$ with the box output in the previous step.
 </p>
 <p>
  Image Credit:
  <a href="https://github.com/martinkersner/non-maximum-suppression-cpp">
   Martin Kersner
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_3.39.11_PM_Vmo7dyu.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Soft-NMS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box $M$ with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold)
with $M$ are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss.
 </p>
 <p>
  <strong>
   Soft-NMS
  </strong>
  solves this problem by decaying the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-13_at_7.01.00_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Matrix NMS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Matrix NMS
  </strong>
  , or
  <strong>
   Matrix Non-Maximum Suppression
  </strong>
  ,  performs
  <a href="https://paperswithcode.com/method/non-maximum-suppression">
   non-maximum suppression
  </a>
  with parallel matrix operations in one shot. It is motivated by
  <a href="https://paperswithcode.com/method/soft-nms">
   Soft-NMS
  </a>
  . Soft-NMS decays the other detection scores as a monotonic decreasing function $f(iou)$ of their overlaps. By decaying the scores according to IoUs recursively, higher IoU detections will be eliminated with a minimum score threshold. However, such process is sequential like traditional Greedy NMS and can not be implemented in parallel.
 </p>
 <p>
  Matrix NMS views this process from another perspective by considering how a predicted mask $m_{j}$ being suppressed. For $m_{j}$, its decay factor is affected by: (a) The penalty of each prediction $m_{i}$ on $m_{j}$ $\left(s_{i}&gt;s_{j}\right)$, where $s_{i}$ and $s_{j}$ are the confidence scores; and (b) the probability of $m_{i}$ being suppressed. For (a), the penalty of each prediction $m_{i}$ on $m_{j}$ could be easily computed by $f\left(\right.$ iou $\left._{i, j}\right)$. For (b), the probability of $m_{i}$ being suppressed is not so elegant to be computed. However, the probability usually has positive correlation with the IoUs. So here we directly approximate the probability by the most overlapped prediction on $m_{i}$ as
 </p>
 <p>
  $$
f\left(\text { iou. }_{, i}\right)=\min_{\forall s_{k}&gt;s_{i}} f\left(\text { iou }_{k, i}\right)
$$
 </p>
 <p>
  To this end, the final decay factor becomes
 </p>
 <p>
  $$
\operatorname{decay}_{j}=\min_{\forall s_{i}&gt;s_{j}} \frac{f\left(\text { iou }_{i, j}\right)}{f\left(\text { iou }_{\cdot, i}\right)}
$$
 </p>
 <p>
  and the updated score is computed by $s_{j}=s_{j} \cdot$ decay $_{j} .$ The authors consider the two most simple decremented functions, denoted as linear $f\left(\right.$ iou $\left._{i, j}\right)=1-$ iou $_{i, j}$, and Gaussian $f\left(\right.$ iou $\left._{i, j}\right)=\exp \left(-\frac{i o u_{i, j}^{2}}{\sigma}\right)$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DIoU-NMS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DIoU-NMS
  </strong>
  is a type of non-maximum suppression where we use Distance IoU rather than regular DIoU, in which the overlap area and the distance between two central points of bounding boxes are simultaneously considered when suppressing redundant boxes.
 </p>
 <p>
  In original NMS, the IoU metric is used to suppress the redundant detection boxes, where the overlap area is the unique factor, often yielding false suppression for the cases with occlusion. With DIoU-NMS, we not only consider the overlap area but also central point distance between two boxes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_3.37.38_PM_avBiokQ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>33. Face Recognition Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MFR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Meta Face Recognition
  </strong>
  (MFR) is a meta-learning face recognition method. MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, domain-shift batches are built through a domain-level sampling strategy and back-propagated gradients/meta-gradients are obtained on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.00.49_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MagFace</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MagFace
  </strong>
  is a category of losses for face recognition that learn a universal feature embedding whose magnitude can measure the quality of a given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, MagFace introduces an adaptive mechanism to learn a well-structured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. For face recognition, MagFace helps prevent model overfitting on noisy and low-quality samples by an adaptive mechanism to learn well-structured within-class feature distributions -- by pulling easy samples to class centers while pushing hard samples away.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_10.16.50_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NFR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Negative Face Recognition
  </strong>
  , or
  <strong>
   NFR
  </strong>
  , is a face recognition approach that enhances the soft-biometric privacy on the template-level by representing face templates in a complementary (negative) domain. While ordinary templates characterize facial properties of an individual, negative templates describe facial properties that does not exist for this individual. This suppresses privacy-sensitive information from stored templates. Experiments are conducted on two publicly available datasets captured under controlled and uncontrolled scenarios on three privacy-sensitive attributes.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.43.55_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CurricularFace</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CurricularFace
  </strong>
  , or
  <strong>
   Adaptive Curriculum Learning
  </strong>
  , is a method for face recognition that embeds the idea of curriculum learning into the loss function to achieve a new training scheme. This training scheme mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.33.51_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>34. Multi-Object Tracking Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Wizard</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Computer vision is an interesting tool for animal behavior monitoring, mainly because it limits animal handling and it can be used to record various traits using only one sensor. From previous studies, this technic has shown to be suitable for various species and behavior. However it remains challenging to collect individual information, i.e. not only to detect animals and behavior on the video frames, but also to identify them. Animal identification is a prerequisite to gather individual information in order to characterize individuals and compare them. A common solution to this problem, known as multiple objects tracking, consists in detecting the animals on each video frame, and then associate detections to a unique animal ID. Association of detections between two consecutive frames are generally made to maintain coherence of the detection locations and appearances. To extract appearance information, a common solution is to use a convolutional neural network (CNN), trained on a large dataset before running the tracking algorithm. For farmed animals, designing such network is challenging as far as large training dataset are still lacking. In this article, we proposed an innovative solution, where the CNN used to extract appearance information is parameterized using offline unsupervised training. The algorithm, named Wizard, was evaluated for the purpose of goats monitoring in outdoor conditions. 17 annotated videos were used, for a total of 4H30, with various number of animals on the video (from 3 to 8) and different level of color differences between animals. First, the ability of the algorithm to track the detected animals was evaluated. When animals were detected, the algorithm found the correct animal ID in 94.82% of the frames. When tracking and detection were evaluated together, we found that Wizard found the correct animal ID in 86.18% of the video length. In situations where the animal detection rate could be high, Wizard seems to be a suitable solution for individual behavior analysis experiments based on computer vision.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CenterTrack</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FairMOT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FairMOT
  </strong>
  is a model for multi-object tracking which consists of two homogeneous branches to predict pixel-wise objectness scores and re-ID features. The achieved fairness between the tasks is used to achieve high levels of detection and tracking accuracy. The detection branch is implemented in an anchor-free style which estimates object centers and sizes represented as position-aware measurement maps. Similarly, the re-ID branch estimates a re-ID feature for each pixel to characterize the object centered at the pixel. Note that the two branches are completely homogeneous which essentially differs from the previous methods which perform detection and re-ID in a cascaded style. It is also worth noting that FairMOT operates on high-resolution feature maps of strides four while the previous anchor-based methods operate on feature maps of stride 32. The elimination of anchors as well as the use of high-resolution feature maps better aligns re-ID features to object centers which significantly improves the tracking accuracy.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_3.44.10_PM_mPkAMWJ.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SMOT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Single-Shot Multi-Object Tracker
  </strong>
  or
  <strong>
   SMOT
  </strong>
  , is a tracking framework that converts any single-shot detector (SSD) model into an online multiple object tracker, which emphasizes simultaneously detecting and tracking of the object paths. Contrary to the existing tracking by detection approaches which suffer from errors made by the object detectors, SMOT adopts the recently proposed scheme of tracking by re-detection.
 </p>
 <p>
  The proposed SMOT consists of two stages. The first stage generates temporally consecutive tracklets by exploring the temporal and spatial correlations from previous frame. The second stage performs online linking of the tracklets to generate a face track for each person (better view in color).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/e0377488-07a4-4bb5-8e6d-22db7baf1aa2.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>35. Pose Estimation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>OpenPose</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Stacked Hourglass Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Stacked Hourglass Networks
  </strong>
  are a type of convolutional neural network for pose estimation. They are based on the successive steps of pooling and upsampling that are done to produce a final set of predictions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_12.49.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ZoomNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ZoomNet
  </strong>
  is a 2D human whole-body pose estimation technique. It aims to localize dense landmarks on the entire human body including face, hands, body, and feet. ZoomNet follows the top-down paradigm. Given a human bounding box of each person, ZoomNet first localizes the easy-to-detect body keypoints and estimates the rough position of hands and face. Then it zooms in to focus on the hand/face areas and predicts keypoints using features with higher resolution for accurate localization. Unlike previous approaches which usually assemble multiple networks, ZoomNet has a single network that is end-to-end trainable. It unifies five network heads including the human body pose estimator, hand and face detectors, and hand and face pose estimators into a single network with shared low-level features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_1.51.11_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>FCPose</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FCPose
  </strong>
  is a fully convolutional multi-person
  <a href="https://paperswithcode.com/methods/category/pose-estimation-models">
   pose estimation framework
  </a>
  using dynamic instance-aware convolutions. Different from existing methods, which often require ROI (Region of Interest) operations and/or grouping post-processing, FCPose eliminates the ROIs and grouping pre-processing with dynamic instance aware keypoint estimation heads. The dynamic keypoint heads are conditioned on each instance (person), and can encode the instance concept in the dynamically-generated weights of their filters.
 </p>
 <p>
  Overall, FCPose is built upon the one-stage object detector
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  . The controller that generates the weights of the keypoint heads is attached to the FCOS heads. The weights $\theta_{i}$ generated by the controller is used to fulfill the keypoint head $f$ for the instance $i$. Moreover, a keypoint refinement module is introduced to predict the offsets from each location of the heatmaps to the ground-truth keypoints. Finally, the coordinates derived from the predicted heatmaps are refined by the offsets predicted by the keypoint refinement module, resulting in the final keypoint results. "Rel. coord." is a map of the relative coordinates from all the locations of the feature maps $F$ to the location where the weights are generated. The relative coordinate map is concatenated to $F$ as the input to the keypoint head.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/035974ee-9795-4a24-9bf3-f734a677a290.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>36. Video Model Blocks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CRN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Conditional Relation Network
  </strong>
  , or
  <strong>
   CRN
  </strong>
  , is a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.04.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>IFBlock</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IFBlock
  </strong>
  is a video model block used in the
  <a href="https://paperswithcode.com/method/ifnet">
   IFNet
  </a>
  architecture for video frame interpolation. IFBlocks do not contain expensive operators like cost volume or forward warping and use 3 × 3 convolution and deconvolution as building blocks. Each IFBlock has a feed-forward structure consisting of several convolutional layers and an upsampling operator. Except for the layer that outputs the optical flow residuals and the fusion map,
  <a href="https://paperswithcode.com/method/prelu">
   PReLU
  </a>
  activations are used.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/dcd4d435-6277-4b13-a751-b3103e276655.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>WRQE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Weighted Recurrent Quality Enhancement
  </strong>
  , or
  <strong>
   WRQE
  </strong>
  , is a recurrent quality enhancement network for video compression that takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.28.18_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Sscs</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Sscs
  </strong>
  , or
  <strong>
   Support-set Based Cross-Supervision
  </strong>
  , is a module for video grounding which consists of two main components: a discriminative contrastive objective and a generative caption objective. The contrastive objective aims to learn effective representations by contrastive learning, while the caption objective can train a powerful video encoder supervised by texts. Due to the co-existence of some visual entities in both ground-truth and background intervals, i.e., mutual exclusion, naively contrastive learning is unsuitable to video grounding. This problem is addressed by boosting the cross-supervision with the support-set concept, which collects visual information from the whole video and eliminates the mutual exclusion of entities.
 </p>
 <p>
  Specifically, in the Figure to the right, two video-text pairs { $V_{i}, L_{i}$}, {$V_{j} , L_{j}$ } in the batch are presented for clarity. After feeding them into a video and text encoder, the clip-level and sentence-level embedding ( {$X_{i}, Y_{i}$} and {$X_{j} , Y_{j}$} ) in a shared space are acquired. Base on the support-set module, the weighted average of $X_{i}$ and $X_{j}$ is computed to obtain $\bar{X}_{i}$, $\bar{X}_{j}$ respectively. Finally, the contrastive and caption objectives are combined to pull close the representations of the clips and text from the same samples and push away those from other pairs
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-27_at_10.54.32_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>37. Region Proposal</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>RPN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A
  <strong>
   Region Proposal Network
  </strong>
  , or
  <strong>
   RPN
  </strong>
  , is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. RPN and algorithms like
  <a href="https://paperswithcode.com/method/fast-r-cnn">
   Fast R-CNN
  </a>
  can be merged into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look.
 </p>
 <p>
  RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_12.14.44_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Selective Search</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Selective Search
  </strong>
  is a region proposal algorithm for object detection tasks. It starts by over-segmenting the image based on intensity of the pixels using a graph-based segmentation method by Felzenszwalb and Huttenlocher. Selective Search then takes these oversegments as initial input and performs the following steps
 </p>
 <ol>
  <li>
   Add all bounding boxes corresponding to segmented parts to the list of regional proposals
  </li>
  <li>
   Group adjacent segments based on similarity
  </li>
  <li>
   Go to step 1
  </li>
 </ol>
 <p>
  At each iteration, larger segments are formed and added to the list of region proposals. Hence we create region proposals from smaller segments to larger segments in a bottom-up approach. This is what we mean by computing “hierarchical” segmentations using Felzenszwalb and Huttenlocher’s oversegments.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_11.44.51_AM_cltn2Mh.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CAG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Class activation guide is a module which uses weak localization information from the instrument activation maps to guide the verb and target recognition.
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/2007.05405v1.pdf">
   Nwoye et al.
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-11_at_10.47.27_TGEthQB.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DeepMask</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeepMask
  </strong>
  is an object proposal algorithm based on a convolutional neural network. Given an input image patch, DeepMask generates a class-agnostic mask and an associated score which estimates the likelihood of the patch fully containing a centered object (without any notion of an object category). The core of the model is a ConvNet which jointly predicts the mask and the object score. A large part of the network is shared between those two tasks: only the last few network
layers are specialized for separately outputting a mask and score prediction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_11.58.21_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>38. 3D Face Mesh Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DECA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Detailed Expression Capture and Animation
  </strong>
  , or
  <strong>
   DECA
  </strong>
  , is a model for 3D face reconstruction that is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. A detail-consistency loss is used to disentangle person-specific details and expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_1.01.30_PM_bdhig5b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Attention Mesh</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Attention Mesh
  </strong>
  is a neural network architecture for 3D face mesh prediction that uses attention to semantically meaningful regions. Specifically region-specific heads are employed that transform the feature maps with spatial transformers.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_10.00.19_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>VQ-VAE-2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VQ-VAE-2
  </strong>
  is a type of variational autoencoder that combines a a two-level hierarchical VQ-
  <a href="https://paperswithcode.com/method/vae">
   VAE
  </a>
  with a self-attention autoregressive model (
  <a href="https://paperswithcode.com/method/pixelcnn">
   PixelCNN
  </a>
  ) as a prior. The encoder and decoder architectures are kept simple and light-weight as in the original
  <a href="https://paperswithcode.com/method/vq-vae">
   VQ-VAE
  </a>
  , with the only difference that hierarchical multi-scale latent maps are used for increased resolution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.56.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>39. Unpaired Image-to-Image Translation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CycleGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CycleGAN
  </strong>
  , or
  <strong>
   Cycle-Consistent GAN
  </strong>
  , is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \rightarrow Y$ and $F: Y \rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a
  <a href="https://paperswithcode.com/method/cycle-consistency-loss">
   cycle consistency loss
  </a>
  that encourages $F\left(G\left(x\right)\right) \approx x$ and $G\left(F\left(y\right)\right) \approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.
 </p>
 <p>
  For the mapping $G : X \rightarrow Y$ and its discriminator $D_{Y}$ we have the objective:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) =\mathbb{E}_{y \sim p_{data}\left(y\right)}\left[\log D_{Y}\left(y\right)\right] + \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[log(1 − D_{Y}\left(G\left(x\right)\right)\right] $$
 </p>
 <p>
  where $G$ tries to generate images $G\left(x\right)$ that look similar to images from domain $Y$, while $D_{Y}$ tries to discriminate between translated samples $G\left(x\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \rightarrow X$ and its discriminator $D_{X}$.
 </p>
 <p>
  The Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:
 </p>
 <p>
  $$ \mathcal{L}_{cyc}\left(G, F\right) = \mathbb{E}_{x \sim p_{data}\left(x\right)}\left[||F\left(G\left(x\right)\right) - x||_{1}\right] + \mathbb{E}_{y \sim p_{data}\left(y\right)}\left[||G\left(F\left(y\right)\right) - y||_{1}\right] $$
 </p>
 <p>
  The full objective is:
 </p>
 <p>
  $$ \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) = \mathcal{L}_{GAN}\left(G, D_{Y}, X, Y\right) + \mathcal{L}_{GAN}\left(F, D_{X}, X, Y\right) + \lambda\mathcal{L}_{cyc}\left(G, F\right) $$
 </p>
 <p>
  Where we aim to solve:
 </p>
 <p>
  $$ G^{*}, F^{*} = \arg \min_{G, F} \max_{D_{X}, D_{Y}} \mathcal{L}_{GAN}\left(G, F, D_{X}, D_{Y}\right) $$
 </p>
 <p>
  For the original architecture the authors use:
 </p>
 <ul>
  <li>
   two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\frac{1}{2}$.
  </li>
  <li>
   <a href="https://paperswithcode.com/method/instance-normalization">
    instance normalization
   </a>
  </li>
  <li>
   PatchGANs for the discriminator
  </li>
  <li>
   Least Square Loss for the
   <a href="https://paperswithcode.com/method/gan">
    GAN
   </a>
   objectives.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_3.54.24_PM_aoT8JRU.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ALDA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adversarial-Learned Loss for Domain Adaptation
  </strong>
  is a method for domain adaptation that combines adversarial learning with self-training. Specifically, the domain discriminator has to produce different corrected labels for different domains, while the feature generator aims to confuse the domain discriminator. The adversarial process finally leads to a proper confusion matrix on the target domain. In this way, ALDA takes the strengths of domain-adversarial learning and self-training based methods.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.51.52_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>pixel2style2pixel</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pixel2Style2Pixel
  </strong>
  , or
  <strong>
   pSp
  </strong>
  , is an image-to-image translation framework that is based on a novel encoder that directly generates a series of style vectors which are fed into a pretrained
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  generator, forming the extended $\mathcal{W+}$ latent space. Feature maps are first extracted using a standard feature pyramid over a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  backbone. Then, for each of $18$ target styles, a small mapping network is trained to extract the learned styles from the corresponding feature map, where styles $(0-2)$ are generated from the small feature map, $(3-6)$ from the medium feature map, and $(7-18)$ from the largest feature map. The mapping network, map2style, is a small fully convolutional network, which gradually reduces spatial size using a set of 2-strided convolutions followed by
  <a href="https://paperswithcode.com/method/leaky-relu">
   LeakyReLU
  </a>
  activations. Each generated 512 vector, is fed into
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  , starting from its matching affine transformation, $A$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/ccbdc679-5dba-4457-a621-7f78e2674a22.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>COCO-FUNIT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   COCO-FUNIT
  </strong>
  is few-shot image translation model which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. It builds on top of
  <a href="https://arxiv.org/abs/1905.01723">
   FUNIT
  </a>
  by identifying the content loss problem and then addressing it with a novel content-conditioned style encoder architecture.
 </p>
 <p>
  The FUNIT method suffers from the content loss problem—the translation result is not well-aligned with the input image. While a direct theoretical analysis is likely elusive, we conduct an empirical study, aiming at identify the cause of the content loss problem. In analyses, the authors show that the FUNIT style encoder produces very different style codes using different crops -- suggesting the style code contains other information about the style image such as the object pose.
 </p>
 <p>
  To make the style embedding more robust to small variations in the style image, a new style encoder architecture, the Content-Conditioned style encoder (COCO), is introduced. The most distinctive feature of this new encoder is the conditioning in the content image as illustrated in the top-right of the Figure. Unlike the style encoder in FUNIT, COCO takes both content and style image as input. With this content-conditioning scheme, a direct feedback path is created during learning to let the content image influence how the style code is computed. It also helps reduce the direct influence of the style image to the extract style code.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3e68e5a5-0c75-4755-9258-013b28a5a50d.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        <li>
            <details class="category depth1">
            <summary>Few-Shot Image-to-Image Translation</summary>
            <ul>
                <li class="col-md-12">
                    
                </li>
                
        <li>
            <details class="method-all depth2">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>COCO-FUNIT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   COCO-FUNIT
  </strong>
  is few-shot image translation model which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. It builds on top of
  <a href="https://arxiv.org/abs/1905.01723">
   FUNIT
  </a>
  by identifying the content loss problem and then addressing it with a novel content-conditioned style encoder architecture.
 </p>
 <p>
  The FUNIT method suffers from the content loss problem—the translation result is not well-aligned with the input image. While a direct theoretical analysis is likely elusive, we conduct an empirical study, aiming at identify the cause of the content loss problem. In analyses, the authors show that the FUNIT style encoder produces very different style codes using different crops -- suggesting the style code contains other information about the style image such as the object pose.
 </p>
 <p>
  To make the style embedding more robust to small variations in the style image, a new style encoder architecture, the Content-Conditioned style encoder (COCO), is introduced. The most distinctive feature of this new encoder is the conditioning in the content image as illustrated in the top-right of the Figure. Unlike the style encoder in FUNIT, COCO takes both content and style image as input. With this content-conditioning scheme, a direct feedback path is created during learning to let the content image influence how the style code is computed. It also helps reduce the direct influence of the style image to the extract style code.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3e68e5a5-0c75-4755-9258-013b28a5a50d.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
                
            </ul>
            </details>
        </li>

        
        </ul>
        
        <ul class="parent">
            <p>40. Instance Segmentation Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Blender</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Blender
  </strong>
  is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.
 </p>
 <p>
  The inputs of the blender module are bottom-level bases $\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First
  <a href="https://paperswithcode.com/method/roi-pooling">
   RoIPool
  </a>
  of Mask R-CNN to crop bases with each proposal $\mathbf{p}_{d}$ and then resize the region to a fixed size $R \times R$ feature map $\mathbf{r}_{d}$
 </p>
 <p>
  $$
\mathbf{r}_{d}=\operatorname{RoIPool}_{R \times R}\left(\mathbf{B}, \mathbf{p}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  More specifically,  asampling ratio 1 is used for
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  , i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference,
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  prediction results are used.
 </p>
 <p>
  The attention size $M$ is smaller than $R$. We interpolate $\mathbf{a}_{d}$ from $M \times M$ to $R \times R$, into the shapes of $R=\left(\mathbf{r}_{d} \mid d=1 \ldots D\right)$
 </p>
 <p>
  $$
\mathbf{a}_{d}^{\prime}=\text { interpolate }_{M \times M \rightarrow R \times R}\left(\mathbf{a}_{d}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then $\mathbf{a}_{d}^{\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\mathbf{s}_{d}$.
 </p>
 <p>
  $$
\mathbf{s}_{d}=\operatorname{softmax}\left(\mathbf{a}_{d}^{\prime}\right), \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  Then we apply element-wise product between each entity $\mathbf{r}_{d}, \mathbf{s}_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\mathbf{m}_{d}:$
 </p>
 <p>
  $$
\mathbf{m}_{d}=\sum_{k=1}^{K} \mathbf{s}_{d}^{k} \circ \mathbf{r}_{d}^{k}, \quad \forall d \in{1 \ldots D}
$$
 </p>
 <p>
  where $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8f129739-0f31-4b55-814d-798ea57ef403.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PointRend</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PointRend
  </strong>
  is a module for image segmentation tasks, such as instance and semantic segmentation, that attempts to treat segmentation as image rending problem to efficiently "render" high-quality label maps. It uses a subdivision strategy to adaptively select a non-uniform set of points at which to compute labels. PointRend can be incorporated into popular meta-architectures for both instance segmentation (e.g.
  <a href="https://paperswithcode.com/method/mask-r-cnn">
   Mask R-CNN
  </a>
  ) and semantic segmentation (e.g.
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  ). Its subdivision strategy efficiently computes high-resolution segmentation maps using an order of magnitude fewer floating-point operations than direct, dense computation.
 </p>
 <p>
  PointRend is a general module that admits many possible implementations. Viewed abstractly, a PointRend module accepts one or more typical CNN feature maps $f\left(x_{i}, y_{i}\right)$ that are defined over regular grids, and outputs high-resolution predictions $p\left(x^{'}_{i}, y^{'}_{i}\right)$ over a finer grid. Instead of making excessive predictions over all points on the output grid, PointRend makes predictions only on carefully selected points. To make these predictions, it extracts a point-wise feature representation for the selected points by interpolating $f$, and uses a small point head subnetwork to predict output labels from the point-wise features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/POINTREND_JBHqa6Q.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>bilayer decoupling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PolarMask</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PolarMask
  </strong>
  is an anchor-box free and single-shot instance segmentation method. Specifically, PolarMask takes an image as input and predicts the distance from a sampled positive location (ie a candidate object's center) with respect to the object's contour at each angle, and then assembles the predicted points to produce the final mask. There are several benefits to the system: (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework (2) Two modules are designed (i.e. soft polar centerness and polar IoU loss) to sample high-quality center examples and optimize polar contour regression, making the performance of PolarMask does not depend on the bounding box prediction results and more efficient in training. (3) PolarMask is fully convolutional and can be embedded into most off-the-shelf detection methods.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_11.05.52_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>41. Image Denoising Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>PCA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Principle Components Analysis (PCA)
  </strong>
  is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.26.36_PM_2Nh5OmX.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Noise2Fast</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Noise2Fast
  </strong>
  is a model for single image blind denoising. It is similar to masking based methods -- filling in the pixel gaps -- in that the network is blind to many of the input pixels during training. The method is inspired by Neighbor2Neighbor, where the neural network learns a mapping between adjacent pixels. Noise2Fast is tuned to speed by using a discrete four image training set obtained by a form of downsampling called “checkerboard downsampling.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_10.00.52_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DU-GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DU-GAN
  </strong>
  is a
  <a href="https://www.paperswithcode.com/methods/category/generative-adversarial-networks">
   generative adversarial network
  </a>
  for LDCT denoising in medical imaging. The generator produces denoised LDCT images, and two independent branches with
  <a href="https://paperswithcode.com/method/u-net">
   U-Net
  </a>
  based discriminators perform at the image and gradient domains. The U-Net based discriminator provides both global structure and local per-pixel feedback to the generator. Furthermore, the image discriminator encourages the generator to produce photo-realistic CT images while the gradient discriminator is utilized for better edge and alleviating streak artifacts caused by photon starvation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-01_at_10.00.21_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Lower Bound on Transmission using Non-Linear Bounding Function in Single Image Dehazing</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>42. Face Restoration Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ISPL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Implicit Subspace Prior Learning
  </strong>
  , or
  <strong>
   ISPL
  </strong>
  , is a framework to approach dual-blind face restoration, with two major distinctions from previous restoration methods: 1) Instead of assuming an explicit degradation function between LQ and HQ domain, it establishes an implicit correspondence between both domains via a mutual embedding space, thus avoid solving the pathological inverse problem directly. 2) A subspace prior decomposition and fusion mechanism to dynamically handle inputs at varying degradation levels with consistent high-quality restoration results.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.12.50_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>GFP-GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   GFP-GAN
  </strong>
  is a generative adversarial network for blind face restoration that leverages a generative facial prior (GFP). This Generative Facial Prior (GFP) is incorporated into the face restoration process via channel-split spatial feature transform layers, which allow for a good balance between realness and fidelity. As a whole, the GFP-GAN consists of a degradation removal module (
  <a href="https://paperswithcode.com/method/u-net">
   U-Net
  </a>
  ) and a pretrained face
  <a href="https://paperswithcode.com/method/stylegan">
   StyleGAN
  </a>
  as a facial prior. They are bridged by a latent code mapping and several Channel-Split
  <a href="https://paperswithcode.com/method/spatial-feature-transform">
   Spatial Feature Transform
  </a>
  (CS-SFT) layers. During training, 1) intermediate restoration losses are employed to remove complex degradation, 2) Facial component loss with discriminators is used to enhance facial details, and 3) identity preserving loss is used to retain face identity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/49e78e31-b1ce-4600-ab2f-f65d2d61777f.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DFDNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DFDNet
  </strong>
  , or
  <strong>
   DFDNet
  </strong>
  , is a deep face dictionary network for face restoration to guide the restoration process of degraded observations. Given a LQ image $I_{d}$, the DFDNet selects the dictionary features that have the most similar structure with the input. Specially, we re-norm the whole dictionaries via component AdaIN (termed as CAdaIN) based on the input component to eliminate the distribution or style diversity. The selected dictionary features are then utilized to guide the restoration process via dictionary feature transformation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_10.40.54_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PSFR-GAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PSFR-GAN
  </strong>
  is a semantic-aware style transformation framework for face restoration. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of inputs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_9.58.03_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>43. Generative Training</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Denoising Score Matching</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Training a denoiser on signals gives you a powerful prior over this signal that you can then use to sample examples of this signal.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Informative Sample Mining Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Informative Sample Mining Network
  </strong>
  is a multi-stage sample training scheme for GANs to reduce sample hardness while preserving sample informativeness. Adversarial Importance Weighting is proposed to select informative samples and assign them greater weight. The authors also propose Multi-hop Sample Training to avoid the potential problems in model training caused by sample mining. Based on the principle of divide-and-conquer, the authors produce target images by multiple hops, which means the image translation is decomposed into several separated steps.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.55.31_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ILVR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Iterative Latent Variable Refinement
  </strong>
  , or
  <strong>
   ILVR
  </strong>
  , is a method to guide the generative process in denoising diffusion probabilistic models (DDPMs) to generate high-quality images based on a given reference image. ILVR conditions the generation process in well-performing unconditional DDPM. Each transition in the generation process is refined utilizing a given reference image. By matching each latent variable, ILVR ensures the given condition in each transition thus enables sampling from a conditional distribution. Thus, ILVR generates high-quality images sharing desired semantics.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.50.18_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Safety-llamas</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>44. Image Super-Resolution Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>PULSE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PULSE
  </strong>
  is a self-supervised photo upsampling algorithm. Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the downscaling loss, which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, the authors aim to restrict the search space to guarantee realistic outputs.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.45.56_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ClassSR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ClassSR
  </strong>
  is a framework to accelerate super-resolution (SR) networks on large images (2K-8K). ClassSR combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.53.18_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>45. Scene Text Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Semantic Reasoning Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Semantic reasoning network
  </strong>
  , or
  <strong>
   SRN
  </strong>
  , is an end-to-end trainable framework for scene text recognition that consists of four parts: backbone network, parallel
  <a href="https://paperswithcode.com/method/visual-attention">
   visual attention
  </a>
  module (PVAM), global semantic reasoning module (GSRM), and visual-semantic fusion decoder (VSFD). Given an input image, the backbone network is first used to extract 2D features $V$. Then, the PVAM is used to generate $N$ aligned 1-D features $G$, where each feature corresponds to a character in the text and captures the aligned visual information. These $N$ 1-D features $G$ are then fed into a GSRM to capture the semantic information $S$. Finally, the aligned visual features $G$ and the semantic information $S$ are fused by the VSFD to predict $N$ characters. For text string shorter than $N$, ’EOS’ are padded.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_9.53.55_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ABCNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adaptive Bezier-Curve Network
  </strong>
  , or
  <strong>
   ABCNet
  </strong>
  , is an end-to-end framework for arbitrarily-shaped scene text spotting. It adaptively fits arbitrary-shaped text by a parameterized bezier curve. It also utilizes a feature alignment layer,
  <a href="https://paperswithcode.com/method/bezieralign">
   BezierAlign
  </a>
  , to calculate convolutional features of text instances in curved shapes. These features are then passed to a light-weight recognition head.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8de9f883-eab8-4984-9240-f4e930289762.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PGNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PGNet
  </strong>
  is a point-gathering network for reading arbitrarily-shaped text in real-time. It is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_9.59.19_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>46. Image Restoration Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>NAFNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TLC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  TLC convert the global operation to a local one so that it extract representations based on local spatial region of features as in training phase.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/4eb7df8d-c136-4882-95e8-8fc7f98da08e.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MPRNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MPRNet
  </strong>
  is a multi-stage progressive image restoration architecture that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, the model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, a per-pixel adaptive design is introduced that leverages in-situ supervised attention to reweight the local features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.08.22_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Conffusion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Given a corrupted input image, Con\textit{ffusion}, repurposes a pretrained diffusion model to generate lower and upper bounds around each reconstructed pixel. The true pixel value is guaranteed to fall within these bounds with probability $p$.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>47. 6D Pose Estimation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FFB6D</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FFB6D
  </strong>
  is a full flow bidirectional fusion network for 6D pose estimation of known objects from a single RGBD image. Unlike previous works that extract the RGB and point cloud features independently and fuse them in the final stage, FFB6D builds bidirectional fusion modules as communication bridges in the full flow of the two networks. In this way, the two networks can obtain complementary information from the other and learn representations containing rich appearance and geometry information of the scene.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_3.37.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PixLoc</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PixLoc
  </strong>
  is a scene-agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model. It is based on the direct alignment of multiscale deep features, casting camera localization as metric learning. PixLoc learns strong data priors by end-to-end training from pixels to pose and exhibits exceptional generalization to new scenes by separating model parameters and scene geometry. As the CNN never sees 3D points, PixLoc can generalize to any 3D structure available. This includes sparse SfM point clouds, dense depth maps from stereo or RGBD sensors, meshes, Lidar scans, but also lines and other primitives.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.05.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ARShoe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ARShoe
  </strong>
  is a multi-branch network for pose estimation and segmentation tackling the "try-on" problem for augmented reality shoes. Consisting of an encoder and a decoder, the multi-branch network is trained to predict keypoints
  <a href="https://paperswithcode.com/method/heatmap">
   heatmap
  </a>
  (heatmap),
  <a href="https://paperswithcode.com/method/pafs">
   PAFs
  </a>
  heatmap (pafmap), and segmentation results (segmap) simultaneously. Post processes are then performed for a smooth and realistic virtual try-on.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_3.29.58_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PO3D-VQA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and a deep neural network with 3D generative representations of objects for robust visual scene parsing.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/5b1a61b4-fe81-4dce-a239-a6c53c1d55d0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>48. Action Recognition Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Motion Disentanglement</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  A self-supervised learning method to disentangle irregular (anomalous) motion from regular motion in unlabeled videos.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/c2faa0fb-2f7e-4697-9b14-21c6d0fc1b83.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>TDN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TDN
  </strong>
  , or
  <strong>
   Temporaral Difference Network
  </strong>
  , is an action recognition model that aims to capture multi-scale temporal information. To fully capture temporal information over the entire video, the TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.35.08_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Asynchronous Interaction Aggregation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Asynchronous Interaction Aggregation
  </strong>
  , or
  <strong>
   AIA
  </strong>
  , is a network that leverages different interactions to boost action detection. There are two key designs in it: one is the Interaction Aggregation structure (IA) adopting a uniform paradigm to model and integrate multiple types of interaction; the other is the Asynchronous Memory Update algorithm (AMU) that enables us to achieve better performance by modeling very long-term interaction dynamically.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_9.46.45_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HalluciNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Approximating Spatiotemporal Representations Using a 2DCNN
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6df40768-e7a3-4d60-a976-477889cb9be7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>49. Multi-Scale Training</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SNIP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SNIP
  </strong>
  , or
  <strong>
   Scale Normalization for Image Pyramids
  </strong>
  , is a multi-scale training scheme that selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. SNIP is a modified version of MST where only the object instances that have a resolution close to the pre-training dataset, which is typically 224x224, are used for training the detector. In multi-scale training (MST), each image is observed at different resolutions therefore, at a high resolution (like 1400x2000) large objects are hard to classify and at a low resolution (like 480x800) small objects are hard to classify. Fortunately, each object instance appears at several different scales and some of those appearances fall in the desired scale range. In order to eliminate extreme scale objects, either too large or too small, training is only performed on objects that fall in the desired scale range and the remainder are simply ignored during back-propagation. Effectively, SNIP uses all the object instances during training, which helps capture all the variations in appearance and
pose, while reducing the domain-shift in the scale-space for the pre-trained network.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_9.22.44_PM_f74I61X.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>SNIPER</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SNIPER
  </strong>
  is a multi-scale training approach for instance-level recognition tasks like object detection and instance-level segmentation. Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips). This can help to speed up multi-scale training as it operates on low-resolution chips. Due to its memory-efficient design, SNIPER can benefit from
  <a href="https://paperswithcode.com/method/batch-normalization">
   Batch Normalization
  </a>
  during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_2.35.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>50. Video-Text Retrieval Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VLG-Net</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  VLG-Net leverages recent advantages in Graph Neural Networks (GCNs) and leverages a novel multi-modality graph-based fusion method for the task of natural language video grounding.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CoVR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The composed video retrieval (CoVR) task is a new task, where the goal is to find a video that matches both a query image and a query text. The query image represents a visual concept that the user is interested in, and the query text specifies how the concept should be modified or refined. For example, given an image of a fountain and the text
  <em>
   during show at night
  </em>
  , the CoVR task is to retrieve a video that shows the fountain at night with a show.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>CAMoE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   CAMoE
  </strong>
  is a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (MoE) for video-text retrieval. The CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video representations, including action, entity, scene, etc., then align them with the corresponding part of the text. A
  <a href="https://paperswithcode.com/method/dual-softmax-loss">
   Dual Softmax Loss
  </a>
  (DSL) is used to avoid the one-way optimum-match which occurs in previous contrastive methods. Introducing the intrinsic prior of each pair in a batch, DSL serves as a reviser to correct the similarity matrix and achieves the dual optimal match.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.35.55_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ReGaDa</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>51. Localization Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Fragmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Given a pattern $P,$ that is more complicated than the patterns, we fragment $P$ into simpler patterns such that their exact count is known. In the subgraph GNN proposed earlier, look into the subgraph of the host graph. We have seen that this technique is scalable on large graphs. Also, we have seen that subgraph GNN is more expressive and efficient than traditional GNN. So, we tried to explore the expressibility when the pattern is fragmented into smaller subpatterns.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ORB-SLAM2</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  ORB-SLAM2 is a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city.
 </p>
 <p>
  Source:
  <a href="https://arxiv.org/pdf/1610.06475v2.pdf">
   Mur-Artal and Tardos
  </a>
 </p>
 <p>
  Image source:
  <a href="https://arxiv.org/pdf/1610.06475v2.pdf">
   Mur-Artal and Tardos
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screenshot_2021-02-11_at_14.31.51_98vTzPI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>IoU-Net</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IoU-Net
  </strong>
  is an object detection architecture that introduces localization confidence. IoU-Net learns to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-14_at_4.26.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>52. Adversarial Image Data Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DiffAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Differentiable Augmentation (DiffAugment)
  </strong>
  is a set of differentiable image transformations used to augment data during
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  training. The transformations are applied to the real and generated images. It enables the gradients to be propagated through the augmentation back to the generator, regularizes
the discriminator without manipulating the target distribution, and maintains the balance of training
dynamics. Three choices of transformation are preferred by the authors in their experiments: Translation,
  <a href="https://paperswithcode.com/method/cutout">
   CutOut
  </a>
  , and Color.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/diffaugment_UH9jHYa.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Adversarial Color Enhancement</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Adversarial Color Enhancement
  </strong>
  is an approach to generating unrestricted adversarial images by optimizing a color filter via gradient descent.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.48.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MaxUp</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MaxUp
  </strong>
  is an adversarial data augmentation technique for improving the generalization performance of machine learning models. The idea is to generate a set of augmented data with some random perturbations or transforms, and minimize the maximum, or worst case loss over the augmented data.  By doing so, we implicitly introduce a smoothness or robustness regularization against the random perturbations, and hence improve the generation performance.  For example, in the case of Gaussian perturbation, MaxUp is asymptotically equivalent to using the gradient norm of the loss as a penalty to encourage smoothness.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_1.18.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>53. Medical Image Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>UNETR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   UNETR
  </strong>
  , or
  <strong>
   UNet Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based architecture for
  <a href="https://paperswithcode.com/task/medical-image-segmentation">
   medical image segmentation
  </a>
  that utilizes a pure
  <a href="https://paperswithcode.com/method/transformer">
   transformer
  </a>
  as the encoder to learn sequence representations of the input volume -- effectively capturing the global multi-scale information. The transformer encoder is directly connected to a decoder via
  <a href="https://paperswithcode.com/methods/category/skip-connections">
   skip connections
  </a>
  at different resolutions like a
  <a href="https://paperswithcode.com/method/u-net">
   U-Net
  </a>
  to compute the final semantic segmentation output.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/19c387ec-df70-4d60-847f-331910aa384f.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Co-Correcting</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Co-Correcting
  </strong>
  is a noise-tolerant deep learning framework for medical image classification based on mutual learning and annotation correction. It consists of three modules: the dual-network architecture, the curriculum learning module, and the label correction module.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_2.04.25_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BS-Net</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BS-Net
  </strong>
  is an architecture for COVID-19 severity prediction based on clinical data from different modalities. The architecture comprises 1) a shared multi-task feature extraction backbone, 2) a lung segmentation branch, 3) an original registration mechanism that acts as a ”multi-resolution feature alignment” block operating on the encoding backbone , and 4) a multi-regional classification part for the final six-valued score estimation.
 </p>
 <p>
  All these blocks act together in the final training thanks to a loss specifically crated for this task. This loss guarantees also performance robustness, comprising a differentiable version of the target discrete metric. The learning phase operates in a weakly-supervised fashion. This is due to the fact that difficulties and pitfalls in the visual interpretation of the disease signs on CXRs (spanning from subtle findings to heavy lung impairment), and the lack of detailed localization information, produces unavoidable inter-rater variability among radiologists in assigning scores.
 </p>
 <p>
  Specifically the architectural details are:
 </p>
 <ul>
  <li>
   The input image is processed with a convolutional backbone; the authors opt for a
   <a href="https://paperswithcode.com/method/resnet">
    ResNet
   </a>
   -18.
  </li>
  <li>
   Segmentation is performed by a nested version of
   <a href="https://paperswithcode.com/method/u-net">
    U-Net
   </a>
   (U-Net++).
  </li>
  <li>
   Alignment is estimated through the segmentation probability map produced by the U-Net++ decoder, which is achieved through a
   <a href="https://paperswithcode.com/method/spatial-transformer">
    spatial transformer network
   </a>
   -- able to estimate the spatial transform matrix in order to center, rotate, and correctly zoom the lungs. After alignment at various scales, features are forward to a
   <a href="https://paperswithcode.com/method/roi-pooling">
    ROIPool
   </a>
   .
  </li>
  <li>
   The alignment block is pre-trained on the synthetic alignment dataset in a weakly-supervised setting, using a Dice loss.
  </li>
  <li>
   The scoring head uses
   <a href="https://paperswithcode.com/method/fpn">
    FPNs
   </a>
   for the combination of multi-scale feature maps. The multiresolution feature aligner produces input feature maps that are well focused on the specific area of interest. Eventually, the output of the FPN layer flows in a series of convolutional blocks to retrieve the output map. The classification is performed by a final
   <a href="https://paperswithcode.com/method/global-average-pooling">
    Global Average Pooling
   </a>
   layer and a
   <a href="https://paperswithcode.com/method/softmax">
    SoftMax
   </a>
   activation.
  </li>
  <li>
   The Loss function used for training is a sparse categorical cross entropy (SCCE) with a (differentiable) mean absolute error contribution.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/820af4ec-31e7-4158-a101-d4ac0030b0cd.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>54. Image Quality Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DKL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MUSIQ</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MUSIQ
  </strong>
  , or
  <strong>
   Multi-scale Image Quality Transformer
  </strong>
  , is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based model for multi-scale image quality assessment. It processes native resolution images with varying sizes and aspect ratios. In MUSIQ, we construct a multi-scale image representation as input, including the native resolution image and its ARP resized variants.  Each image is split into fixed-size patches which are embedded by a patch encoding module (blue boxes). To capture 2D structure of the image and handle images of varying aspect ratios, the spatial embedding is encoded by hashing the patch position $(i,j)$ to $(t_{i},t_{j})$ within a grid of learnable embeddings (red boxes). Scale Embedding (green boxes) is introduced to capture scale information. The Transformer encoder takes the input tokens and performs multi-head self-attention. To predict the image quality, MUSIQ follows a common strategy in Transformers to add an [CLS] token to the sequence to represent the whole multi-scale input and the corresponding Transformer output is used as the final representation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.32.56_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>55. Video Object Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VOS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VOS
  </strong>
  is a type of video object segmentation model consisting of two network components. The target appearance model consists of a light-weight module, which is learned during the inference stage using fast optimization techniques to predict a coarse but robust target segmentation. The segmentation model is exclusively trained offline, designed to process the coarse scores into high quality segmentation masks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.58.16_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>State-Aware Tracker</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   State-Aware Tracker
  </strong>
  is a pipeline for semi-supervised video object segmentation. It takes each target object as a tracklet, which not only makes the pipeline more efficient but also filters distractors to facilitate target modeling. For more stable and robust performance over video sequences, SAT gets awareness for each state and makes self-adaptation via two feedback loops. One loop assists SAT in generating more stable tracklets. The other loop helps to construct a more robust and holistic target representation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.28.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MiVOS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MiVOS
  </strong>
  is a video object segmentation model which decouples interaction-to-mask and mask propagation. By decoupling interaction from propagation, MiVOS is versatile and not limited by the type of interactions. It uses three modules: Interaction-to-Mask, Propagation and Difference-Aware Fusion. Trained separately, the interaction module converts user interactions to an object mask, which is then temporally propagated by our propagation module using a novel top-filtering strategy in reading the space-time memory. To effectively take the user's intent into account, a novel difference-aware module is proposed to learn how to properly fuse the masks before and after each interaction, which are aligned with the target frames by employing the space-time memory.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.11.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>56. Video Recognition Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>3D ResNet-RS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   3D ResNet-RS
  </strong>
  is an architecture and scaling strategy for 3D ResNets for video recognition. The key additions are:
 </p>
 <ul>
  <li>
   <p>
    <strong>
     3D ResNet-D stem
    </strong>
    : The
    <a href="https://paperswithcode.com/method/resnet-d">
     ResNet-D
    </a>
    stem is adapted to 3D inputs by using three consecutive
    <a href="https://paperswithcode.com/method/3d-convolution">
     3D convolutional layers
    </a>
    . The first convolutional layer employs a temporal kernel size of 5 while the remaining two convolutional layers employ a temporal kernel size of 1.
   </p>
  </li>
  <li>
   <p>
    <strong>
     3D Squeeze-and-Excitation
    </strong>
    :
    <a href="https://paperswithcode.com/method/squeeze-and-excitation-block">
     Squeeze-and-Excite
    </a>
    is adapted to spatio-temporal inputs by using a 3D
    <a href="https://paperswithcode.com/method/global-average-pooling">
     global average pooling
    </a>
    operation for the squeeze operation. A SE ratio of 0.25 is applied in each 3D bottleneck block for all experiments.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Self-gating
    </strong>
    : A self-gating module is used in each 3D bottleneck block after the SE module.
   </p>
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_4.08.49_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>MoViNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Mobile Video Network
  </strong>
  , or
  <strong>
   MoViNet
  </strong>
  , is a type of computation and memory efficient video network that can operate on streaming video for online inference. Three techniques are used to improve efficiency while reducing the peak memory usage of 3D CNNs. First, a video network search space is designed and
  <a href="https://paperswithcode.com/method/neural-architecture-search">
   neural architecture search
  </a>
  employed to generate efficient and diverse 3D CNN architectures. Second, a Stream Buffer technique is introduced that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, a simple ensembling technique is used to improve accuracy further without sacrificing efficiency.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_12.05.06_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>AVSlowFast</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Audiovisual SlowFast Network
  </strong>
  , or
  <strong>
   AVSlowFast
  </strong>
  , is an architecture for integrated audiovisual perception. AVSlowFast has Slow and Fast visual pathways that are integrated with a Faster Audio pathway to model vision and sound in a unified representation. Audio and visual features are fused at multiple layers, enabling audio to contribute to the formation of hierarchical audiovisual concepts. To overcome training difficulties that arise from different learning dynamics for audio and visual modalities,
  <a href="https://paperswithcode.com/method/droppathway">
   DropPathway
  </a>
  is used, which randomly drops the Audio pathway during training as an effective regularization technique. Inspired by prior studies in neuroscience, hierarchical audiovisual synchronization is performed to learn joint audiovisual features.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8d604811-746f-450c-8c2e-d326f78c97cf.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>57. Image Retrieval Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>RFE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DELG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DELG
  </strong>
  is a convolutional neural network for image retrieval that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads – requiring only image-level labels. This allows for efficient inference by extracting an image’s global feature, detected keypoints and local descriptors within a single model.
 </p>
 <p>
  The model is enabled by leveraging hierarchical image representations that arise in
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   CNNs
  </a>
  , which are coupled to
  <a href="https://paperswithcode.com/method/generalized-mean-pooling">
   generalized mean pooling
  </a>
  and attentive local feature detection. Secondly, a convolutional autoencoder module is adopted that can successfully learn low-dimensional local descriptors. This can be readily integrated into the unified model, and avoids the need of post-processing learning steps, such as
  <a href="https://paperswithcode.com/method/pca">
   PCA
  </a>
  , that are commonly used. Finally, a procedure is used that enables end-to-end training of the proposed model using only image-level supervision. This requires carefully controlling the gradient flow between the global and local network heads during backpropagation, to avoid disrupting the desired representations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_9.44.51_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>DOLG</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Image Retrieval is a fundamental task of obtaining images similar to the query one from a database. A common image retrieval practice is to firstly retrieve candidate images via similarity search using global image features and then re-rank the candidates by leveraging their
local features. Previous learning-based studies mainly focus on either global or local image representation learning
to tackle the retrieval task. In this paper, we abandon the
two-stage paradigm and seek to design an effective singlestage solution by integrating local and global information
inside images into compact image representations. Specifically, we propose a Deep Orthogonal Local and Global
(DOLG) information fusion framework for end-to-end image retrieval. It attentively extracts representative local information with multi-atrous convolutions and self-attention
at first. Components orthogonal to the global image representation are then extracted from the local information.
At last, the orthogonal components are concatenated with
the global representation as a complementary, and then aggregation is performed to generate the final representation.
The whole framework is end-to-end differentiable and can
be trained with image-level labels. Extensive experimental
results validate the effectiveness of our solution and show
that our model achieves state-of-the-art image retrieval performances on Revisited Oxford and Paris datasets.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6af20902-a339-44de-847e-0c5622dcf099.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>58. Anchor Supervision</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FreeAnchor</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FreeAnchor
  </strong>
  is an anchor supervision method for object detection. Many CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In contrast, FreeAnchor is a learning-to-match approach that breaks the IoU restriction, allowing objects to match anchors in a flexible manner. It updates hand-crafted anchor assignment to free anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/new_3Dpipeline.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>59. Conditional Image-to-Image Translation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Pix2Pix</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Pix2Pix
  </strong>
  is a conditional image-to-image translation architecture that uses a conditional
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  objective combined with a reconstruction loss. The conditional GAN objective for observed images $x$, output images $y$ and the random noise vector $z$ is:
 </p>
 <p>
  $$ \mathcal{L}_{cGAN}\left(G, D\right) =\mathbb{E}_{x,y}\left[\log D\left(x, y\right)\right]+
\mathbb{E}_{x,z}\left[log(1 − D\left(x, G\left(x, z\right)\right)\right] $$
 </p>
 <p>
  We augment this with a reconstruction term:
 </p>
 <p>
  $$ \mathcal{L}_{L1}\left(G\right) = \mathbb{E}_{x,y,z}\left[||y - G\left(x, z\right)||_{1}\right] $$
 </p>
 <p>
  and we get the final objective as:
 </p>
 <p>
  $$ G^{*} = \arg\min_{G}\max_{D}\mathcal{L}_{cGAN}\left(G, D\right) + \lambda\mathcal{L}_{L1}\left(G\right) $$
 </p>
 <p>
  The architectures employed for the generator and discriminator closely follow
  <a href="https://paperswithcode.com/method/dcgan">
   DCGAN
  </a>
  , with a few modifications:
 </p>
 <ul>
  <li>
   Concatenated skip connections are used to "shuttle" low-level information between the input and output, similar to a
   <a href="https://paperswithcode.com/method/u-net">
    U-Net
   </a>
   .
  </li>
  <li>
   The use of a
   <a href="https://paperswithcode.com/method/patchgan">
    PatchGAN
   </a>
   discriminator that only penalizes structure at the scale of patches.
  </li>
 </ul>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-05_at_12.37.47_PM_dZrgNzj.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>OASIS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  OASIS is a
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  -based model to translate semantic label maps into realistic-looking images. The model builds on preceding work such as
  <a href="https://paperswithcode.com/method/pix2pix">
   Pix2Pix
  </a>
  and SPADE. OASIS introduces the following innovations:
 </p>
 <ol>
  <li>
   <p>
    The method is not dependent on the perceptual loss, which is commonly used for the semantic image synthesis task. A
    <a href="https://paperswithcode.com/method/vgg">
     VGG
    </a>
    network trained on ImageNet is routinely employed as the perceptual loss to strongly improve the synthesis quality. The authors show that this perceptual loss also has negative effects: First, it reduces the diversity of the generated images. Second, it negatively influences the color distribution to be more biased towards ImageNet. OASIS eliminates the dependence on the perceptual loss by changing the common discriminator design: The OASIS discriminator segments an image into one of the real classes or an additional fake class. In doing so, it makes more efficient use of the label maps that the discriminator normally receives. This distinguishes the discriminator from the commonly used encoder-shaped discriminators, which concatenate the label maps to the input image and predict a single score per image. With the more fine-grained supervision through the loss of the OASIS discriminator, the perceptual loss is shown to become unnecessary.
   </p>
  </li>
  <li>
   <p>
    A user can generate a diverse set of images per label map by simply resampling noise. This is achieved by conditioning the
    <a href="https://arxiv.org/abs/1903.07291">
     spatially-adaptive denormalization
    </a>
    module in each layer of the GAN generator directly on spatially replicated input noise. A side effect of this conditioning is that at inference time an image can be resampled either globally or locally (either the complete image changes or a restricted region in the image).
   </p>
  </li>
 </ol>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/oasis_HhBOKx6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>LipGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LipGAN
  </strong>
  is a generative adversarial network for generating realistic talking faces conditioned on translated speech. It employs an adversary that measures the extent of lip synchronization in the frames generated by the generator. The system is capable of handling faces in random poses without the need for realignment to a template pose. LipGAN is a fully self-supervised approach that learns a phoneme-viseme mapping, making it language independent.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.25.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>60. Stereo Depth Estimation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Spatial Propagation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Inspired by the spatial propagation mechanism utilized in the depth completion task \cite{NLSPN}, we introduce a normal incorporated non-local disparity propagation module in which we hub NDP to generate non-local affinities and offsets for spatial propagation at the disparity level. The motivation lies that the sampled pixels for edges and occluded regions are supposed to be selected. The propagation process aggregates disparities via plane affinity relations, which alleviates the phenomenon of disparity blurring at object edges due to frontal parallel windows. And the disparities in occluded areas are also optimized at the same time by being propagated from non-occluded areas where the predicted disparities are with high confidence.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/21016c6c-ee3e-4367-935e-3a7659d5ee68.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Bi3D</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Bi3D
  </strong>
  is a stereo depth estimation framework that estimates depth via a series of binary classifications. Rather than testing if objects are at a particular depth
  <em>
   D
  </em>
  , as existing stereo methods do, it classifies them as being closer or farther than
  <em>
   D
  </em>
  . It takes the stereo pair and a disparity $d_{i}$ and produces a confidence map, which can be thresholded to yield the binary segmentation. To estimate depth on $N + 1$ quantization levels we run this network $N$ times and maximize the probability in Equation 8 (see paper). To estimate continuous depth, whether full or selective, we run the
  <a href="https://paperswithcode.com/method/segnet">
   SegNet
  </a>
  block of Bi3DNet for each disparity level and work directly on the confidence volume.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.47.13_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>HITNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   HITNet
  </strong>
  is a framework for neural network based depth estimation which overcomes the computational disadvantages of operating on a 3D volume by integrating image warping, spatial propagation and a fast high resolution initialization step into the network architecture, while keeping the flexibility of a learned representation by allowing features to flow through the network. The main idea of the approach is to represent image tiles as planar patches which have a learned compact feature descriptor attached to them. The basic principle of the approach is to fuse information from the high resolution initialization and the current hypotheses using spatial propagation. The propagation is implemented via a
  <a href="https://paperswithcode.com/methods/category/convolutional-neural-networks">
   convolutional neural network
  </a>
  module that updates the estimate of the planar patches and their attached features.
 </p>
 <p>
  In order for the network to iteratively increase the accuracy of the disparity predictions, the network is provided a local cost volume in a narrow band (±1 disparity) around the planar patch using in-network image warping allowing the network to minimize image dissimilarity. To reconstruct fine details while also capturing large texture-less areas we start at low resolution and hierarchically upsample predictions to higher resolution. A critical feature of the architecture is that at each resolution, matches from the initialization module are provided to facilitate recovery of thin structures that cannot be represented at low resolution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/22c9e1f6-aad0-42c5-90e4-1a8ac355cd1f.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>61. Layout Annotation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>BoundaryNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BoundaryNet
  </strong>
  is a resizing-free approach for layout annotation. The variable-sized user selected region of interest is first processed by an attention-guided skip network. The network optimization is guided via Fast Marching distance maps to obtain a good quality initial boundary estimate and an associated feature representation. These outputs are processed by a Residual Graph
  <a href="https://paperswithcode.com/method/convolution">
   Convolution
  </a>
  Network optimized using Hausdorff loss to obtain the final region boundary.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-25_at_9.46.35_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>62. Video Sampling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Temporal Jittering</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Temporal Jittering
  </strong>
  is a method used in deep learning for video, where we sample multiple training clips from each video with random start times during at every epoch.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>63. Meshing</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>NICE-SLAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  NICE-SLAM, a dense RGB-D SLAM system that combines neural implicit decoders with hierarchical grid-based representations, which can be applied to large-scale scenes.
 </p>
 <p>
  Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/de7409fc-3ba3-4fe5-8c4f-f597722814e1.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>NeuralRecon</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   NeuralRecon
  </strong>
  is a framework for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, NeuralRecon proposes to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.52.57_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>64. Downsampling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SMOTE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Perhaps the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique, or SMOTE for short. This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled “SMOTE: Synthetic Minority Over-sampling Technique.”
 </p>
 <p>
  SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Anti-Alias Downsampling</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Anti-Alias Downsampling (AA)
  </strong>
  aims to improve the shift-equivariance of deep networks. Max-pooling is inherently composed of two operations. The first operation is to densely evaluate the max operator and second operation is naive subsampling. AA is proposed as a low-pass filter between them to achieve practical anti-aliasing in any existing strided layer such as strided
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  . The smoothing factor can be adjusted by changing the blur kernel filter size, where a larger filter size results in increased blur.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-08_at_5.50.19_PM_xc2QEMD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>65. Whitening</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ZCA Whitening</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ZCA Whitening
  </strong>
  is an image preprocessing method that leads to a transformation of data such that the covariance matrix $\Sigma$ is the identity matrix, leading to decorrelated features.
 </p>
 <p>
  Image Source:
  <a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">
   Alex Krizhevsky
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_4.26.36_PM_RRByEKN.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PCA Whitening</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PCA Whitening
  </strong>
  is a processing step for image based data that makes input less redundant. Adjacent pixel or feature values can be highly correlated, and whitening through the use of
  <a href="https://paperswithcode.com/method/pca">
   PCA
  </a>
  reduces this degree of correlation.
 </p>
 <p>
  Image Source:
  <a href="https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg">
   Wikipedia
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_8.26.36_PM_2Nh5OmX_ateYpGD.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>66. Reversible Image Conversion Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>IICNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Invertible Image Conversion Net
  </strong>
  , or
  <strong>
   IICNet
  </strong>
  , is a generic framework for reversible image conversion tasks. Unlike previous encoder-decoder based methods, IICNet maintains a highly invertible structure based on invertible neural networks (INNs) to better preserve the information during conversion. It uses a relation module and a channel squeeze layer to improve the INN nonlinearity to extract cross-image relations and the network flexibility, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_8.12.02_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RevSilo</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Invertible multi-input multi-output coupling module. In RevBiFPN it is used as a bidirectional multi-scale feature pyramid fusion module that is invertible.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/5590576f-33ee-4566-8e95-c60e5fc95dc8.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>67. Counting Methods</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>KNN and IOU based verification</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   KNN and IoU-based Verification
  </strong>
  is used to verify detections and choose between multiple detections of the same underlying object. It was originally used within the context of blood cell counting in medical images. To avoid this double counting problem, the KNN algorithm is applied in each platelet to determine its closest platelet and then using the intersection of union (IOU) between two platelets we calculate their extent of overlap. The authors allow 10% of the overlap between platelet and its closest platelet based on empirical observations. If the overlap is larger than that, they ignore that cell as a double count to get rid of spurious counting.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-31_at_11.37.55_AM_FCJgCSG.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>EBC</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Traditional methods are based on block-wise regression. This framework, Enhanced Blockwise Classification (
  <strong>
   EBC
  </strong>
  ), however, is based on the idea that aims to classify the count value within each block into several pre-defined bins. The enhancement comes from 3 aspects: discretization policy, label correction and loss function.
 </p>
 <p>
  Notice that the original block-wise classification concept was introduced by Liu
  <em>
   et al.
  </em>
  in
  <em>
   Counting Objects by Blockwise Classification
  </em>
  .
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/bbcc4564-4c3f-4311-a703-8c04aa405523.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>68. Style Transfer Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Revision Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Revision Network
  </strong>
  is a style transfer module that aims to revise the rough stylized image via generating residual details image $r_{c s}$, while the final stylized image is generated by combining $r_{c s}$ and rough stylized image $\bar{x}_{c s}$. This procedure ensures that the distribution of global style pattern in $\bar{x}_{c s}$ is properly kept. Meanwhile, learning to revise local style patterns with residual details image is easier for the Revision Network.
 </p>
 <p>
  As shown in the Figure, the Revision Network is designed as a simple yet effective encoder-decoder architecture, with only one down-sampling and one up-sampling layer. Further, a
  <a href="https://paperswithcode.com/method/patchgan">
   patch discriminator
  </a>
  is used to help Revision Network to capture fine patch textures under adversarial learning setting. The patch discriminator $D$ is defined following SinGAN, where $D$ owns 5 convolution layers and 32 hidden channels. A relatively shallow $D$ is chosen to (1) avoid overfitting since we only have one style image and (2) control the receptive field to ensure D can only capture local patterns.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/290a871b-058e-48dc-ae1e-a4614ad8545d.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Drafting Network</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Drafting Network
  </strong>
  is a style transfer module designed to transfer global style patterns in low-resolution, since global patterns can be transferred easier in low resolution due to larger receptive field and less local details. To achieve single style transfer, earlier work trained an encoder-decoder module, where only the content image is used as input. To better combine the style feature and the content feature, the Drafting Network adopts the
  <a href="https://paperswithcode.com/method/adaptive-instance-normalization">
   AdaIN module
  </a>
  .
 </p>
 <p>
  The architecture of Drafting Network is shown in the Figure, which includes an encoder, several AdaIN modules and a decoder. (1) The encoder is a pre-trained
  <a href="https://paperswithcode.com/method/vgg">
   VGG
  </a>
  -19 network, which is fixed during training. Given $\bar{x}_{c}$ and $\bar{x}_{s}$, the VGG encoder extracts features in multiple granularity at 2_1, 3_1 and 4_1 layers. (2) Then, we apply feature modulation between the content and style feature using AdaIN modules after 2_1, 3_1 and 4_1 layers, respectively. (3) Finally, in each granularity of decoder, the corresponding feature from the AdaIN module is merged via a
  <a href="https://paperswithcode.com/methods/category/skip-connections">
   skip-connection
  </a>
  . Here, skip-connections after AdaIN modules in both low and high levels are leveraged to help to reserve content structure, especially for low-resolution image.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/0fc96e91-bdfd-448b-bb00-663ae8bad3a4.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>69. Explainable CNNs</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>XGrad-CAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   XGrad-CAM
  </strong>
  , or
  <strong>
   Axiom-based Grad-CAM
  </strong>
  , is a class-discriminative visualization method and able to highlight the regions belonging to the objects of interest. Two axiomatic properties are introduced in the derivation of XGrad-CAM: Sensitivity and Conservation. In particular, the proposed XGrad-CAM is still a linear combination of feature maps, but able to meet the constraints of those two axioms.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/dd114463-f467-4156-b271-848369319c1b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PolyCAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>70. Video Frame Interpolation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>IFNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   IFNet
  </strong>
  is an architecture for video frame interpolation that adopts a coarse-to-fine strategy with progressively increased resolutions: it iteratively updates intermediate flows and soft fusion mask via successive
  <a href="https://paperswithcode.com/method/ifblock">
   IFBlocks
  </a>
  . Conceptually, according to the iteratively updated flow fields, we can move corresponding pixels from two input frames to the same location in a latent intermediate frame and use a fusion mask to combine pixels from two input frames. Unlike most previous optical flow models, IFBlocks do not contain expensive operators like cost volume or forward warping and use 3 × 3
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  and deconvolution as building blocks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/60022485-3c99-4a3a-a51a-b96973344ac6.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>RIFE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   RIFE
  </strong>
  , or
  <strong>
   Real-time Intermediate Flow Estimation
  </strong>
  is an intermediate flow estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named
  <a href="https://paperswithcode.com/method/ifnet">
   IFNet
  </a>
  that can directly estimate the intermediate flows from coarse-to-fine with much better speed. It introduces a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement.
 </p>
 <p>
  In RIFE training, given two input frames $I_{0}, I_{1}$, we directly feed them into the IFNet to approximate intermediate flows $F_{t \rightarrow 0}, F_{t \rightarrow 1}$ and the fusion map $M$. During training phase, a privileged teacher refines student's results to get $F_{t \rightarrow 0}^{T e a}, F_{t \rightarrow 1}^{T e a}$ and $M^{\text {Tea }}$ based on ground truth $I_{t}$. The student model and the teacher model are jointly trained from scratch using the reconstruction loss. The teacher's approximations are more accurate so that they can guide the student to learn.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a6e66b34-2000-4efd-8447-a7aa06cbda2b.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>71. Portrait Matting Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MODNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MODNet
  </strong>
  is a light-weight matting objective decomposition network that can process portrait matting from a single input image in real time. The design of MODNet benefits from optimizing a series of correlated sub-objectives simultaneously via explicit constraints. To overcome the domain shift problem, MODNet introduces a self-supervised strategy based on subobjective consistency (SOC) and  a one-frame delay trick to smooth the results when applying MODNet to portrait video sequence.
 </p>
 <p>
  Given an input image $I$, MODNet predicts human semantics $s_{p}$, boundary details $d_{p}$, and final alpha matte $\alpha_{p}$ through three interdependent branches, $S, D$, and $F$, which are constrained by specific supervisions generated from the ground truth matte $\alpha_{g}$. Since the decomposed sub-objectives are correlated and help strengthen each other, we can optimize MODNet end-to-end.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/d6adc97f-aae1-4bd5-8b30-93f754bf0038.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>STATEGAME MAINTAIN PICTURE BALANCED PLAY STABLE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>72. Face Privacy</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Fawkes</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fawkes
  </strong>
  is an image cloaking system that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes ("cloaks") to their own photos before releasing them. When used to train facial recognition models, these "cloaked" images produce functional models that consistently cause normal images of the user to be misidentified.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_12.55.46_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PrivacyNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PrivacyNet
  </strong>
  is a
  <a href="https://paperswithcode.com/method/gan">
   GAN
  </a>
  -based semi-adversarial network (SAN) that modifies an input face image such that it can be used by a face matcher for matching purposes but cannot be reliably used by an attribute classifier. PrivacyNet allows a person to choose specific attributes that have to be obfuscated in the input face images (e.g., age and race), while allowing for other types of attributes to be extracted (e.g., gender).
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_10.57.02_AM_GQIN0cu.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>73. Face-to-Face Translation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>LipGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LipGAN
  </strong>
  is a generative adversarial network for generating realistic talking faces conditioned on translated speech. It employs an adversary that measures the extent of lip synchronization in the frames generated by the generator. The system is capable of handling faces in random poses without the need for realignment to a template pose. LipGAN is a fully self-supervised approach that learns a phoneme-viseme mapping, making it language independent.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.25.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>74. Point Cloud Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>PointAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PointAugment
  </strong>
  is a an auto-augmentation framework that automatically optimizes and augments point cloud samples to enrich the data diversity when we train a classification network. Different from existing auto-augmentation methods for 2D images, PointAugment is sample-aware and takes an adversarial learning strategy to jointly optimize an augmentor network and a classifier network, such that the augmentor can learn to produce augmented samples that best fit the classifier.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_4.39.17_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PatchAugment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Recent deep neural network models trained on smaller and less diverse datasets use data augmentation to alleviate limitations such as overfitting, reduced robustness, and lower generalization. Methods using 3D datasets are among the most common to use data augmentation techniques such as random point drop, scaling, translation, rotations, and jittering. However, these data augmentation techniques are fixed and are often applied to the entire object, ignoring the object’s local geometry. Different local neighborhoods on the object surface hold a different amount of geometric complexity. Applying the same data augmentation techniques at the object level is less effective in augmenting local neighborhoods with complex structures. This paper presents PatchAugment, a data augmentation framework to apply different augmentation techniques to the local neighborhoods. Our experimental studies on PointNet++ and DGCNN models demonstrate the effectiveness of PatchAugment on the task of 3D Point Cloud Classification. We evaluated our technique against these models using four benchmark datasets, ModelNet40 (synthetic), ModelNet10 (synthetic), SHREC’16 (synthetic), and ScanObjectNN (real-world).
 </p>
 <p>
  <a href="https://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.pdf">
   [ICCVW 2021]
  </a>
  PatchAugment: Local Neighborhood Augmentation in Point Cloud Classification.
  <a href="https://github.com/VimsLab/PatchAugment">
   [Code]
  </a>
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/a494dfaa-ec1a-4de5-8bfb-783cd817edc5.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>75. Pose Estimation Blocks</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>KPE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ORN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Orientation Regularized Network
  </strong>
  (ORN) is a multi-view image fusion technique for pose estimation. It uses IMU orientations as a structural prior to mutually fuse the image features of each pair of joints linked by IMUs. For example, it uses the features of the elbow to reinforce those of the wrist based on the IMU at the lower-arm.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_5.24.00_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>76. Feature Upsampling</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CARAFE</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Content-Aware ReAssembly of FEatures (CARAFE)
  </strong>
  is an operator for feature upsampling in convolutional neural networks. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_10.01.18_AM_QKWM5fM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>77. OCR Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>TrOCR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TrOCR
  </strong>
  is an end-to-end
  <a href="https://paperswithcode.com/methods/category/transformers">
   Transformer
  </a>
  -based OCR model for text recognition with pre-trained CV and NLP models. It leverages the
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  architecture for both image understanding and wordpiece-level text generation. It first resizes the input text image into $384 × 384$ and then the image is split into a sequence of 16 patches which are used as the input to image Transformers.  Standard Transformer architecture with the
  <a href="https://paperswithcode.com/method/scaled">
   self-attention mechanism
  </a>
  is leveraged on both encoder and decoder parts, where wordpiece units are generated as the recognized text from the input image.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/1f40c110-162f-4098-81cc-363958864490.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>PP-OCR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PP-OCR
  </strong>
  is an OCR system that consists of three parts, text detection, detected boxes rectification and text recognition. The purpose of text detection is to locate the text area in the image. In PP-OCR, Differentiable Binarization (DB) is used as text detector which is based on a simple segmentation network. It integrates feature extraction and sequence modeling. It adopts the Connectionist Temporal Classification (CTC) loss to avoid the inconsistency between prediction and label.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-06_at_9.57.17_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>78. Text Instance Representations</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Fourier Contour Embedding</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Fourier Contour Embedding
  </strong>
  is a text instance representation that allows networks to learn diverse text geometry variances. Most of existing methods model text instances in image spatial domain via masks or contour point sequences in the Cartesian or the polar coordinate system. However, the mask representation might lead to expensive post-processing, while the point sequence one may have limited capability to model texts with highly-curved shapes. This motivates modeling text instances in the Fourier domain.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-09_at_9.29.23_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>79. Point Cloud Representations</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>PolarNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   PolarNet
  </strong>
  is an improved grid representation for online, single-scan LiDAR point clouds. Instead of using common spherical or bird's-eye-view projection, the polar bird's-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network's attention with the long-tailed distribution of the points along the radial axis.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_10.05.09_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>BTF</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>80. Video Panoptic Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VPSNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Video Panoptic Segmentation Network
  </strong>
  , or
  <strong>
   VPSNet
  </strong>
  , is a model for video panoptic segmentation. On top of UPSNet, which is a method for image panoptic segmentation, VPSNet is designed to take an additional frame as the reference to correlate time information at two levels: pixel-level fusion and object-level tracking. To pick up the complementary feature points in the reference frame, a flow-based feature map alignment module is introduced along with an asymmetric attention block that computes similarities between the target and reference features to fuse them into one-frame shape. Additionally, to associate object instances across time, 
 an object track head is added which learns the correspondence between the instances in the target and reference frames based
on their RoI feature similarity.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_10.25.03_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>ViP-DeepLab</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ViP-DeepLab
  </strong>
  is a model for depth-aware video panoptic segmentation. It extends Panoptic-
  <a href="https://paperswithcode.com/method/deeplab">
   DeepLab
  </a>
  by adding a depth prediction head to perform monocular depth estimation and a next-frame instance branch which regresses to the object centers in frame $t$ for frame $t + 1$.  This allows the model to jointly perform video panoptic segmentation and monocular depth estimation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.36.55_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>81. Monocular Depth Estimation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ViP-DeepLab</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ViP-DeepLab
  </strong>
  is a model for depth-aware video panoptic segmentation. It extends Panoptic-
  <a href="https://paperswithcode.com/method/deeplab">
   DeepLab
  </a>
  by adding a depth prediction head to perform monocular depth estimation and a next-frame instance branch which regresses to the object centers in frame $t$ for frame $t + 1$.  This allows the model to jointly perform video panoptic segmentation and monocular depth estimation.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.36.55_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>82. Anchor Generation Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Guided Anchoring</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Guided Anchoring
  </strong>
  is an anchoring scheme for object detection which leverages semantic features to guide the anchoring. The method is motivated by the observation that objects are not distributed evenly over the image. The scale of an object is also closely related to the imagery content, its location and geometry of the scene. Following this intuition, the method generates sparse anchors in two steps: first identifying sub-regions that may contain objects and then determining the shapes at different locations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_12.21.41_PM_WXDvg8q.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>Probabilistic Anchor Assignment</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Probabilistic anchor assignment (PAA)
  </strong>
  adaptively separates a set of anchors into positive and negative samples for a GT box according to the learning status of the model associated with it. To do so we first define a score of a detected bounding box that reflects both the classification and localization qualities. We then identify the connection between this score and the training objectives and represent the score as the combination of two loss objectives. Based on this scoring scheme, we calculate the scores of individual anchors that reflect how the model finds useful cues to detect a target object in each anchor. With these anchor scores, we aim to find a probability distribution of two modalities that best represents the scores as positive or negative samples as in the Figure.
 </p>
 <p>
  Under the found probability distribution, anchors with probabilities from the positive component are high are selected as positive samples. This transforms the anchor assignment problem to a maximum likelihood estimation for a probability distribution where the parameters of the distribution is determined by anchor scores. Based on the assumption that anchor scores calculated by the model are samples drawn from a probability distribution, it is expected that the model can infer the sample separation in a probabilistic way, leading to easier training of the model compared to other non-probabilistic assignments. Moreover, since positive samples are adaptively selected based on the anchor score distribution, it does not require a pre-defined number of positive samples nor an IoU threshold.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-02-23_at_1.55.54_PM_7AT9xK0.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>83. VQA Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MODERN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MODERN
  </strong>
  , or
  <strong>
   Modulated Residual Network
  </strong>
  , is an architecture for
  <a href="https://paperswithcode.com/task/visual-question-answering">
   visual question answering
  </a>
  (VQA). It employs
  <a href="https://paperswithcode.com/method/conditional-batch-normalization">
   conditional batch normalization
  </a>
  to allow a linguistic embedding from an
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  to modulate the
  <a href="https://paperswithcode.com/method/batch-normalization">
   batch normalization
  </a>
  parameters of a
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  . This enables the linguistic embedding to manipulate entire feature maps by scaling them up or down, negating them, or shutting them off, etc.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-04_at_4.19.29_PM_hQ5PNtV.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
        <li>
            <details class="method">
            <summary>U-CAM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide
  <a href="https://paperswithcode.com/method/visual-attention">
   visual attention
  </a>
  maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a tool for obtaining improved certainty estimates and explanations for deep learning models.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>84. CAD Design Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>BRepNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BRepNet
  </strong>
  is a neural network for CAD applications. It is designed to operate directly on B-rep data structures, avoiding the need to approximate the model as meshes or point clouds. BRepNet defines convolutional kernels with respect to oriented coedges in the data structure. In the neighborhood of each coedge, a small collection of faces, edges and coedges can be identified and patterns in the feature vectors from these entities detected by specific learnable parameters.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.43.30_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>85. Style Transfer Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>LapStyle</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   LapStyle
  </strong>
  , or
  <strong>
   Laplacian Pyramid Network
  </strong>
  , is a feed-forward style transfer method. It uses a
  <a href="https://paperswithcode.com/method/drafting-network">
   Drafting Network
  </a>
  to transfer global style patterns in low-resolution, and adopts higher resolution
  <a href="https://paperswithcode.com/method/revision-network">
   Revision Networks
  </a>
  to revise local styles in a pyramid manner according to outputs of multi-level Laplacian filtering of the content image. Higher resolution details can be generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels.
 </p>
 <p>
  Specifically, we first generate image pyramid $\left(\bar{x}_{c}, r_{c}\right)$ from content image $x_{c}$ with the help of Laplacian filter. Rough low-resolution stylized image are then generated by the Drafting Network. Then the Revision Network generates stylized detail image in high resolution. Then the final stylized image is generated by aggregating the outputs pyramid. $L, C$ and $A$ in an image represent Laplacian, concatenate and aggregation operation separately.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/8a194ee9-acf4-402b-819e-a7ee89523c45.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>86. Image dataset comparison metric</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Metrics for comparing image semantic segmentation datasets
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
            
        </ul>
        
        <ul class="parent">
            <p>87. Image Scaling Strategies</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FixRes</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FixRes
  </strong>
  is an image scaling strategy that seeks to optimize classifier performance. It is motivated by the observation that data augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! FixRes is a simple strategy to optimize the classifier performance, that employs different train and test resolutions. The calibrations are: (a) calibrating the object sizes by adjusting the crop size and (b) adjusting statistics before spatial pooling.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_11.25.10_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>88. Rendezvous</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    A transformer-based approach with a mixture of several types of attentions.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MHMA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  The multi-head of mixed attention combines both self- and cross-attentions, encouraging high-level learning of interactions between entities captured in the various attention features. It is build with several attention heads, each of the head can implement either self or cross attention. A self attention is when the key and query features are the same or come from the same domain features. A cross attention is when the key and query features are generated from different features. Modeling MHMA allows a model to identity the relationship between features of different domains. This is very useful in tasks involving relationship modeling such as human-object interaction, tool-tissue interaction, man-machine interaction, human-computer interface, etc.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/6ccc765a-9fa3-48d9-809d-80e6da7be14a.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>89. Image Semantic Segmentation Metric</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Useful for comparing image semantic segmentation datasets.
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>IPBI</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  In a given dataset for semantic image segmentation, the number of samples per class should be the same, so that no classifier would be biased towards the majority class (here included the background). It is very difficult, if not impossible, to achieve a perfect balance between the several classes of objects of a dataset. Considering that the segmentation of the objects  is accomplished at the pixel level, the number of pixels for each class must be taken into account. As a matter of fact, in image semantic segmentation, 
different classes and the background may have quite different
sizes. Therefore, the image segmentation problem is naturally unbalanced. The IPBI is based on the concept of entropy, a common measure used in many fields of science. In a general sense, it measures the amount of disorder of a system. For the sake of semantic image segmentation, the ideal dataset should have the same number of instances per class, as well as the same number of pixels in all classes. Similar reasoning can be done considering the number of pixels of all samples in a class, so that we can obtain the
pixels balance measure for the dataset. Overall, IPBI evaluates the balance of pixels and number of instances of an image semantic segmentation dataset and, so, it is usefull to compare different datasets.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>90. Image Manipulation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DeepSIM</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeepSIM
  </strong>
  is a generative model for conditional image manipulation based on a single image. The network learns to map between a primitive representation of the image to the image itself. At manipulation time, the generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. The choice of a primitive representations has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual, or hybrid such as edges on top of segmentations.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-15_at_5.21.30_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>91. Face Detection Models</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Models for face detection
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>TinaFace</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   TinaFace
  </strong>
  is a type of face detection method that is based on generic object detection. It consists of (a) Feature Extractor:
  <a href="https://paperswithcode.com/method/resnet">
   ResNet
  </a>
  -50 and 6 level
  <a href="https://www.paperswithcode.com/method/fpn">
   Feature Pyramid Network
  </a>
  to extract the multi-scale features of input image; (b) an Inception block to enhance receptive field; (c) Classification Head: 5 layers
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  for classification of anchors; (d) Regression Head: 5 layers
  <a href="https://paperswithcode.com/method/fcn">
   FCN
  </a>
  for regression of anchors to ground-truth objects boxes; (e) IoU Aware Head: a single convolutional layer for IoU prediction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.43.36_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>92. Super-Resolution Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SRGAN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SRGAN
  </strong>
  is a generative adversarial network for single image super-resolution. It uses a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes the solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, the authors use a content loss motivated by perceptual similarity instead of similarity in pixel space. The actual networks - depicted in the Figure to the right - consist mainly of residual blocks for feature extraction.
 </p>
 <p>
  Formally we write the perceptual loss function as a weighted sum of a (
  <a href="https://paperswithcode.com/method/vgg">
   VGG
  </a>
  ) content loss $l^{SR}_{X}$ and an adversarial loss component $l^{SR}_{Gen}$:
 </p>
 <p>
  $$ l^{SR} = l^{SR}_{X} + 10^{-3}l^{SR}_{Gen} $$
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-19_at_11.13.45_AM_zsF2pa7.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>93. Video Inpainting Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FuseFormer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FuseFormer
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  -based model designed for video inpainting via fine-grained feature fusion based on novel
  <a href="https://paperswithcode.com/method/soft-split-and-soft-composition">
   Soft Split and Soft Composition
  </a>
  operations. The soft split divides feature map into many patches with given overlapping interval while the soft composition stitches them back into a whole feature map where pixels in overlapping regions are summed up. FuseFormer builds soft composition and soft split into its
  <a href="https://paperswithcode.com/method/feedforward-network">
   feedforward network
  </a>
  for further enhancing subpatch level feature fusion.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_10.27.49_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>94. Trajectory Data Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>SimAug</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   SimAug
  </strong>
  , or
  <strong>
   Simulation as Augmentation
  </strong>
  , is a data augmentation method for trajectory prediction. It augments the representation such that it is robust to the variances in semantic scenes and camera views.  First, to deal with the gap between real and synthetic semantic scene, it represents each training trajectory by high-level scene semantic segmentation features, and defends the model from adversarial examples generated by whitebox attack methods. Second, to overcome the changes in camera views, it generates multiple views for the same trajectory, and encourages the model to focus on the “hardest” view to which the model has learned. The classification loss is adopted and the view with the highest loss is favored during training. Finally, the augmented trajectory is computed as a convex combination of the trajectories generated in previous steps. The trajectory prediction model is built on a multi-scale representation and the final model is trained to minimize the empirical vicinal risk over the distribution of augmented trajectories.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-30_at_1.26.43_PM_Rt8MywE.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>95. Thermal Image Processing Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DeepIR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DeepIR
  </strong>
  , or
  <strong>
   Deep InfraRed image processing
  </strong>
  , is a thermal image processing framework for recovering high quality images from a very small set of images captured with camera motion. Enhancement is achieved by noting that camera motion, which is usually a hinderance, can be exploited to our advantage to separate a sequence of images into the scene-dependent radiant flux, and a slowly changing scene-independent non-uniformity. DeepIR combines the physics of microbolometer sensors, with powerful regularization capabilities by neural network-based representations. DeepIR relies on the key observation that jittering a camera, while unwanted in visible domain, is highly desirable in the thermal domain as it allows an accurate separation of the sensor-specific non-uniformities from the scene’s radiant flux.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-27_at_10.51.00_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>96. Interactive Semantic Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>EdgeFlow</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   EdgeFlow
  </strong>
  is an interactive segmentation architecture that fully utilizes interactive information of user clicks with edge-guided flow. Edge guidance is the idea that interactive segmentation improves segmentation masks progressively with user clicks. Based on user clicks, an edge mask scheme is used, which takes the object edges estimated from the previous iteration as prior information, instead of direct mask estimation (if the previous mask is used as input, poor segmentation results could result).
 </p>
 <p>
  The architecture consists of a coarse-to-fine network including CoarseNet and FineNet. For CoarseNet,
  <a href="https://paperswithcode.com/method/hrnet">
   HRNet
  </a>
  -18+OCR is utilized as the base segmentation model and the edge-guided flow is appended to deal with interactive information. For FineNet, three
  <a href="https://paperswithcode.com/method/dilated-convolution">
   atrous convolution
  </a>
  blocks are utilized to refine the coarse masks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/97e2dc43-ab5a-4cd3-82d8-5f7706ad3fe4.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>97. Trajectory Prediction Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Social-STGCNN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Social-STGCNN
  </strong>
  is a method for human trajectory prediction. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_1.12.43_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>98. Cashier-Free Shopping</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Grab</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Grab
  </strong>
  is a sensor processing system for cashier-free shopping. Grab needs to accurately identify and track customers, and associate each shopper with items he or she retrieves from shelves. To do this, it uses a keypoint-based pose tracker as a building block for identification and tracking, develops robust feature-based face trackers, and algorithms for associating and tracking arm movements. It also uses a probabilistic framework to fuse readings from camera, weight and RFID sensors in order to accurately assess which shopper picks up which item.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-04_at_11.49.40_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>99. Arbitrary Object Detectors</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CSL</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Circular Smooth Label
  </strong>
  (CSL) is a classification-based rotation detection technique for arbitrary-oriented object detection. It is used for circularly distributed angle classification and addresses the periodicity of the angle and increases the error tolerance to adjacent angles.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.44.38_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>100. Human Object Interaction Detectors</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VSGNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Visual-Spatial-Graph Network
  </strong>
  (VSGNet) is a network for human-object interaction detection. It extracts visual features from the image representing the human-object pair, refines the features with spatial configurations of the pair, and utilizes the structural connections between the pair via graph convolutions.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.47.39_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>101. Motion Prediction Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MotionNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   MotionNet
  </strong>
  is a system for joint perception and motion prediction based on a bird's eye view (BEV) map, which encodes the object category and motion information from 3D point clouds in each grid cell. MotionNet takes a sequence of LiDAR sweeps as input and outputs the bird's eye view (BEV) map. The backbone of MotionNet is a spatio-temporal pyramid network, which extracts deep spatial and temporal features in a hierarchical fashion. To enforce the smoothness of predictions over both space and time, the training of MotionNet is further regularized with novel spatial and temporal consistency losses.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_9.58.10_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>102. Image Colorization Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Colorization Transformer</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Colorization Transformer
  </strong>
  is a probabilistic
  <a href="https://paperswithcode.com/method/colorization">
   colorization
  </a>
  model composed only of
  <a href="https://paperswithcode.com/method/axial">
   axial self-attention blocks
  </a>
  . The main advantages of these blocks are the ability to capture a global receptive field with only two layers and $\mathcal{O}(D\sqrt{D})$ instead of $\text{O}(D^{2})$ complexity. In order to enable colorization of high-resolution grayscale images, the task is decomposed into three simpler sequential subtasks: coarse low resolution autoregressive colorization, parallel color and spatial super-resolution.
 </p>
 <p>
  For coarse low resolution colorization, a conditional variant of
  <a href="https://paperswithcode.com/method/axial">
   Axial Transformer
  </a>
  is applied. The authors leverage the semi-parallel sampling mechanism of Axial Transformers. Finally, fast parallel deterministic upsampling models are employed to super-resolve the coarsely colorized image into the final high resolution output.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_3.11.14_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>103. Deraining Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MSPFN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Multi-scale Progressive Fusion Network
  </strong>
  (MSFPN) is a neural network representation for single image deraining. It aims to exploit the correlated information of rain streaks across scales for single image deraining.
 </p>
 <p>
  Specifically, we first generate the Gaussian pyramid rain images using Gaussian kernels to down-sample the original rain image in sequence. A coarse-fusion module (CFM) is designed to capture the global texture information from these multi-scale rain images through recurrent calculation (Conv-
  <a href="https://paperswithcode.com/method/lstm">
   LSTM
  </a>
  ), thus enabling the network to cooperatively represent the target rain streak using similar counterparts from global feature space. Meanwhile, the representation of the high-resolution pyramid layer is guided by previous outputs as well as all low-resolution pyramid layers. A finefusion module (FFM) is followed to further integrate these correlated information from different scales. By using the channel attention mechanism, the network not only discriminatively learns the scale-specific knowledge from all preceding pyramid layers, but also reduces the feature redundancy effectively. Moreover, multiple FFMs can be cascaded to form a progressive multi-scale fusion. Finally, a reconstruction module (RM) is appended to aggregate the coarse and fine rain information extracted respectively from CFM and FFM for learning the residual rain image, which is the approximation of real rain streak distribution.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_11.48.43_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>104. Video Instance Segmentation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>VisTR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   VisTR
  </strong>
  is a
  <a href="https://paperswithcode.com/method/transformer">
   Transformer
  </a>
  based video instance segmentation model. It views video instance segmentation as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_1.38.42_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>105. Detection Assignment Rules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>POTO</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Prediction-aware One-To-One
  </strong>
  , or
  <strong>
   POTO
  </strong>
  , is an assignment rule for object detection which dynamically assigns the foreground samples according to the quality of classification and regression simultaneously.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-10_at_2.06.26_PM_kNmEr9T.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>106. Image Inpainting Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Contextual Residual Aggregation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Contextual Residual Aggregation
  </strong>
  , or
  <strong>
   CRA
  </strong>
  , is a module for image inpainting. It can produce high-frequency residuals for missing contents by weighted aggregating residuals from contextual patches, thus only requiring a low-resolution prediction from the network. Specifically, it involves a neural network to predict a low-resolution inpainted result and up-sample it to yield a large blurry image. Then we produce the high-frequency residuals for in-hole patches by aggregating weighted high-frequency residuals from contextual patches. Finally, we add the aggregated residuals to the large blurry image to obtain a sharp result.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_9.49.46_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>107. Graphics Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ManifoldPlus</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ManifoldPlus
  </strong>
  is a method for robust and scalable conversion of triangle soups to watertight manifolds. It extracts exterior faces between occupied voxels and empty voxels, and uses a projection based optimization method to accurately recover a watertight manifold that resembles the reference mesh. It does not rely on face normals of the input triangle soups and can accurately recover zero-volume structures. For scalability, it employs an adaptive Gauss-Seidel method for shape optimization, in which each step is an easy-to-solve convex problem.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.00.00_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>108. Font Generation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Attribute2Font</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Attribute2Font
  </strong>
  is a model that automatically creates fonts by synthesizing visually pleasing glyph images according to user-specified attributes and their corresponding values. Specifically, Attribute2Font is trained to perform font style transfer between any two fonts conditioned on their attribute values. After training, the model can generate glyph images in accordance with an arbitrary set of font attribute values. A unit named Attribute Attention Module is designed to make those generated glyph images better embody the prominent font attributes. A semi-supervised learning scheme is also introduced to exploit a large number of unlabeled fonts
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.32.54_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>109. Oriented Object Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>DAFNe</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   DAFNe
  </strong>
  is a dense one-stage anchor-free deep model for oriented object detection. It is a deep neural network that performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, it reduces the prediction complexity by refraining from employing bounding box anchors. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. Moreover, it introduces an orientation-aware generalization of the center-ness function to arbitrary quadrilaterals that takes into account the object's orientation and that, accordingly, accurately down-weights low-quality predictions
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-16_at_9.11.24_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>110. Video Interpolation Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>FLAVR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   FLAVR
  </strong>
  is an architecture for video frame interpolation. It uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Overall, it consists of a
  <a href="https://paperswithcode.com/method/u-net">
   U-Net
  </a>
  style architecture with 3D space-time convolutions and
deconvolutions (yellow blocks). Channel gating is used after all (de-)
  <a href="https://paperswithcode.com/method/convolution">
   convolution
  </a>
  layers (blue blocks). The final prediction layer (the purple block) is implemented as a convolution layer to project the 3D feature maps into $(k−1)$ frame predictions. This design allows FLAVR to predict multiple frames in one inference forward pass.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.40.53_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>111. Mask Branches</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Spatial Attention-Guided Mask</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   A Spatial Attention-Guided Mask
  </strong>
  is a module for
  <a href="https://paperswithcode.com/task/instance-segmentation">
   instance segmentation
  </a>
  that predicts a segmentation mask on each detected box with a spatial attention map that helps to focus on informative pixels and suppress noise. The goal is to guide the mask head for spotlighting meaningful pixels and repressing uninformative ones.
 </p>
 <p>
  Once features inside the predicted RoIs are extracted by
  <a href="https://paperswithcode.com/method/roi-align">
   RoIAlign
  </a>
  with 14×14 resolution, those features are fed into four conv layers and the
  <a href="https://paperswithcode.com/method/spatial-attention-module">
   spatial attention module
  </a>
  (SAM) sequentially. To exploit the spatial attention map $A_{sag}\left(X_{i}\right) \in \mathcal{R}^{1\times{W}\times{H}}$ as a feature descriptor given input feature map $X_{i} \in \mathcal{R}^{C×W×H}$, the SAM first generates pooled features $P_{avg}, P_{max} \in \mathcal{R}^{1\times{W}\times{H}}$ by both average and
  <a href="https://paperswithcode.com/method/max-pooling">
   max pooling
  </a>
  operations respectively along the channel axis and aggregates them via concatenation. Then it is followed by a 3 × 3 conv layer and normalized by the sigmoid function. The computation process
is summarized as follow:
 </p>
 <p>
  $$
A_{sag}\left(X_{i}\right) = \sigma\left(F_{3\times{3}}(P_{max} \cdot P_{avg})\right)
$$
 </p>
 <p>
  where $\sigma$ denotes the sigmoid function, $F_{3\times{3}}$ is 3 × 3 conv layer and $\cdot$ represents the concatenate operation. Finally, the attention guided feature map $X_{sag} ∈ \mathcal{R}^{C\times{W}\times{H}}$ is computed as:
 </p>
 <p>
  $$
X_{sag} = A_{sag}\left(X_{i}\right) \otimes X_{i}
$$
 </p>
 <p>
  where ⊗ denotes element-wise multiplication. After then, a 2 × 2 deconv upsamples the spatially attended feature map to 28 × 28 resolution. Lastly, a 1 × 1 conv is applied for predicting class-specific masks.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.06.19_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>112. Degridding</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Hierarchical Feature Fusion</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Hierarchical Feature Fusion (HFF)
  </strong>
  is a feature fusion method employed in
  <a href="https://paperswithcode.com/method/esp">
   ESP
  </a>
  and
  <a href="https://paperswithcode.com/method/eesp">
   EESP
  </a>
  image model blocks for degridding. In the ESP module, concatenating the outputs of dilated convolutions gives the ESP module a large effective receptive field, but it introduces unwanted checkerboard or gridding artifacts. To address the gridding artifact in ESP, the feature maps obtained using kernels of different dilation rates are hierarchically added before concatenating them (HFF). This solution is simple and effective and does not increase the complexity of the ESP module.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-27_at_2.49.59_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>113. Generative Discrimination</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Minibatch Discrimination</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Minibatch Discrimination
  </strong>
  is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-03_at_2.31.12_PM_l5lBwCI.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>114. RGB-D Saliency Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>UCNet</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   UCNet
  </strong>
  is a probabilistic framework for RGB-D Saliency Detection that employs uncertainty by learning from the data labelling process. It utilizes conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-11_at_12.45.40_PM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>115. Math Formula Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>ScanSSD</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   ScanSSD
  </strong>
  is a single-shot Detector (
  <a href="https://paperswithcode.com/method/ssd">
   SSD
  </a>
  ) for locating math formulas offset from text and embedded in textlines. It uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-05_at_10.09.40_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>116. Lane Detection Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>YOLOP</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   YOLOP
  </strong>
  is a panoptic driving perception network for handling traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. It can be thought of a lightweight version of Tesla's HydraNet model for self-driving cars.
 </p>
 <p>
  A lightweight CNN, from Scaled-yolov4, is used as the encoder to extract features from the image. Then these feature maps are fed to three decoders to complete their respective tasks. The detection decoder is based on the current best-performing single-stage detection network,
  <a href="https://paperswithcode.com/method/yolov4">
   YOLOv4
  </a>
  ,  for two main reasons: (1) The single-stage detection network is faster than the two-stage detection network. (2) The grid-based prediction mechanism of the single-stage detector is more related to the other two semantic segmentation tasks, while instance segmentation is usually combined with the region based detector as in
  <a href="https://paperswithcode.com/method/mask-r-cnn">
   Mask R-CNN
  </a>
  . The feature map output by the encoder incorporates semantic features of different levels and scales, and our segmentation branch can use these feature maps to complete pixel-wise semantic prediction.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_7.13.00_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>117. Person Search Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>AlignPS</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   AlignPS
  </strong>
  , or
  <strong>
   Feature-Aligned Person Search Network
  </strong>
  , is an anchor-free framework for efficient person search. The model employs the typical architecture of an anchor-free detection model (i.e.,
  <a href="https://paperswithcode.com/method/fcos">
   FCOS
  </a>
  ). An aligned feature aggregation (AFA) module is designed to make the model focus more on the re-id subtask. Specifically, AFA reshapes some building blocks of
  <a href="https://paperswithcode.com/method/fpn">
   FPN
  </a>
  to overcome the issues of region and scale misalignment in re-id feature learning. A
  <a href="https://paperswithcode.com/method/deformable-convolution">
   deformable convolution
  </a>
  is exploited to make the re-id embeddings adaptively aligned with the foreground regions. A feature fusion scheme is designed to better aggregate features from different FPN levels, which makes the re-id features more robust to scale variations. The training procedures of re-id and detection are also optimized to place more emphasis on generating robust re-id embeddings.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-02_at_8.28.33_AM_Vww1lcc.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>118. Image Decomposition Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>BIDeN</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BIDeN
  </strong>
  , or
  <strong>
   Blind Image Decomposition Network
  </strong>
  , is a model for blind image decomposition, which requires separating a superimposed image into constituent underlying images in a blind setting, that is, both the source components involved in mixing as well as the mixing mechanism are unknown.  For example, rain may consist of multiple components, such as rain streaks, raindrops, snow, and haze.
 </p>
 <p>
  The Figure shows an example where $N = 4, L = 2, x = {a, b, c, d}$, and $I = {1, 3}$. $a, c$ are selected then passed to the mixing function $f$, and outputs the mixed input image $z$, which is $f\left(a, c\right)$ here. The generator consists of an encoder $E$ with three branches and multiple heads $H$. $\bigotimes$ denotes the concatenation operation. Depth and receptive field of each branch is different to capture multiple scales of features. Each specified head points to the corresponding source component, and the number of heads varies with the maximum number of source components N. All reconstructed images $\left(a', c'\right)$ and their corresponding real images $\left(a, c\right)$ are sent to an unconditional discriminator. The discriminator also predicts the source components of the input image $z$. The outputs from other heads $\left(b', d'\right)$ do not contribute to the optimization.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-08-26_at_7.20.40_AM.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>119. Video Data Augmentation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Temporally Consistent Spatial Augmentation</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Temporally Consistent Spatial Augmentation
  </strong>
  is a video data augmentation technique used for contrastive learning in the
  <a href="https://paperswithcode.com/method/cvrl">
   Contrastive Video Representation Learning
  </a>
  framework. It fixes the randomness of spatial augmentation across frames; this prevents spatial augmentation hurting learning if applied independently across frames, because in that case it breaks the natural motion. In contrast, having temporally consistent spatial augmentation does not break the natural motion in the frames.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-09-17_at_10.48.32_AM_hzlJHIK.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>120. Object Detection Modules</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Grid Sensitive</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Grid Sensitive
  </strong>
  is a trick for object detection introduced by
  <a href="https://paperswithcode.com/method/yolov4">
   YOLOv4
  </a>
  . When we decode the coordinate of the bounding box center $x$ and $y$, in original
  <a href="https://paperswithcode.com/method/yolov3">
   YOLOv3
  </a>
  , we can get them by
 </p>
 <p>
  $$
\begin{aligned}
&amp;x=s \cdot\left(g_{x}+\sigma\left(p_{x}\right)\right) \
&amp;y=s \cdot\left(g_{y}+\sigma\left(p_{y}\right)\right)
\end{aligned}
$$
 </p>
 <p>
  where $\sigma$ is the sigmoid function, $g_{x}$ and $g_{y}$ are integers and $s$ is a scale factor. Obviously, $x$ and $y$ cannot be exactly equal to $s \cdot g_{x}$ or $s \cdot\left(g_{x}+1\right)$. This makes it difficult to predict the centres of bounding boxes that just located on the grid boundary. We can address this problem, by changing the equation to
 </p>
 <p>
  $$
\begin{aligned}
&amp;x=s \cdot\left(g_{x}+\alpha \cdot \sigma\left(p_{x}\right)-(\alpha-1) / 2\right) \
&amp;y=s \cdot\left(g_{y}+\alpha \cdot \sigma\left(p_{y}\right)-(\alpha-1) / 2\right)
\end{aligned}
$$
 </p>
 <p>
  This makes it easier for the model to predict bounding box center exactly located on the grid boundary. The FLOPs added by Grid Sensitive are really small, and can be totally ignored.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>121. Few-Shot Image-to-Image Translation</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>COCO-FUNIT</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   COCO-FUNIT
  </strong>
  is few-shot image translation model which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. It builds on top of
  <a href="https://arxiv.org/abs/1905.01723">
   FUNIT
  </a>
  by identifying the content loss problem and then addressing it with a novel content-conditioned style encoder architecture.
 </p>
 <p>
  The FUNIT method suffers from the content loss problem—the translation result is not well-aligned with the input image. While a direct theoretical analysis is likely elusive, we conduct an empirical study, aiming at identify the cause of the content loss problem. In analyses, the authors show that the FUNIT style encoder produces very different style codes using different crops -- suggesting the style code contains other information about the style image such as the object pose.
 </p>
 <p>
  To make the style embedding more robust to small variations in the style image, a new style encoder architecture, the Content-Conditioned style encoder (COCO), is introduced. The most distinctive feature of this new encoder is the conditioning in the content image as illustrated in the top-right of the Figure. Unlike the style encoder in FUNIT, COCO takes both content and style image as input. With this content-conditioning scheme, a direct feedback path is created during learning to let the content image influence how the style code is computed. It also helps reduce the direct influence of the style image to the extract style code.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/3e68e5a5-0c75-4755-9258-013b28a5a50d.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>122. Video Super-Resolution Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>BasicVSR</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   BasicVSR
  </strong>
  is a video super-resolution pipeline including optical flow and
  <a href="https://paperswithcode.com/method/residual-connection">
   residual blocks
  </a>
  . It adopts a typical bidirectional recurrent network. The upsampling module $U$ contains multiple
  <a href="https://paperswithcode.com/method/pixelshuffle">
   pixel-shuffle
  </a>
  and convolutions. In the Figure, red and blue colors represent the backward and forward propagations, respectively.  The propagation branches contain only generic components. $S, W$, and $R$ refer to the flow estimation module, spatial warping module, and residual blocks, respectively.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/99cb6935-be9f-4f6f-9e5e-970ef97d7ebc.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>123. Output Heads</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>Dynamic Keypoint Head</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  <strong>
   Dynamic Keypoint Head
  </strong>
  is an output head for pose estimation that are conditioned on each instance (person), and can encode the instance concept in the dynamically-generated weights of their filters. They are used in the
  <a href="https://paperswithcode.com/method/fcpose">
   FCPose
  </a>
  architecture.
 </p>
 <p>
  The Figure shows the core idea. $F$ denotes a level of feature maps. "Rel. Coord." means the relative coordinates, denoting the relative offsets from the locations of $F$ to the location where the filters are generated. Refer to the text for details. $f_{\theta_{i}}$ is the dynamically-generated keypoint head for the $i$-th person instance. Note that each person instance has its own keypoint head.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/c806a95d-11ba-4ad7-b327-184cc7717551.png" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>124. Webpage Object Detection Pipeline</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    End-to-end training pipeline for Object Detection from Webpages where candidate bounding boxes are obtained from the DOM tree
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>CoVA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
 <p>
  Context-Aware Visual Attention-based end-to-end pipeline for Webpage Object Detection (
  <em>
   CoVA
  </em>
  ) aims to learn function
  <em>
   f
  </em>
  to predict labels
  <em>
   y = [$y_1, y_2, ..., y_N$]
  </em>
  for a webpage containing
  <em>
   N
  </em>
  elements. The input to CoVA consists of:
1. a screenshot of a webpage,
2. list of bounding boxes
  <em>
   [x, y, w, h]
  </em>
  of the web elements, and
3. neighborhood information for each element obtained from the DOM tree.
 </p>
 <p>
  This information is processed in four stages:
1. the graph representation extraction for the webpage,
2. the Representation Network (
  <em>
   RN
  </em>
  ),
3. the Graph Attention Network (
  <em>
   GAT
  </em>
  ), and
4. a fully connected (
  <em>
   FC
  </em>
  ) layer.
 </p>
 <p>
  The graph representation extraction computes for every web element
  <em>
   i
  </em>
  its set of
  <em>
   K
  </em>
  neighboring web elements
  <em>
   $N_i$
  </em>
  . The
  <em>
   RN
  </em>
  consists of a Convolutional Neural Net (
  <em>
   CNN
  </em>
  ) and a positional encoder aimed to learn a visual representation
  <em>
   $v_i$
  </em>
  for each web element
  <em>
   i ∈ {1, ..., N}
  </em>
  . The
  <em>
   GAT
  </em>
  combines the visual representation
  <em>
   $v_i$
  </em>
  of the web element
  <em>
   i
  </em>
  to be classified and those of its neighbors, i.e.,
  <em>
   $v_k$ ∀k ∈ $N_i$
  </em>
  to compute the contextual representation
  <em>
   $c_i$
  </em>
  for web element
  <em>
   i
  </em>
  . Finally, the visual and contextual representations of the web element are concatenated and passed through the
  <em>
   FC
  </em>
  layer to obtain the classification output.
 </p>
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/methods/199aec2b-13bf-4512-8296-15ab1e782afc.jpg" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>125. Video Quality Models</p>
            <hr>
            <li class="col-md-12">
                
            </li>
            
        <li>
            <details class="method-all depth1">
            <summary>Top methods</summary>
            <ul>
                
        <li>
            <details class="method">
            <summary>MDTVSFA</summary>
            <ul class="col-md-12">
                <li>
                    <div class="row">
                        <div class="col-md-8 description">
</div>

                        <div class="col-md-4">
                            <img src="https://production-media.paperswithcode.com/tasks/default.gif" style="width: 100%">
                        </div>
                    </div>
                </li>
            </ul>
            </details>
        </li>
        
            </ul>
            </details>
        </li>
    
            
        </ul>
        
        <ul class="parent">
            <p>126. Medical Image Deblurring</p>
            <hr>
            <li class="col-md-12">
                <div class="row task-content">
 <div class="col-md-9 description">
  <div class="description-content">
   <p>
    Medical image deblurring aims to remove blurs from medical images
   </p>
  </div>
 </div>
 <div class="col-md-3 task-infobox">
 </div>
</div>

            </li>
            
            
        </ul>
        </div></body></html>